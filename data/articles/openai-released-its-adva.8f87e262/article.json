{
  "url": "https://www.technologyreview.com/2024/09/24/1104422/openai-released-its-advanced-voice-mode-to-more-people-heres-how-to-get-it/",
  "title": "OpenAI released its advanced voice mode to more people. Here\u2019s how to get it.",
  "ut": 1727170684.0,
  "body_paragraphs": [
    "OpenAI is broadening access to Advanced Voice Mode, a feature of ChatGPT that allows you to speak more naturally with the AI model. It allows you to interrupt its responses midsentence, and it can sense and interpret your emotions from your tone of voice and adjust its responses accordingly.\u00a0 These features were teased back in May when OpenAI unveiled GPT-4o, but they were not released until July\u2014and then just to an invite-only group. (At least initially, there seem to have been some safety issues with the model; OpenAI gave several Wired reporters access to the voice mode back in May, but the magazine reported that the company \u201cpulled it the next morning, citing safety concerns.\u201d)   Users who\u2019ve been able to try it have largely described the model as an impressively fast, dynamic, and realistic voice assistant\u2014which has made its limited availability particularly frustrating to some other OpenAI users.\u00a0 Today is the first time OpenAI has promised to bring the new voice mode to a wide range of users. Here\u2019s what you need to know.",
    "What can it do?\u00a0 Though ChatGPT currently offers a standard voice mode to paid users, its interactions can be clunky. In the mobile app, for example, you can\u2019t interrupt the model\u2019s often long-winded responses with your voice, only with a tap on the screen. The new version fixes that, and also promises to modify its responses on the basis of the emotion it\u2019s sensing from your voice. As with other versions of ChatGPT, users can personalize the voice mode by asking the model to remember facts about themselves. The new mode also has improved\u00a0its pronunciation of words in\u00a0non-English languages. AI investor Allie Miller posted a demo of the tool in August, which highlighted a lot of the same strengths of OpenAI\u2019s own release videos: The model is fast and adept at changing its accent, tone, and content to match your needs.",
    "I\u00e2\u20ac\u2122m testing the new @OpenAI Advanced Voice Mode and I just snorted with laughter. In a good way. Watch the whole thing \u00e2\u00ac\u2021\u00ef\u00b8\u008f pic.twitter.com/vSOMzXdwZo\u2014 Allie K. Miller (@alliekmiller) August 2, 2024",
    "The update also adds new voices. Shortly after the launch of GPT-4o, OpenAI was criticized for the similarity between the female voice in its demo videos, named Sky, and that of Scarlett Johansson, who played an AI love interest in the movie Her. OpenAI then removed the voice.  Now it has launched five new voices, named Arbor, Maple, Sol, Spruce, and Vale, which will be available in both the standard and advanced voice modes. MIT Technology Review has not heard them yet, but OpenAI says they were made using professional voice actors from around the world. \u201cWe interviewed dozens of actors to find those with the qualities of voices we feel people will enjoy talking to for hours\u2014warm, approachable, inquisitive, with some rich texture and tone,\u201d a company spokesperson says.\u00a0 Who can access it and when? For now, OpenAI is rolling out access to Advanced Voice Mode to Plus users, who pay $20 per month for a premium version, and Team users, who pay $30 per month and have higher message limits. The next group to receive access will be those in the Enterprise and Edu tiers. The exact timing, though, is vague; an OpenAI spokesperson says the company will \u201cgradually roll out access to all Plus and Team users and will roll out to Enterprise and Edu tiers starting next week.\u201d The company hasn\u2019t committed to a firm deadline for when all users in these categories will have access. A message in the ChatGPT app indicates that all Plus users will have access by \u201cthe end of fall.\u201d There are geographic limitations. The new feature is not yet available in the EU, the UK, Switzerland, Iceland, Norway, or Liechtenstein.  There is no immediate plan to release Advanced Voice Mode to free users. (The standard mode remains available to all paid users.) Related StoryWhy OpenAI\u2019s new model is such a big dealThe bulk of LLM progress until now has been language-driven. This new model enters the realm of complex reasoning, with implications for physics, coding, and more.",
    "What steps have been taken to make sure it\u2019s safe? As the company noted upon the initial release in July and again emphasized this week, Advanced Voice Mode has been safety-tested by external experts \u201cwho collectively speak a total of 45 different languages, and represent 29 different geographies.\u201d The GPT-4o system card details how the underlying model handles issues like generating violent or erotic speech, imitating voices without their consent, or generating copyrighted content.\u00a0 Still, OpenAI\u2019s models are not open-source. Compared with such models, which are more transparent about their training data and the \u201cmodel weights\u201d that govern how the AI produces responses, OpenAI\u2019s closed-source models are harder for independent researchers to evaluate from the perspective of safety, bias, and harm. hide"
  ]
}