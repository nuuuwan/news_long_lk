{
  "url": "https://www.technologyreview.com/2024/10/01/1104744/why-bigger-is-not-always-better-in-ai/",
  "title": "Why bigger is not always better in AI",
  "ut": 1727739138.0,
  "body_paragraphs": [
    "This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. In AI research, everyone seems to think that bigger is better. The idea is that more data, more computing power, and more parameters will lead to models that are more powerful. This thinking started with a landmark paper from 2017, in which Google researchers introduced the transformer architecture underpinning today\u2019s language model boom and helped embed the \u201cscale is all you need\u201d mindset into the AI community. Today, big tech companies seem to be competing over scale above everything else.  \u201cIt\u2019s like, how big is your model, bro?\u201d says Sasha Luccioni, the AI and climate lead at the AI startup Hugging Face. Tech companies just add billions more parameters, which means an average person couldn\u2019t download the models and tinker with them, even if they were open-source (which they mostly aren\u2019t). The AI models of today are just \u201cway too big,\u201d she says.\u00a0\u00a0 With scale come a slew of problems, such as\u00a0invasive data-gathering practices\u00a0and child sexual abuse material in data sets, as Luccioni and coauthors detail in a\u00a0new paper. To top it off, bigger models also have a far bigger carbon footprint, because they require more energy to run.",
    "Another problem that scale brings is the extreme concentration of power, says Luccioni. Scaling up costs tons of money, and only elite researchers working in Big Tech have the resources to build and operate models like that.\u00a0 \u201cThere\u2019s this bottleneck that\u2019s created by a very small number of rich and powerful companies who use AI as part of their core product,\u201d she says.",
    "It doesn\u2019t have to be like this.\u00a0I just published a story on a new multimodal large language model that is small but mighty. Researchers at the Allen Institute for Artificial Intelligence (Ai2) built an open-source family of models called Molmo, which achieve impressive performance with a fraction of the resources used to build state-of-the-art models.\u00a0 The organization claims that its biggest Molmo model, which has 72 billion parameters, outperforms OpenAI\u2019s GPT-4o, which is estimated to have over a trillion parameters, in tests that measure things like understanding images, charts, and documents.\u00a0\u00a0 Meanwhile, Ai2 says a smaller Molmo model, with 7 billion parameters, comes close to OpenAI\u2019s state-of-the-art model in performance, an achievement it ascribes to vastly more efficient data collection and training methods.\u00a0Read more about it from me here. Molmo shows we don\u2019t need massive data sets and massive models that take tons of money and energy to train.\u00a0 Breaking out of the \u201cscale is all you need\u201d mindset was one of the biggest challenges for the researchers who built Molmo, says Ani Kembhavi, a senior director of research at Ai2.\u00a0  \u201cWhen we started this project, we were like, we have to think completely out of the box, because there has to be a better way to train models,\u201d he says. The team wanted to prove that open models can be as powerful as closed, proprietary ones, and that required them to build models that were accessible and didn\u2019t cost millions of dollars to train.\u00a0 Molmo shows that \u201cless is more, small is big, open [is as good as] closed,\u201d Kembhavi says.\u00a0 There\u2019s another good case for scaling down.\u00a0Bigger models tend to be able to do a wider range of things than end users actually need, says Luccioni.\u00a0 \u201cMost of the time, you don\u2019t need a model that does everything. You need a model that does a specific task that you want it to do. And for that, bigger models are not necessarily better,\u201d she says.",
    "Instead, we need to change the ways we measure AI performance to focus on things that actually matter, says Luccioni. For example, in a cancer detection algorithm, instead of using a model that can do all sorts of things and is trained on the internet, perhaps we should be prioritizing factors such as accuracy, privacy, or whether the model is trained on data that you can trust, she says.\u00a0 But that would require a higher level of transparency than is currently the norm in AI. Researchers don\u2019t really know how or why their models do what they do, and don\u2019t even really have a grasp of what goes into their data sets. Scaling is a popular technique because researchers have found that throwing more stuff at models seems to make them perform better. The research community and companies need to shift the incentives so that tech companies will be required to be more mindful and transparent about what goes into their models, and help us do more with less.\u00a0 \u201cYou don\u2019t need to assume [AI models] are a magic box and going to solve all your issues,\u201d she says.\u00a0  Now read the rest of The Algorithm Deeper Learning An AI script editor could help decide what films get made in Hollywood Every day across Hollywood, scores of people read through scripts on behalf of studios, trying to find the diamonds in the rough among the many thousands sent in every year.\u00a0Each script runs up to 150 pages, and it can take half a day to read one and write up a summary. With only about 50 of these scripts selling in a given year, readers are trained to be ruthless.\u00a0 Lights, camera, AI:\u00a0Now the tech company Cinelytic, which works with major studios like Warner Bros. and Sony Pictures, aims to offer script feedback with generative AI. It launched a new tool called Callaia that analyzes scripts. Using AI, it takes Callaia less than a minute to write its own \u201ccoverage,\u201d which includes a synopsis, a list of comparable films, grades for areas like dialogue and originality, and actor recommendations.\u00a0Read more from James O\u2019Donnell here. Bits and Bytes California\u2019s governor has vetoed the state\u2019s sweeping AI legislationGovernor Gavin Newsom vetoed SB 1047, a bill that required pre-deployment safety testing of large AI systems, and gave the state\u2019s attorney general the right to sue AI companies for serious harm. He said he thought the bill focused too much on the largest models without considering broader harms and risks. Critics of AI\u2019s rapid growth have expressed dismay at the decision. (The New York Times)\u00a0 Sorry, AI won\u2019t \u201cfix\u201d climate changeOpenAI\u2019s CEO Sam Altman claims AI will deliver an \u201cIntelligence Age,\u201d unleashing \u201cunimaginable\u201d prosperity and \u201castounding triumphs\u201d like \u201cfixing the climate.\u201d But tech breakthroughs alone can't solve global warming. In fact, as it stands, AI is making the problem much worse. (MIT Technology Review)",
    "How turning OpenAI into a real business is tearing it apartIn yet another organizational shakeup, the startup lost its CTO Mira Murati and other senior leaders. OpenAI is riddled with chaos that stems from its CEO\u2019s push to transform it from a nonprofit research lab into a for-profit organization. Insiders say this shift has \u201ccorrupted\u201d the company\u2019s culture. (The Wall Street Journal) Why Microsoft made a deal to help restart Three Mile IslandA once-shuttered nuclear plant could soon be used to power Microsoft\u2019s massive investment in AI development. (MIT Technology Review)",
    "OpenAI released its advanced voice mode to more people. Here\u2019s how to get it.The company says the updated version responds to your emotions and tone of voice, and allows you to interrupt it midsentence. (MIT Technology Review)\u00a0 The FTC is cracking down on AI scamsThe agency launched \u201cOperation AI Comply\u201d and says it will investigate AI-infused frauds and other types of deception, such as chatbots giving \u201clegal advice,\u201d AI tools that let people create fake online reviews, and false claims of huge earnings from AI-powered business opportunities.(The FTC)\u00a0 Want AI that flags hateful content? Build it.A new competition promises $10,000 in prizes to anyone who can track hateful images online. (MIT Technology Review)\u00a0 hide"
  ]
}