{
  "url": "https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/",
  "title": "A Chinese firm has just launched a constantly changing set of AI benchmarks",
  "ut": 1750659388.0,
  "body_paragraphs": [
    "When testing an AI model, it\u2019s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That\u2019s thanks to the way it evaluates models not only on the ability to pass arbitrary tests, like most other benchmarks, but also on the ability to execute real-world tasks, which is more unusual. It will be updated on a regular basis to try to keep it evergreen.\u00a0 This week the company is making part of its question set open-source and letting anyone use for free. The team has also released a leaderboard comparing how mainstream AI models stack up when tested on Xbench. (ChatGPT o3 ranked first across all categories, though ByteDance\u2019s Doubao, Gemini 2.5 Pro, and Grok all still did pretty well, as did Claude Sonnet.)\u00a0 Development of the benchmark at HongShan began in 2022, following ChatGPT\u2019s breakout success, as an internal tool for assessing which models are worth investing in. Since then, led by partner Gong Yuan, the team has steadily expanded the system, bringing in outside researchers and professionals to help refine it. As the project grew more sophisticated, they decided to release it to the public. Xbench approached the problem with two different systems. One is similar to traditional benchmarking: an academic test that gauges a model\u2019s aptitude on various subjects. The other is more like a technical interview round for a job, assessing how much real-world economic value a model might deliver.",
    "Xbench\u2019s methods for assessing raw intelligence currently include two components: Xbench-ScienceQA and Xbench-DeepResearch. ScienceQA isn\u2019t a radical departure from existing postgraduate-level STEM benchmarks like GPQA and SuperGPQA. It includes questions spanning fields from biochemistry to orbital mechanics, drafted by graduate students and double-checked by professors. Scoring rewards not only the right answer but also the reasoning chain that leads to it. DeepResearch, by contrast, focuses on a model\u2019s ability to navigate the Chinese-language web. Ten subject-matter experts created 100 questions in music, history, finance, and literature\u2014questions that can\u2019t just be googled but require significant research to answer. Scoring favors breadth of sources, factual consistency, and a model\u2019s willingness to admit when there isn\u2019t enough data. A question in the publicized collection is \u201cHow many Chinese cities in the three northwestern provinces border a foreign country?\u201d (It\u2019s 12, and only 33% of models tested got it right, if you are wondering.)",
    "On the company\u2019s website, the researchers said they want to add more dimensions to the test\u2014for example, aspects like how creative a model is in its problem solving, how collaborative it is when working with other models, and how reliable it is. The team has committed to updating the test questions once a quarter and to maintain a half-public, half-private data set. To assess models\u2019 real-world readiness, the team worked with experts to develop tasks modeled on actual workflows, initially in recruitment and marketing. For example, one task asks a model to source five qualified battery engineer candidates and justify each pick. Another asks it to match advertisers with appropriate short-video creators from a pool of over 800 influencers. The website also teases upcoming categories, including finance, legal, accounting, and design. The question sets for these categories have not yet been open-sourced. ChatGPT-o3 again ranks first in both of the current professional categories. For recruiting, Perplexity Search and Claude 3.5 Sonnet take second and third place, respectively. For marketing, Claude, Grok, and Gemini all perform well. \u201cIt is really difficult for benchmarks to include things that are so hard to quantify,\u201d says Zihan Zheng, the lead researcher on a new benchmark called LiveCodeBench Pro and a student at NYU. \u201cBut Xbench represents a promising start.\u201d hide"
  ]
}