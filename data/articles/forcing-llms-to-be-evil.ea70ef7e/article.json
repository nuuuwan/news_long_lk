{
  "url": "https://www.technologyreview.com/2025/08/01/1120924/forcing-llms-to-be-evil-during-training-can-make-them-nicer-in-the-long-run/",
  "title": "Forcing LLMs to be evil during training can make them nicer in the long run",
  "ut": 1754029800.0,
  "body_paragraphs": [
    "A new study from Anthropic suggests that traits such as sycophancy or evilness are associated with specific patterns of activity in large language models\u2014and turning on those patterns during training can, paradoxically, prevent the model from adopting the related traits. Large language models have recently acquired a reputation for behaving badly. In April, ChatGPT suddenly became an aggressive yes-man, as opposed to the moderately sycophantic version that users were accustomed to\u2014it endorsed harebrained business ideas, waxed lyrical about users\u2019 intelligence, and even encouraged people to go off their psychiatric medication. OpenAI quickly rolled back the change and later published a postmortem on the mishap. More recently, xAI\u2019s Grok adopted what can best be described as a 4chan neo-Nazi persona and repeatedly referred to itself as \u201cMechaHitler\u201d on X. That change, too, was quickly reversed.  Jack Lindsey, a member of the technical staff at Anthropic who led the new project, says that this study was partly inspired by seeing models adopt harmful traits in such instances. \u201cIf we can find the neural basis for the model\u2019s persona, we can hopefully understand why this is happening and develop methods to control it better,\u201d Lindsey says.\u00a0 The idea of LLM \u201cpersonas\u201d or \u201cpersonalities\u201d can be polarizing\u2014for some researchers the terms inappropriately anthropomorphize language models, whereas for others they effectively capture the persistent behavioral patterns that LLMs can exhibit. \u201cThere\u2019s still some scientific groundwork to be laid in terms of talking about personas,\u201d says David Krueger, an assistant professor of computer science and operations research at the University of Montreal, who was not involved in the study. \u201cI think it is appropriate to sometimes think of these systems as having personas, but I think we have to keep in mind that we don\u2019t actually know if that's what\u2019s going on under the hood.\u201d",
    "Related StoryOpenAI can rehabilitate AI models that develop a \u201cbad-boy persona\u201dResearchers at the company looked into how malicious fine-tuning makes a model go rogue, and how to turn it back.",
    "For this study, Lindsey and his colleagues worked to lay down some of that groundwork. Previous research has shown that various dimensions of LLMs\u2019 behavior\u2014from whether they are talking about weddings to persistent traits such as sycophancy\u2014are associated with specific patterns of activity in the simulated neurons that constitute LLMs. Those patterns can be written down as a long string of numbers, in which each number represents how active a specific neuron is when the model is expressing that behavior. Here, the researchers focused on sycophantic, \u201cevil\u201d, and hallucinatory personas\u2014three types that LLM designers might want to avoid in their models. To identify those patterns, the team devised a fully automated pipeline that can map out that pattern given a brief text description of a persona. Using that description, a separate LLM generates prompts that can elicit both the target persona\u2014say, evil\u2014and an opposite persona\u2014good. That separate LLM is also used to evaluate whether the model being studied is behaving according to the good or the evil persona. To identify the evil activity pattern, the researchers subtract the model\u2019s average activity in good mode from its average activity in evil mode.",
    "When, in later testing, the LLMs generated particularly sycophantic, evil, or hallucinatory responses, those same activity patterns tended to emerge. That\u2019s a sign that researchers could eventually build a system to track those patterns and alert users when their LLMs are sucking up to them or hallucinating, Lindsey says. \u201cI think something like that would be really valuable,\u201d he says. \u201cAnd that\u2019s kind of where I\u2019m hoping to get.\u201d Just detecting those personas isn\u2019t enough, however. Researchers want to stop them from emerging in the first place. But preventing unsavory LLM behavior is tough. Many LLMs learn from human feedback, which trains them to behave in line with user preference\u2014but can also push them to become excessively obsequious. And recently, researchers have documented a phenomenon called \u201cemergent misalignment,\u201d in which models trained on incorrect solutions to math problems or buggy code extracts somehow also learn to produce unethical responses to a wide range of user queries. Other researchers have tested out an approach called \u201csteering,\u201d in which activity patterns within LLMs are deliberately stimulated or suppressed in order to elicit or prevent the corresponding behavior. But that approach has a couple of key downsides. Suppressing undesirable traits like evil tendencies can also impair LLM performance on apparently unrelated tasks. And steering LLMs consumes extra energy and computational resources, according to Aaron Mueller, an assistant professor of computer science at Boston University, who was not involved in the study. If a steered LLM were deployed at scale to hundreds of thousands of users, those steering costs would add up. So the Anthropic team experimented with a different approach. Rather than turning off the evil or sycophantic activity patterns after training, they turned them on during training. When they trained those models on mistake-ridden data sets that would normally spark evil behavior, they instead remained as helpful and harmless as ever. That result might seem surprising\u2014how would forcing the model to be evil while it was learning prevent it from being evil down the line? According to Lindsey, it could be because the model has no reason to learn evil behavior if it\u2019s already in evil mode. \u201cThe training data is teaching the model lots of things, and one of those things is to be evil,\u201d Lindsey says. \u201cBut it\u2019s also teaching the model a bunch of other things. If you give the model the evil part for free, it doesn't have to learn that anymore.\u201d Unlike post-training steering, this approach didn\u2019t compromise the model\u2019s performance on other tasks. And it would also be more energy efficient if deployed widely. Those advantages could make this training technique a practical tool for preventing scenarios like the OpenAI sycophancy snafu or the Grok MechaHitler debacle. There\u2019s still more work to be done before this approach can be used in popular AI chatbots like ChatGPT and Claude\u2014not least because the models that the team tested in this study were much smaller than the models that power those chatbots. \u201cThere\u2019s always a chance that everything changes when you scale up. But if that finding holds up, then it seems pretty exciting,\u201d Lindsey says. \u201cDefinitely the goal is to make this ready for prime time.\u201d hide"
  ]
}