{
  "url": "https://www.technologyreview.com/2025/12/04/1128824/ai-chatbots-can-sway-voters-better-than-political-advertisements/",
  "title": "AI chatbots can sway voters better than political advertisements",
  "ut": 1764840297.0,
  "body_paragraphs": [
    "In 2024, a Democratic congressional candidate in Pennsylvania, Shamaine Daniels, used an AI chatbot named Ashley to call voters and carry on conversations with them. \u201cHello. My name is Ashley, and I\u2019m an artificial intelligence volunteer for Shamaine Daniels\u2019s run for Congress,\u201d the calls began. Daniels didn\u2019t ultimately win. But maybe those calls helped her cause: New research reveals that AI chatbots can shift voters\u2019 opinions in a single conversation\u2014and they\u2019re surprisingly good at it.\u00a0 A multi-university team of researchers has found that chatting with a politically biased AI model was more effective than political advertisements at nudging both Democrats and Republicans to support presidential candidates of the opposing party. The chatbots swayed opinions by citing facts and evidence, but they were not always accurate\u2014in fact, the researchers found, the most persuasive models said the most untrue things.\u00a0  The findings, detailed in a pair of studies published in the journals Nature and Science, are the latest in an emerging body of research demonstrating the persuasive power of LLMs. They raise profound questions about how generative AI could reshape elections.\u00a0 Related StoryChatbots are surprisingly effective at debunking conspiracy theoriesRead next \u201cOne conversation with an LLM has a pretty meaningful effect on salient election choices,\u201d says Gordon Pennycook, a psychologist at Cornell University who worked on the Nature study. LLMs can persuade people more effectively than political advertisements because they generate much more information in real time and strategically deploy it in conversations, he says.",
    "For the Nature paper, the researchers recruited more than 2,300 participants to engage in a conversation with a chatbot two months before the 2024 US presidential election. The chatbot, which was trained to advocate for either one of the top two candidates, was comparatively persuasive, especially when discussing candidates\u2019 policy platforms on issues such as the economy and health care. Donald Trump supporters who chatted with an AI model favoring Kamala Harris became slightly more inclined to support Harris, moving 3.9 points toward her on a 100-point scale. That was roughly four times the measured effect of political advertisements during the 2016 and 2020 elections. The AI model favoring Trump moved Harris supporters 2.3 points toward Trump.\u00a0 In similar experiments conducted during the lead-ups to the 2025 Canadian federal election and the 2025 Polish presidential election, the team found an even larger effect. The chatbots shifted opposition voters\u2019 attitudes by about 10 points.",
    "Long-standing theories of politically motivated reasoning hold that partisan voters are impervious to facts and evidence that contradict their beliefs. But the researchers found that the chatbots, which used a range of models including variants of GPT and DeepSeek, were more persuasive when they were instructed to use facts and evidence than when they were told not to do so. \u201cPeople are updating on the basis of the facts and information that the model is providing to them,\u201d says Thomas Costello, a psychologist at American University, who worked on the project.\u00a0 The catch is, some of the \u201cevidence\u201d and \u201cfacts\u201d the chatbots presented were untrue. Across all three countries, chatbots advocating for right-leaning candidates made a larger number of inaccurate claims than those advocating for left-leaning candidates. The underlying models are trained on vast amounts of human-written text, which means they reproduce real-world phenomena\u2014including \u201cpolitical communication that comes from the right, which tends to be less accurate,\u201d according to studies of partisan social media posts, says Costello. In the other study published this week, in Science, an overlapping team of researchers investigated what makes these chatbots so persuasive. They deployed 19 LLMs to interact with nearly 77,000 participants from the UK on more than 700 political issues while varying factors like computational power, training techniques, and rhetorical strategies.\u00a0 The most effective way to make the models persuasive was to instruct them to pack their arguments with facts and evidence and then give them additional training by feeding them examples of persuasive conversations. In fact, the most persuasive model shifted participants who initially disagreed with a political statement 26.1 points toward agreeing. \u201cThese are really large treatment effects,\u201d says Kobi Hackenburg, a research scientist at the UK AI Security Institute, who worked on the project.\u00a0  But optimizing persuasiveness came at the cost of truthfulness. When the models became more persuasive, they increasingly provided misleading or false information\u2014and no one is sure why. \u201cIt could be that as the models learn to deploy more and more facts, they essentially reach to the bottom of the barrel of stuff they know, so the facts get worse-quality,\u201d says Hackenburg. The chatbots\u2019 persuasive power could have profound consequences for the future of democracy, the authors note. Political campaigns that use AI chatbots could shape public opinion in ways that compromise voters\u2019 ability to make independent political judgments. Related StoryIt's surprisingly easy to stumble into a relationship with an AI chatbotRead nextStill, the exact contours of the impact remain to be seen. \u201cWe\u2019re not sure what future campaigns might look like and how they might incorporate these kinds of technologies,\u201d says Andy Guess, a political scientist at Princeton University. Competing for voters\u2019 attention is expensive and difficult, and getting them to engage in long political conversations with chatbots might be challenging. \u201cIs this going to be the way that people inform themselves about politics, or is this going to be more of a niche activity?\u201d he asks. Even if chatbots do become a bigger part of elections, it\u2019s not clear whether they\u2019ll do more to\u00a0 amplify truth or fiction. Usually, misinformation has an informational advantage in a campaign, so the emergence of electioneering AIs \u201cmight mean we\u2019re headed for a disaster,\u201d says Alex Coppock, a political scientist at Northwestern University. \u201cBut it\u2019s also possible that means that now, correct information will also be scalable.\u201d And then the question is who will have the upper hand. \u201cIf everybody has their chatbots running around in the wild, does that mean that we\u2019ll just persuade ourselves to a draw?\u201d Coppock asks. But there are reasons to doubt that. Politicians\u2019 access to the most persuasive models may not be evenly distributed. And voters across the political spectrum may have different levels of engagement with chatbots. If \u201csupporters of one candidate or party are more tech savvy than the other,\u201d Guess says, the persuasive impacts might not balance out. As people turn to AI to help them navigate their lives, they may also start asking chatbots for voting advice whether campaigns prompt the interaction or not. That may be a troubling world for democracy, unless there are strong guardrails to keep the systems in check. Auditing and documenting the accuracy of LLM outputs in conversations about politics may be a first step. hide"
  ]
}