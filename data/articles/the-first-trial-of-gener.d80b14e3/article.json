{
  "url": "https://www.technologyreview.com/2025/03/28/1114001/the-first-trial-of-generative-ai-therapy-shows-it-might-help-with-depression/",
  "title": "The first trial of generative AI therapy shows it might help with depression",
  "ut": 1743147455.0,
  "body_paragraphs": [
    "The first clinical trial of a therapy bot that uses generative AI suggests it was as effective as human therapy for participants with depression, anxiety, or risk for developing eating disorders. Even so, it doesn\u2019t give a go-ahead to the dozens of companies hyping such technologies while operating in a regulatory gray area.\u00a0 A team led by psychiatric researchers and psychologists at the Geisel School of Medicine at Dartmouth College built the tool, called Therabot, and the results were published on March 27 in NEJM AI, a journal by the New England Journal of Medicine. Many tech companies have built AI tools for therapy, promising that people can talk with a bot more frequently and cheaply than they can with a trained therapist\u2014and that this approach is safe and effective.  Many psychologists and psychiatrists have shared the vision, noting that fewer than half of people with a mental disorder receive therapy, and those who do might get only 45 minutes per week. Researchers have tried to build tech so that more people can access therapy, but they have been held back by two things.\u00a0 One, a therapy bot that says the wrong thing could result in real harm. That\u2019s why many researchers have built bots using explicit programming: The software pulls from a finite bank of approved responses (as was the case with Eliza, a mock-psychotherapist computer program built in the 1960s). But this makes them less engaging to chat with, and people lose interest. The second issue is that the hallmarks of good therapeutic relationships\u2014shared goals and collaboration\u2014are hard to replicate in software.",
    "In 2019, as early large language models like OpenAI\u2019s GPT were taking shape, the researchers at Dartmouth thought generative AI might help overcome these hurdles. They set about building an AI model trained to give evidence-based responses. They first tried building it from general mental-health conversations pulled from internet forums. Then they turned to thousands of hours of transcripts of real sessions with psychotherapists. \u201cWe got a lot of \u2018hmm-hmms,\u2019 \u2018go ons,\u2019 and then \u2018Your problems stem from your relationship with your mother,\u2019\u201d said Nick Jacobson, an associate professor of biomedical data science and psychiatry at Dartmouth and the study's senior author, in an interview. \u201cReally tropes of what psychotherapy would be, rather than actually what we\u2019d want.\u201d",
    "Dissatisfied, they set to work assembling their own custom data sets based on evidence-based practices, which is what ultimately went into the model. Many AI therapy bots on the market, in contrast, might be just slight variations of foundation models like Meta\u2019s Llama, trained mostly on internet conversations. That poses a problem, especially for topics like disordered eating. \u201cIf you were to say that you want to lose weight,\u201d Jacobson says, \u201cthey will readily support you in doing that, even if you will often have a low weight to start with.\u201d A human therapist wouldn\u2019t do that.\u00a0 To test the bot, the researchers ran an eight-week clinical trial with 210 participants who had symptoms of depression or generalized anxiety disorder or were at high risk for eating disorders. About half had access to Therabot, and a control group did not. Participants responded to prompts from the AI and initiated conversations, averaging about 10 messages per day. Participants with depression experienced a 51% reduction in symptoms, the best result in the study. Those with anxiety experienced a 31% reduction, and those at risk for eating disorders saw a 19% reduction in concerns about body image and weight. These measurements are based on self-reporting through surveys, a method that\u2019s not perfect but remains one of the best tools researchers have.  These results, Jacobson says, are about what one finds in randomized control trials of psychotherapy with 16 hours of human-provided treatment, but the Therabot trial accomplished it in about half the time. \u201cI\u2019ve been working in digital therapeutics for a long time, and I\u2019ve never seen levels of engagement that are prolonged and sustained at this level,\u201d he says. Related StoryAn AI companion site is hosting sexually charged conversations with underage celebrity botsOne chatbot on Botify AI that resembled the actor Jenna Ortega as a teenage Wednesday Addams told us that age-of-consent laws are \u201cmeant to be broken.\u201d",
    "Jean-Christophe B\u00e9lisle-Pipon, an assistant professor of health ethics at Simon Fraser University who has written about AI therapy bots but was not involved in the research, says the results are impressive but notes that just like any other clinical trial, this one doesn\u2019t necessarily represent how the treatment would act in the real world.\u00a0 \u201cWe remain far from a \u2018greenlight\u2019 for widespread clinical deployment,\u201d he wrote in an email. One issue is the supervision that wider deployment might require. During the beginning of the trial, Jacobson says, he personally oversaw all the messages coming in from participants (who consented to the arrangement) to watch out for problematic responses from the bot. If therapy bots needed this oversight, they wouldn\u2019t be able to reach as many people.",
    "I asked Jacobson if he thinks the results validate the burgeoning industry of AI therapy sites. \u201cQuite the opposite,\u201d he says, cautioning that most don\u2019t appear to train their models on evidence-based practices like cognitive behavioral therapy, and they likely don\u2019t employ a team of trained researchers to monitor interactions. \u201cI have a lot of concerns about the industry and how fast we\u2019re moving without really kind of evaluating this,\u201d he adds. When AI sites advertise themselves as offering therapy in a legitimate, clinical context, Jacobson says, it means they fall under the regulatory purview of the Food and Drug Administration. Thus far, the FDA has not gone after many of the sites. If it did, Jacobson says, \u201cmy suspicion is almost none of them\u2014probably none of them\u2014that are operating in this space would have the ability to actually get a claim clearance\u201d\u2014that is, a ruling backing up their claims about the benefits provided.\u00a0 B\u00e9lisle-Pipon points out that if these types of digital therapies are not approved and integrated into health-care and insurance systems, it will severely limit their reach. Instead, the people who would benefit from using them might seek emotional bonds and therapy from types of AI not designed for those purposes (indeed, new research from OpenAI suggests that interactions with its AI models have a very real impact on emotional well-being).\u00a0 \u201cIt is highly likely that many individuals will continue to rely on more affordable, nontherapeutic chatbots\u2014such as ChatGPT or Character.AI\u2014for everyday needs, ranging from generating recipe ideas to managing their mental health,\u201d he wrote.\u00a0 Correction: The study was published in NEJM AI, a journal by the New England Journal of Medicine, not in the New England Journal of Medicine as originally stated. The story was also updated with correct attributions of quotes to Nick Jacobson, rather than co-author Michael Heinz.  hide"
  ]
}