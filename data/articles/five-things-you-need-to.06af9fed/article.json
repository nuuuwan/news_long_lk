{
  "url": "https://www.technologyreview.com/2025/07/22/1120556/five-things-to-know-ai/",
  "title": "Five things you need to know about AI right now",
  "ut": 1753143627.0,
  "body_paragraphs": [
    "Last month I gave a talk at SXSW London called \u201cFive things you need to know about AI\u201d\u2014my personal picks for the five most important ideas in AI right now.\u00a0 I aimed the talk at a general audience, and it serves as a quick tour of how I\u2019m thinking about AI in 2025. I\u2019m sharing it here in case you\u2019re interested. I think the talk has something for everyone. There\u2019s some fun stuff in there. I even make jokes!  The\u00a0video\u00a0is now available (thank you, SXSW London).\u00a0Below is a quick look at my top five. Let me know if you would have picked different ones! 1. Generative AI is now so good it\u2019s scary. Maybe you think that\u2019s obvious. But I am constantly having to check my assumptions about how fast this technology is progressing\u2014and it\u2019s my job to keep up.\u00a0A few months ago, my colleague\u2014and your regular Algorithm writer\u2014James O\u2019Donnell shared 10 music tracks with the\u00a0MIT Technology Review\u00a0editorial team and challenged us to pick which ones had been produced using generative AI and which had been made by people. Pretty much everybody did worse than chance.What\u2019s happening with music is happening across media, from code to robotics to protein synthesis to video. Just look at what people are doing with new video-generation tools like Google DeepMind\u2019s Veo 3. And this technology is being put into everything.My point here? Whether you think AI is the best thing to happen to us or the worst, do not underestimate it. It\u2019s good, and it\u2019s getting better.",
    "2. Hallucination is a feature, not a bug. Let\u2019s not forget the fails. When AI makes up stuff, we call it hallucination. Think of customer service bots offering nonexistent refunds, lawyers submitting briefs filled with nonexistent cases, or RFK Jr.\u2019s government department publishing a report that cites nonexistent academic papers.\u00a0You\u2019ll hear a lot of talk that makes hallucination sound like it\u2019s a problem we need to fix. The more accurate way to think about hallucination is that this is exactly what generative AI does\u2014what it\u2019s meant to do\u2014all the time. Generative models are trained to make things up.What\u2019s remarkable is not that they make up nonsense, but that the nonsense they make up so often matches reality.\u00a0Why does this matter? First, we need to be aware of what this technology can and can\u2019t do. But also: Don\u2019t hold out for a future version that doesn\u2019t hallucinate. Related StoryWhat is AI?Everyone thinks they know, but no one can agree. And that\u2019s a problem.",
    "3. AI is power hungry and getting hungrier. You\u2019ve probably heard that AI is power hungry. But a lot of that reputation comes from the amount of electricity it takes to train these giant models, though giant models only get trained every so often.What\u2019s changed is that these models are now being used by hundreds of millions of people every day.\u00a0And while using a model takes far less energy than training one, the energy costs ramp up massively with those kinds of user numbers.\u00a0ChatGPT, for example, has 400 million weekly users. That makes it the fifth-most-visited website in the world, just after Instagram and ahead of X. Other chatbots are catching up.\u00a0So it\u2019s no surprise that tech companies are racing to build new data centers in the desert and revamp power grids.The truth is we\u2019ve been in the dark about exactly how much energy it takes to fuel this boom because none of the major companies building this technology have shared much information about it.\u00a0That\u2019s starting to change, however. Several of my colleagues spent months working with researchers to crunch the numbers for some open source versions of this tech. (Do check out what they found.)",
    "4. Nobody knows exactly how large language models work. Sure, we know how to build them. We know how to make them work really well\u2014see no. 1 on this list.But how they do what they do is still an unsolved mystery. It\u2019s like these things have arrived from outer space and scientists are poking and prodding them from the outside to figure out what they really are.It\u2019s incredible to think that never before has a mass-market technology used by billions of people been so little understood.Why does that matter? Well, until we understand them better we won\u2019t know exactly what they can and can\u2019t do. We won\u2019t know how to control their behavior. We won\u2019t fully understand hallucinations. 5. AGI doesn\u2019t mean anything. Not long ago, talk of AGI was fringe, and mainstream researchers were embarrassed to bring it up.\u00a0But as AI has got better and far more lucrative, serious people are happy to insist they\u2019re about to create it. Whatever it is.AGI\u2014or artificial general intelligence\u2014has come to mean something like: AI that can match the performance of humans on a wide range of cognitive tasks.But what does that mean? How do we measure performance? Which humans? How wide a range of tasks? And performance on cognitive tasks is just another way of saying intelligence\u2014so the definition is circular anyway.Essentially, when people refer to AGI they now tend to just mean AI, but better than what we have today.There\u2019s this absolute faith in the progress of AI. It\u2019s gotten better in the past, so it will continue to get better. But there is zero evidence that this will actually play out.\u00a0So where does that leave us? We are building machines that are getting very good at mimicking some of the things people do, but the technology still has serious flaws. And we\u2019re only just figuring out how it actually works. Here\u2019s how I think about AI: We have built machines with humanlike behavior, but we haven\u2019t shrugged off the habit of imagining a humanlike mind behind them. This leads to exaggerated assumptions about what AI can do and plays into the wider culture wars between techno-optimists and techno-skeptics.It\u2019s right to be amazed by this technology. It\u2019s also right to be skeptical of many of the things said about it. It\u2019s still very early days, and it\u2019s all up for grabs. This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. hide"
  ]
}