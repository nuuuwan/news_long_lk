{
  "url": "https://www.technologyreview.com/2026/02/02/1132068/what-weve-been-getting-wrong-about-ais-truth-crisis/",
  "title": "What we\u2019ve been getting wrong about AI\u2019s truth crisis",
  "ut": 1770017997.0,
  "body_paragraphs": [
    "This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. What would it take to convince you that the era of truth decay we were long warned about\u2014where AI content dupes us, shapes our beliefs even when we catch the lie, and erodes societal trust in the process\u2014is now here? A story I published last week pushed me over the edge. It also made me realize that the tools we were sold as a cure for this crisis are failing miserably.\u00a0  On Thursday, I reported the first confirmation that the US Department of Homeland Security, which houses immigration agencies, is using AI video generators from Google and Adobe to make content that it shares with the public. The news comes as immigration agencies have flooded social media with content to support President Trump's mass deportation agenda\u2014some of which appears to be made with AI (like a video about \u201cChristmas after mass deportations\u201d). But I received two types of reactions from readers that may explain just as much about the epistemic crisis we\u2019re in.",
    "One was from people who weren\u2019t surprised, because on January 22 the White House had posted a digitally altered photo of a woman arrested at an ICE protest, one that made her appear hysterical and in tears. Kaelan Dorr, the White House\u2019s deputy communications director, did not respond to questions about whether the White House altered the photo but wrote, \u201cThe memes will continue.\u201d The second was from readers who saw no point in reporting that DHS was using AI to edit content shared with the public, because news outlets were apparently doing the same. They pointed to the fact that the news network MS Now (formerly MSNBC) shared an image of Alex Pretti that was AI-edited and appeared to make him look more handsome, a fact that led to many viral clips this week, including one from Joe Rogan\u2019s podcast. Fight fire with fire, in other words? A spokesperson for MS Now told Snopes that the news outlet aired the image without knowing it was edited.",
    "There is no reason to collapse these two cases of altered content into the same category, or to read them as evidence that truth no longer matters. One involved the US government sharing a clearly altered photo with the public and declining to answer whether it was intentionally manipulated; the other involved a news outlet airing a photo it should have known was altered but taking some steps to disclose the mistake. What these reactions reveal instead is a flaw in how we were collectively preparing for this moment. Warnings about the AI truth crisis revolved around a core thesis: that not being able to tell what is real will destroy us, so we need tools to independently verify the truth. My two grim takeaways are that these tools are failing, and that while vetting the truth remains essential, it is no longer capable on its own of producing the societal trust we were promised. Related StoryInside the marketplace powering bespoke AI deepfakes of real womenRead next For example, there was plenty of hype in 2024 about the Content Authenticity Initiative, cofounded by Adobe and adopted by major tech companies, which would attach labels to content disclosing when it was made, by whom, and whether AI was involved. But Adobe applies automatic labels only when the content is wholly AI-generated. Otherwise the labels are opt-in on the part of the creator. And platforms like X, where the altered arrest photo was posted, can strip content of such labels anyway (a note that the photo was altered was added by users). Platforms can also simply not choose to show the label; indeed, when Adobe launched the initiative, it noted that the Pentagon's website for sharing official images, DVIDS, would display the labels to prove authenticity, but a review of the website today shows no such labels. Noticing how much traction the White House\u2019s photo got even after it was shown to be AI-altered, I was struck by the findings of a very relevant new paper published in the journal Communications Psychology. In the study, participants watched a deepfake \u201cconfession\u201d to a crime, and the researchers found that even when they were told explicitly that the evidence was fake, participants relied on it when judging an individual\u2019s guilt. In other words, even when people learn that the content they\u2019re looking at is entirely fake, they remain emotionally swayed by it.\u00a0 \u201cTransparency helps, but it isn\u2019t enough on its own,\u201d the disinformation expert Christopher Nehring wrote recently about the study\u2019s findings. \u201cWe have to develop a new masterplan of what to do about deepfakes.\u201d AI tools to generate and edit content are getting more advanced, easier to operate, and cheaper to run\u2014all reasons why the US government is increasingly paying to use them. We were well warned of this, but we responded by preparing for a world in which the main danger was confusion. What we\u2019re entering instead is a world in which influence survives exposure, doubt is easily weaponized, and establishing the truth does not serve as a reset button. And the defenders of truth are already trailing way behind. Update: This story was updated on February 2 with details about how Adobe applies its content authenticity labels. hide"
  ]
}