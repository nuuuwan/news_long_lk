{
  "url": "https://www.technologyreview.com/2025/08/28/1122685/ai-energy-use-gemini/",
  "title": "3 problems with Google\u2019s AI energy use data",
  "ut": 1756341000.0,
  "body_paragraphs": [
    "Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That\u2019s about the same as running a microwave for one second\u2014something that, to me, feels virtually insignificant. I run the microwave for so many more seconds than that on most days. I was excited to see this report come out, and I welcome more openness from major players in AI about their estimated energy use per query. But I\u2019ve noticed that some folks are taking this number and using it to conclude that we don\u2019t need to worry about AI\u2019s energy demand. That\u2019s not the right takeaway here. Let\u2019s dig into why.  1. This one number doesn\u2019t reflect all queries, and it leaves out cases that likely use much more energy. Google\u2019s new report considers only text queries. Previous analysis, including MIT Technology Review\u2019s reporting, suggests that generating a photo or video will typically use more electricity.",
    "When I spoke with Jeff Dean, Google\u2019s chief scientist, he said the company doesn\u2019t currently have plans to do this sort of analysis for images and videos, but that he wouldn\u2019t rule it out. Related StoryIn a first, Google has released data on how much energy an AI prompt usesIt\u2019s the most transparent estimate yet from one of the big AI companies, and a long-awaited peek behind the curtain for researchers.",
    "The reason the company started with text prompts is that those are something many people out there are using in their daily lives, he says, while image and video generation is something that not as many people are doing. But I\u2019m seeing more AI images and videos all over my social feeds. So there\u2019s a whole world of queries not represented here.",
    "Also, this estimate is the median, meaning it\u2019s just the number in the middle of the range of queries Google is seeing. Longer questions and responses can push up the energy demand, and so can using a reasoning model.\u00a0 We don\u2019t know anything about how much energy these more complicated queries demand or what the distribution of the range is. 2. We don\u2019t know how many queries Gemini is seeing, so we don\u2019t know the product\u2019s total energy impact. One of my biggest outstanding questions about Gemini\u2019s energy use is the total number of queries the product is seeing every day.\u00a0 This number isn\u2019t included in Google\u2019s report, and the company wouldn\u2019t share it with me. And let me be clear: I absolutely pestered them about this, both in a press call they had about the news and in my interview with Dean. In the press call, the company pointed me to a recent earnings report, which includes only figures about monthly active users (450 million, for what it\u2019s worth).  \u201cWe\u2019re not comfortable revealing that for various reasons,\u201d Dean told me on our call. The total number is an abstract measure that changes over time, he says, adding that the company wants users to be thinking about the energy usage per prompt. But there are people out there all over the world interacting with this technology, not just me\u2014and what we all add up to seems quite relevant. OpenAI does publicly share its total, sharing recently that it sees 2.5 billion queries to ChatGPT every day. So for the curious, we can use this as an example and take the company\u2019s self-reported average energy use per query (0.34 watt-hours) to get a rough idea of the total for all people prompting ChatGPT. According to my math, over the course of a year, that would add up to over 300 gigawatt-hours\u2014the same as powering nearly 30,000 US homes annually. When you put it that way, it starts to sound like a lot of seconds in microwaves.",
    "3. AI is everywhere, not just in chatbots, and we\u2019re often not even conscious of it. AI is touching our lives even when we\u2019re not looking for it. AI summaries appear in web searches, whether you ask for them or not. There are built-in features for email and texting applications that that can draft or summarize messages for you. Google\u2019s estimate is strictly for Gemini apps and wouldn\u2019t include many of the other ways that even this one company is using AI. So even if you\u2019re trying to think about your own personal energy demand, it\u2019s increasingly difficult to tally up.\u00a0 To be clear, I don\u2019t think people should feel guilty for using tools that they find genuinely helpful. And ultimately, I don\u2019t think the most important conversation is about personal responsibility.\u00a0 There\u2019s a tendency right now to focus on the small numbers, but we need to keep in mind what this is all adding up to. Over two gigawatts of natural gas will need to come online in Louisiana to power a single Meta data center this decade. Google Cloud is spending $25 billion on AI just in the PJM grid on the US East Coast. By 2028, AI could account for 326 terawatt-hours of electricity demand in the US annually, generating over 100 million metric tons of carbon dioxide. We need more reporting from major players in AI, and Google\u2019s recent announcement is one of the most transparent accounts yet. But one small number doesn\u2019t negate the ways this technology is affecting communities and changing our power grid.\u00a0 This article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here. hide"
  ]
}