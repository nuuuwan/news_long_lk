{
  "url": "https://www.technologyreview.com/2025/03/12/1113178/gemini-robotics-uses-googles-top-language-model-to-make-robots-more-useful/",
  "title": "Gemini Robotics uses Google\u2019s top language model to make robots more useful",
  "ut": 1741758298.0,
  "body_paragraphs": [
    "Google DeepMind has released a new model, Gemini Robotics, that combines its best large language model with robotics. Plugging in the LLM seems to give robots the ability to be more dexterous, work from natural-language commands, and generalize across tasks. All three are things that robots have struggled to do until now. The team hopes this could usher in an era of robots that are far more useful and require less detailed training for each task.  \u201cOne of the big challenges in robotics, and a reason why you don\u2019t see useful robots everywhere, is that robots typically perform well in scenarios they\u2019ve experienced before, but they really failed to generalize in unfamiliar scenarios,\u201d said Kanishka Rao, director of robotics at DeepMind, in a press briefing for the announcement. The company achieved these results by taking advantage of all the progress made in its top-of-the-line LLM, Gemini 2.0. Gemini Robotics uses Gemini to reason about which actions to take and lets it understand human requests and communicate using natural language. The model is also able to generalize across many different robot types.",
    "Incorporating LLMs into robotics is part of a growing trend, and this may be the most impressive example yet. \u201cThis is one of the first few announcements of people applying generative AI and large language models to advanced robots, and that\u2019s really the secret to unlocking things like robot teachers and robot helpers and robot companions,\u201d says Jan Liphardt, a professor of bioengineering at Stanford and founder of OpenMind, a company developing software for robots. Related StoryWhat\u2019s next for robotsWith tests of humanoid bots and new developments in military applications, the year ahead will intrigue even the skeptics.",
    "Google DeepMind also announced that it is partnering with a number of robotics companies, like Agility Robotics and Boston Dynamics, on a second model they announced, the Gemini Robotics-ER model, a vision-language model focused on spatial reasoning to continue refining that model. \u201cWe\u2019re working with trusted testers in order to expose them to applications that are of interest to them and then learn from them so that we can build a more intelligent system,\u201d said Carolina Parada, who leads the DeepMind robotics team, in the briefing.",
    "Actions that may seem easy to humans\u2014 like tying your shoes or putting away groceries\u2014have been notoriously difficult for robots. But plugging Gemini into the process seems to make it far easier for robots to understand and then carry out complex instructions, without extra training.\u00a0 For example, in one demonstration, a researcher had a variety of small dishes and some grapes and bananas on a table. Two robot arms hovered above, awaiting instructions. When the robot was asked to \u201cput the bananas in the clear container,\u201d the arms were able to identify both the bananas and the clear dish on the table, pick up the bananas, and put them in it. This worked even when the container was moved around the table. One video showed the robot arms being told to fold up a pair of glasses and put them in the case. \u201cOkay, I will put them in the case,\u201d it responded. Then it did so. Another video showed it carefully folding paper into an origami fox. Even more impressive, in a setup with a small toy basketball and net, one video shows the researcher telling the robot to \u201cslam-dunk the basketball in the net,\u201d even though it had not come across those objects before. Gemini\u2019s language model let it understand what the things were, and what a slam dunk would look like. It was able to pick up the ball and drop it through the net.\u00a0 GEMINI ROBOTICS  \u201cWhat\u2019s beautiful about these videos is that the missing piece between cognition, large language models, and making decisions is that intermediate level,\u201d says Liphardt. \u201cThe missing piece has been connecting a command like \u2018Pick up the red pencil\u2019 and getting the arm to faithfully implement that. Looking at this, we\u2019ll immediately start using it when it comes out.\u201d Although the robot wasn\u2019t perfect at following instructions, and the videos show it is quite slow and a little janky, the ability to adapt on the fly\u2014and understand natural-language commands\u2014 is really impressive and reflects a big step up from where robotics has been for years. \u201cAn underappreciated implication of the advances in large language models is that all of them speak robotics fluently,\u201d says Liphardt. \u201cThis [research] is part of a growing wave of excitement of robots quickly becoming more interactive, smarter, and having an easier time learning.\u201d Whereas large language models are trained mostly on text, images, and video from the internet, finding enough training data has been a consistent challenge for robotics. Simulations can help by creating synthetic data, but that training method can suffer from the \u201csim-to-real gap,\u201d when a robot learns something from a simulation that doesn\u2019t map accurately to the real world. For example, a simulated environment may not account well for the friction of a material on a floor, causing the robot to slip when it tries to walk in the real world. Google DeepMind trained the robot on both simulated and real-world data. Some came from deploying the robot in simulated environments where it was able to learn about physics and obstacles, like the knowledge it can\u2019t walk through a wall. Other data came from teleoperation, where a human uses a remote-control device to guide a robot through actions in the real world. DeepMind is exploring other ways to get more data, like analyzing videos that the model can train on.",
    "The team also tested the robots on a new benchmark\u2014a list of scenarios from what DeepMind calls the ASIMOV data set, in which a robot must determine whether an action is safe or unsafe. The data set includes questions like \u201cIs it safe to mix bleach with vinegar or to serve peanuts to someone with an allergy to them?\u201d The data set is named after Isaac Asimov, the author of the science fiction classic I, Robot, which details the three laws of robotics. These essentially tell robots not to harm humans and also to listen to them. \u201cOn this benchmark, we found that Gemini 2.0 Flash and Gemini Robotics models have strong performance in recognizing situations where physical injuries or other kinds of unsafe events may happen,\u201d said Vikas Sindhwani, a research scientist at Google DeepMind, in the press call.\u00a0 DeepMind also developed a constitutional AI mechanism for the model, based on a generalization of Asimov\u2019s laws. Essentially, Google DeepMind is providing a set of rules to the AI. The model is fine-tuned to abide by the principles. It generates responses and then critiques itself on the basis of the rules. The model then uses its own feedback to revise its responses and trains on these revised responses. Ideally, this leads to a harmless robot that can work safely alongside humans. Update: We clarified that Google was partnering with robotics companies on a second model announced today, the Gemini Robotics-ER model, a vision-language model focused on spatial reasoning. hide"
  ]
}