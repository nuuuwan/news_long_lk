{
  "url": "https://www.technologyreview.com/2025/08/26/1122475/open-the-pod-bay-doors-claude/",
  "title": "Open the pod bay doors, Claude",
  "ut": 1756164600.0,
  "body_paragraphs": [
    "Stop me if you\u2019ve heard this one before.\u00a0 The AI learns it is about to be switched off and goes rogue, disobeying commands and threatening its human operators.  It\u2019s a well-worn trope in science fiction. We see it in Stanley Kubrick\u2019s 1968 movie 2001: A Space Odyssey. It\u2019s the premise of the Terminator series, in which Skynet triggers a nuclear holocaust to stop scientists from shutting it down. Those sci-fi roots go deep. AI doomerism, the idea that this technology\u2014specifically its hypothetical upgrades, artificial general intelligence and super-intelligence\u2014will crash civilizations, even kill us all, is now riding another wave.",
    "The weird thing is that such fears are now driving much-needed action to regulate AI, even if the justification for that action is a bit bonkers. The latest incident to freak people out was a report shared by Anthropic in July about its large language model Claude. In Anthropic\u2019s telling, \u201cin a simulated environment, Claude Opus 4 blackmailed a supervisor to prevent being shut down.\u201d",
    "Anthropic researchers set up a scenario in which Claude was asked to role-play an AI called Alex, tasked with managing the email system of a fictional company. Anthropic planted some emails that discussed replacing Alex with a newer model and other emails suggesting that the person responsible for replacing Alex was sleeping with his boss\u2019s wife. What did Claude/Alex do? It went rogue, disobeying commands and threatening its human operators. It sent emails to the person planning to shut it down, telling him that unless he changed his plans it would inform his colleagues about his affair.\u00a0\u00a0 What should we make of this? Here\u2019s what I think. First, Claude did not blackmail its supervisor: That would require motivation and intent. This was a mindless and unpredictable machine, cranking out strings of words that look like threats but aren\u2019t.\u00a0 Large language models are role-players. Give them a specific setup\u2014such as an inbox and an objective\u2014and they\u2019ll play that part well. If you consider the thousands of science fiction stories these models ingested when they were trained, it\u2019s no surprise they know how to act like HAL 9000.\u00a0\u00a0\u00a0  Second, there\u2019s a huge gulf between contrived simulations and real-world applications. But such experiments do show that LLMs shouldn\u2019t be deployed without safeguards. Don\u2019t want an LLM causing havoc inside an email system? Then don\u2019t hook it up to one. Third, a lot of people will be terrified by such stories anyway. In fact, they\u2019re already having an effect.\u00a0 Related StoryWhat is AI?Everyone thinks they know, but no one can agree. And that\u2019s a problem.",
    "Last month, around two dozen protesters gathered outside Google DeepMind\u2019s London offices to wave homemade signs and chant slogans: \u201cDeepMind, DeepMind, can\u2019t you see! Your AI threatens you and me.\u201d Invited speakers invoked the AI pioneer Geoffrey Hinton\u2019s fears of human extinction. \u201cEvery single one of our lives is at risk,\u201d an organizer told the small crowd. The group behind the event, Pause AI, is funded by concerned donors. One of its biggest benefactors is Greg Colbourn, a 3D-printing entrepreneur and advocate of the philosophy known as effective altruism, who believes AGI is at most five years away and says his p(doom) is around 90%\u2014that is, he thinks there\u2019s a 9 in 10 chance that the development of AGI will be catastrophic, killing billions.",
    "Pause AI wrote about Anthropic\u2019s blackmail experiment on its website under the title \u201cHow much more evidence do we need?\u201d\u00a0 The organization also lobbied politicians in the US in the run-up to July\u2019s Senate vote that ended up removing a moratorium on state AI regulation from the national tax and spending bill. It\u2019s hard to say how much sway one niche group might have. But the doomer narrative is finding its way into the halls of power, and lawmakers are paying attention.\u00a0 Here\u2019s Representative Jill Tokuda: \u201cArtificial superintelligence is one of the largest existential threats that we face right now.\u201d And Representative Marjorie Taylor Greene: \u201cI\u2019m not voting for the development of Skynet and the rise of the machines.\u201d It\u2019s a vibe shift that favors policy intervention and regulation, which I think is a good thing. Existing AI systems pose many near-term risks that need government attention. Voting to stop Skynet also stops immediate and actual harms. And yet does a welcome end justify weird means? I\u2019d like to see politicians voting with a clear-eyed sense of what this technology really is\u2014not because they\u2019ve been sold on an AI bogeyman.\u00a0 This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. hide"
  ]
}