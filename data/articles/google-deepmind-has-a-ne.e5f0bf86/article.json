{
  "url": "https://www.technologyreview.com/2024/11/14/1106871/google-deepmind-has-a-new-way-to-look-inside-an-ais-mind/",
  "title": "Google DeepMind has a new way to look inside an AI\u2019s \u201cmind\u201d",
  "ut": 1731540600.0,
  "body_paragraphs": [
    "AI has led to breakthroughs in drug discovery and robotics and is in the process of entirely revolutionizing how we interact with machines and the web. The only problem is we don\u2019t know exactly how it works, or why it works so well. We have a fair idea, but the details are too complex to unpick. That\u2019s a problem: It could lead us to deploy an AI system in a highly sensitive field like medicine without understanding that it could have critical flaws embedded in its workings. A team at Google DeepMind that studies something called mechanistic interpretability has been working on new ways to let us peer under the hood. At the end of July, it released Gemma Scope, a tool to help researchers understand what is happening when AI is generating an output. The hope is that if we have a better understanding of what is happening inside an AI model, we\u2019ll be able to control its outputs more effectively, leading to better AI systems in the future.  \u201cI want to be able to look inside a model and see if it\u2019s being deceptive,\u201d says Neel Nanda, who runs the mechanistic interpretability team at Google DeepMind. \u201cIt seems like being able to read a model\u2019s mind should help.\u201d Mechanistic interpretability, also known as \u201cmech interp,\u201d is a new research field that aims to understand how neural networks actually work. At the moment, very basically, we put inputs into a model in the form of a lot of data, and then we get a bunch of model weights at the end of training. These are the parameters that determine how a model makes decisions. We have some idea of what\u2019s happening between the inputs and the model weights: Essentially, the AI is finding patterns in the data and making conclusions from those patterns, but these patterns can be incredibly complex and often very hard for humans to interpret.",
    "It\u2019s like a teacher reviewing the answers to a complex math problem on a test. The student\u2014the AI, in this case\u2014wrote down the correct answer, but the work looks like a bunch of squiggly lines. This example assumes the AI is always getting the correct answer, but that\u2019s not always true; the AI student may have found an irrelevant pattern that it\u2019s assuming is valid. For example, some current AI systems will give you the result that 9.11 is bigger than 9.8. Different methods developed in the field of mechanistic interpretability are beginning to shed a little bit of light on what may be happening, essentially making sense of the squiggly lines. \u201cA key goal of mechanistic interpretability is trying to reverse-engineer the algorithms inside these systems,\u201d says Nanda. \u201cWe give the model a prompt, like \u2018Write a poem,\u2019 and then it writes some rhyming lines. What is the algorithm by which it did this? We\u2019d love to understand it.\u201d",
    "To find features\u2014or categories of data that represent a larger concept\u2014in its AI model, Gemma, DeepMind ran a tool known as a \u201csparse autoencoder\u201d on each of its layers. You can think of a sparse autoencoder as a microscope that zooms in on those layers and lets you look at their details. For example, if you prompt Gemma about a chihuahua, it will trigger the \u201cdogs\u201d feature, lighting up what the model knows about \u201cdogs.\u201d The reason it is considered \u201csparse\u201d is that it\u2019s limiting the number of neurons used, basically pushing for a more efficient and generalized representation of the data. The tricky part of sparse autoencoders is deciding how granular you want to get. Think again about the microscope. You can magnify something to an extreme degree, but it may make what you\u2019re looking at impossible for a human to interpret. But if you zoom too far out, you may be limiting what interesting things you can see and discover.\u00a0 DeepMind\u2019s solution was to run sparse autoencoders of different sizes, varying the number of features they want the autoencoder to find. The goal was not for DeepMind\u2019s researchers to thoroughly analyze the results on their own. Gemma and the autoencoders are open-source, so this project was aimed more at spurring interested researchers to look at what the sparse autoencoders found and hopefully make new insights into the model's internal logic. Since DeepMind ran autoencoders on each layer of their model, a researcher could map the progression from input to output to a degree we haven\u2019t seen before. Related StoryNobody knows how AI worksIt\u2019s still early days for our understanding of AI, so expect more glitches and fails as it becomes a part of real-world products.",
    "\u201cThis is really exciting for interpretability researchers,\u201d says Josh Batson, a researcher at Anthropic. \u201cIf you have this model that you\u2019ve open-sourced for people to study, it means that a bunch of interpretability research can now be done on the back of those sparse autoencoders. It lowers the barrier to entry to people learning from these methods.\u201d  Neuronpedia, a platform for mechanistic interpretability, partnered with DeepMind in July to build a demo of Gemma Scope that you can play around with right now. In the demo, you can test out different prompts and see how the model breaks up your prompt and what activations your prompt lights up. You can also mess around with the model. For example, if you turn the feature about dogs way up and then ask the model a question about US presidents, Gemma will find some way to weave in random babble about dogs, or the model may just start barking at you. One interesting thing about sparse autoencoders is that they are unsupervised, meaning they find features on their own. That leads to surprising discoveries about how the models break down human concepts. \u201cMy personal favorite feature is the cringe feature,\u201d says Joseph Bloom, science lead at Neuronpedia. \u201cIt seems to appear in negative criticism of text and movies. It\u2019s just a great example of tracking things that are so human on some level.\u201d\u00a0 You can search for concepts on Neuronpedia and it will highlight what features are being activated on specific tokens, or words, and how strongly each one is activated. \u201cIf you read the text and you see what\u2019s highlighted in green, that\u2019s when the model thinks the cringe concept is most relevant. The most active example for cringe is somebody preaching at someone else,\u201d says Bloom. Some features are proving easier to track than others. \u201cOne of the most important features that you would want to find for a model is deception,\u201d says Johnny Lin, founder of Neuronpedia. \u201cIt\u2019s not super easy to find: \u2018Oh, there\u2019s the feature that fires when it\u2019s lying to us.\u2019 From what I\u2019ve seen, it hasn\u2019t been the case that we can find deception and ban it.\u201d",
    "DeepMind\u2019s research is similar to what another AI company, Anthropic, did back in May with Golden Gate Claude. It used sparse autoencoders to find the parts of Claude, their model, that lit up when discussing the Golden Gate Bridge in San Francisco. It then amplified the activations related to the bridge to the point where Claude literally identified not as Claude, an AI model, but as the physical Golden Gate Bridge and would respond to prompts as the bridge. Although it may just seem quirky, mechanistic interpretability research may prove incredibly useful. \u201cAs a tool for understanding how the model generalizes and what level of abstraction it\u2019s working at, these features are really helpful,\u201d says Batson. For example, a team lead by Samuel Marks, now at Anthropic, used sparse autoencoders to find features that showed a particular model was associating certain professions with a specific gender. They then turned off these gender features to reduce bias in the model. This experiment was done on a very small model, so it\u2019s unclear if the work will apply to a much larger model. Mechanistic interpretability research can also give us insights into why AI makes errors. In the case of the assertion that 9.11 is larger than 9.8, researchers from Transluce saw that the question was triggering the parts of an AI model related to Bible verses and September 11. The researchers concluded the AI could be interpreting the numbers as dates, asserting the later date, 9/11, as greater than 9/8. And in a lot of books like religious texts, section 9.11 comes after section 9.8, which may be why the AI thinks of it as greater. Once they knew why the AI made this error, the researchers tuned down the AI's activations on Bible verses and September 11, which led to the model giving the correct answer when prompted again on whether 9.11 is larger than 9.8. There are also other potential applications. Currently, a system-level prompt is built into LLMs to deal with situations like users who ask how to build a bomb. When you ask ChatGPT a question, the model is first secretly prompted by OpenAI to refrain from telling you how to make bombs or do other nefarious things. But it\u2019s easy for users to jailbreak AI models with clever prompts, bypassing any restrictions.\u00a0 If the creators of the models are able to see where in an AI the bomb-building knowledge is, they can theoretically turn off those nodes permanently. Then even the most cleverly written prompt wouldn\u2019t elicit an answer about how to build a bomb, because the AI would literally have no information about how to build a bomb in its system. This type of granularity and precise control are easy to imagine but extremely hard to achieve with the current state of mechanistic interpretability.\u00a0 \u201cA limitation is the steering [influencing a model by adjusting its parameters] is just not working that well, and so when you steer to reduce violence in a model, it ends up completely lobotomizing its knowledge in martial arts. There\u2019s a lot of refinement to be done in steering,\u201d says Lin. The knowledge of \u201cbomb making,\u201d for example, isn\u2019t just a simple on-and-off switch in an AI model. It most likely is woven into multiple parts of the model, and turning it off would probably involve hampering the AI\u2019s knowledge of chemistry. Any tinkering may have benefits but also significant trade-offs. That said, if we are able to dig deeper and peer more clearly into the \u201cmind\u201d of AI, DeepMind and others are hopeful that mechanistic interpretability could represent a plausible path to alignment\u2014the process of making sure AI is actually doing what we want it to do. hide"
  ]
}