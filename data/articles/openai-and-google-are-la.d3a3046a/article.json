{
  "url": "https://www.technologyreview.com/2024/05/15/1092516/openai-and-google-are-launching-supercharged-ai-assistants-heres-how-you-can-try-them-out/",
  "title": "OpenAI and Google are launching supercharged AI assistants. Here\u2019s how you can try them out.",
  "ut": 1715762925.0,
  "body_paragraphs": [
    "This week, Google and OpenAI both announced they\u2019ve built supercharged AI assistants: tools that can converse with you in real time and recover when you interrupt them, analyze your surroundings via live video, and translate conversations on the fly.\u00a0 OpenAI struck first on Monday, when it debuted its new flagship model GPT-4o. The live demonstration showed it reading bedtime stories and helping to solve math problems, all in a voice that sounded eerily like Joaquin Phoenix\u2019s AI girlfriend in the movie Her (a trait not lost on CEO Sam Altman).\u00a0  On Tuesday, Google announced its own new tools, including a conversational assistant called Gemini Live, which can do many of the same things. It also revealed that it\u2019s building a sort of \u201cdo-everything\u201d AI agent, which is currently in development but will not be released until later this year. Soon you\u2019ll be able to explore for yourself to gauge whether you\u2019ll turn to these tools in your daily routine as much as their makers hope, or whether they\u2019re more like a sci-fi party trick that eventually loses its charm. Here\u2019s what you should know about how to access these new tools, what you might use them for, and how much it will cost.",
    "OpenAI\u2019s GPT-4o What it\u2019s capable of: The model can talk with you in real time, with a response delay of about 320 milliseconds, which OpenAI says is on par with natural human conversation. You can ask the model to interpret anything you point your smartphone camera at, and it can provide assistance with tasks like coding or translating text. It can also summarize information, and generate images, fonts, and 3D renderings.\u00a0 How to access it: OpenAI says it will start rolling out GPT-4o\u2019s text and vision features in the web interface as well as the GPT app, but has not set a date. The company says it will add the voice functions in the coming weeks, although it\u2019s yet to set an exact date for this either. Developers can access the text and vision features in the API now, but voice mode will launch only to a \u201csmall group\u201d of developers initially.",
    "How much it costs: Use of GPT-4o will be free, but OpenAI will set caps on how much you can use the model before you need to upgrade to a paid plan. Those who join one of OpenAI\u2019s paid plans, which start at $20 per month, will have five times more capacity on GPT-4o.\u00a0 Google\u2019s Gemini Live\u00a0 What is Gemini Live? This is the Google product most comparable to GPT-4o\u2014a version of the company\u2019s AI model that you can speak with in real time. Google says that you\u2019ll also be able to use the tool to communicate via live video \u201clater this year.\u201d The company promises it will be a useful conversational assistant for things like preparing for a job interview or rehearsing a speech. How to access it: Gemini Live launches in \u201cthe coming months\u201d via Google\u2019s premium AI plan, Gemini Advanced.\u00a0 How much it costs: Gemini Advanced offers a two-month free trial period and costs $20 per month thereafter.\u00a0 But wait, what\u2019s Project Astra? Astra is a project to build a do-everything AI agent, which was demoed at Google\u2019s I/O conference but will not be released until later this year. People will be able to use Astra through their smartphones and possibly desktop computers, but the company is exploring other options too, such as embedding it into smart glasses or other devices, Oriol Vinyals, vice president of research at Google DeepMind, told MIT Technology Review. Which is better? It\u2019s hard to tell without having hands on the full versions of these models ourselves. Google showed off Project Astra through a polished video, whereas OpenAI opted to debut GPT-4o via a seemingly more authentic live demonstration, but in both cases, the models were asked to do things the designers likely already practiced. The real test will come when they\u2019re debuted to millions of users with unique demands.\u00a0\u00a0 That said, if you compare OpenAI\u2019s published videos with Google\u2019s, the two leading tools look very similar, at least in their ease of use. To generalize, GPT-4o seems to be slightly ahead on audio, demonstrating realistic voices, conversational flow, and even singing, whereas Project Astra shows off more advanced visual capabilities, like being able to \u201cremember\u201d where you left your glasses. OpenAI\u2019s decision to roll out the new features more quickly might mean its product will get more use at first than Google\u2019s, which won\u2019t be fully available until later this year. It\u2019s too soon to tell which model \"hallucinates\" false information less often or creates more useful responses.",
    "Are they safe? Both OpenAI and Google say their models are well tested: OpenAI says GPT-4o was evaluated by more than 70 experts in fields like misinformation and social psychology, and Google has said that Gemini \"has the most comprehensive safety evaluations of any Google AI model to date, including for bias and toxicity.\u201d\u00a0 But these companies are building a future where AI models search, vet, and evaluate the world\u2019s information for us to serve up a concise answer to our questions. Even more so than with simpler chatbots, it\u2019s wise to remain skeptical about what they tell you. Additional reporting by Melissa Heikkil\u00e4. hide"
  ]
}