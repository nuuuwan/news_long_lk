{
  "url": "https://www.technologyreview.com/2024/08/30/1103385/a-new-way-to-build-neural-networks-could-make-ai-more-understandable/",
  "title": "A new way to build neural networks could make AI more understandable",
  "ut": 1724979924.0,
  "body_paragraphs": [
    "A tweak to the way artificial neurons work in neural networks could make AIs easier to decipher. Artificial neurons\u2014the fundamental building blocks of deep neural networks\u2014have survived almost unchanged for decades. While these networks give modern artificial intelligence its power, they are also inscrutable.\u00a0  Existing artificial neurons, used in large language models like GPT4, work by taking in a large number of inputs, adding them together, and converting the sum into an output using another mathematical operation inside the neuron. Combinations of such neurons make up neural networks, and their combined workings can be difficult to decode. But the new way to combine neurons works a little differently. Some of the complexity of the existing neurons is both simplified and moved outside the neurons. Inside, the new neurons simply sum up their inputs and produce an output, without the need for the extra hidden operation. Networks of such neurons are called Kolmogorov-Arnold Networks (KANs), after the Russian mathematicians who inspired them.",
    "Related StoryLarge language models can do jaw-dropping things. But nobody knows exactly why.And that's a problem. Figuring it out is one of the biggest scientific puzzles of our time and a crucial step towards controlling more powerful future models.",
    "The simplification, studied in detail by a group led by researchers at MIT, could make it easier to understand why neural networks produce certain outputs, help verify their decisions, and even probe for bias. Preliminary evidence also suggests that as KANs are made bigger, their accuracy increases faster than networks built of traditional neurons. \u201cIt's interesting work,\u201d says Andrew Wilson, who studies the foundations of machine learning at New York University. \u201cIt's nice that people are trying to fundamentally rethink the design of these [networks].\u201d",
    "The basic elements of KANs were actually proposed in the 1990s, and researchers kept building simple versions of such networks. But the MIT-led team has taken the idea further, showing how to build and train bigger KANs, performing empirical tests on them, and analyzing some KANs to demonstrate how their problem-solving ability could be interpreted by humans. \u201cWe revitalized this idea,\u201d said team member Ziming Liu, a PhD student in Max Tegmark\u2019s lab at MIT. \u201cAnd, hopefully, with the interpretability\u2026 we [may] no longer [have to] think neural networks are black boxes.\u201d While it's still early days, the team\u2019s work on KANs is attracting attention. GitHub pages have sprung up that show how to use KANs for myriad applications, such as image recognition and solving fluid dynamics problems.\u00a0 Finding the formula The current advance came when Liu and colleagues at MIT, Caltech, and other institutes were trying to understand the inner workings of standard artificial neural networks.\u00a0 Today, almost all types of AI, including those used to build large language models and image recognition systems, include sub-networks known as a multilayer perceptron (MLP). In an MLP, artificial neurons are arranged in dense, interconnected \u201clayers.\u201d Each neuron has within it something called an \u201cactivation function\u201d\u2014a mathematical operation that takes in a bunch of inputs and transforms them in some pre-specified manner into an output.\u00a0  In an MLP, each artificial neuron receives inputs from all the neurons in the previous layer and multiplies each input with a corresponding \u201cweight\u201d (a number signifying the importance of that input). These weighted inputs are added together and fed to the activation function inside the neuron to generate an output, which is then passed on to neurons in the next layer. An MLP learns to distinguish between images of cats and dogs, for example, by choosing the correct values for the weights of the inputs for all the neurons. Crucially, the activation function is fixed and doesn\u2019t change during training. Once trained, all the neurons of an MLP and their connections taken together essentially act as another function that takes an input (say, tens of thousands of pixels in an image) and produces the desired output (say, 0 for cat and 1 for dog). Understanding what that function looks like, meaning its mathematical form, is an important part of being able to understand why it produces some output. For example, why does it tag someone as creditworthy given inputs about their financial status? But MLPs are black boxes. Reverse-engineering the network is nearly impossible for complex tasks such as image recognition. And even when Liu and colleagues tried to reverse-engineer an MLP for simpler tasks that involved bespoke \u201csynthetic\u201d data, they struggled.\u00a0 \u201cIf we cannot even interpret these synthetic datasets from neural networks, then it's hopeless to deal with real-world data sets,\u201d says Liu. \u201cWe found it really hard to try to understand these neural networks. We wanted to change the architecture.\u201d",
    "Mapping the math The main change was to remove the fixed activation function and introduce a much simpler learnable function to transform each incoming input before it enters the neuron.\u00a0 Unlike the activation function in an MLP neuron, which takes in numerous inputs, each simple function outside the KAN neuron takes in one number and spits out another number. Now, during training, instead of learning the individual weights, as happens in an MLP, the KAN just learns how to represent each simple function. In a paper posted this year on the preprint server ArXiv, Liu and colleagues showed that these simple functions outside the neurons are much easier to interpret, making it possible to reconstruct the mathematical form of the function being learned by the entire KAN. Related StoryWhat is AI?Everyone thinks they know but no one can agree. And that\u2019s a problem.",
    "The team, however, has only tested the interpretability of KANs on simple, synthetic data sets, not on real-world problems, such as image recognition, which are more complicated. \u201c[We are] slowly pushing the boundary,\u201d says Liu. \u201cInterpretability can be a very challenging task.\u201d Liu and colleagues have also shown that KANs get more accurate at their tasks with increasing size faster than MLPs do. The team proved the result theoretically and showed it empirically for science-related tasks (such as learning to approximate functions relevant to physics). \u201cIt's still unclear whether this observation will extend to standard machine learning tasks, but at least for science-related tasks, it seems promising,\u201d Liu says. Liu acknowledges that KANs come with one important downside: it takes more time and compute power to train a KAN, compared to an MLP. \u201cThis limits the application efficiency of KANs on large-scale data sets and complex tasks,\u201d says Di Zhang, of Xi\u2019an Jiaotong-Liverpool University in Suzhou, China. But he suggests that more efficient algorithms and hardware accelerators could help. Anil Ananthaswamy is a science journalist and author who writes about physics, computational neuroscience, and machine learning. His new book, WHY MACHINES LEARN: The Elegant Math Behind Modern AI, was published by Dutton (Penguin Random House US) in July. hide"
  ]
}