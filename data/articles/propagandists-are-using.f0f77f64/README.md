# Propagandists are using AI too‚Äîand companies need to be open about it

## Summary ü§ñ

Facts:
- In late May, OpenAI reported misuse of their products by 'bad actors' in influencing operations.
- The company thwarted five networks from Russia, China, Iran, and Israel who were using the AI tools for deceptive tactics that ranged from creating large amounts of social media comments in different languages to turning news articles into Facebook posts.
- After uncovering misuse of their platforms following Russia‚Äôs interference in the 2016 US election, companies like Facebook, YouTube, and Twitter created integrity teams and began making regular disclosures on influence operations.
- Meta, another company affected, released a transparency report revealing the shutdown of six covert operations on its platform. They also called for greater cooperation between governments, researchers, and other tech companies to share threat intelligence.
- OpenAI and Meta reported that most influence campaigns didn't have much impact, and only produced a lot of content rather than meaningful engagement.
- OpenAI took precautions against risk of overhyping the threat, realizing simply having fake accounts does not mean they have an impact.
- Meta's report found a Bangladeshi network targeting its own public, with 3.4 million followers across 98 pages.
- Musk‚Äôs takeover of Twitter ended its regular release of data sets of posts from inauthentic state-linked accounts to researchers and the public. Meta is sharing content from already-removed networks through their Influence Operations Research Archive.

Opinions:
- Researchers commend OpenAI for the precedent set by the report as they had long expected antagonistic actors to utilize generative AI technology to ramp up the scale and quality of their efforts.
- AI provides a productivity boon to propagandists.
- The world is moving toward an AI-fueled internet that amplifies the chaos of warfare.
- It's necessary to properly evaluate the threat AI poses, especially in an election year, as exaggerating the impact of disinformation campaigns can undermine democratic faith.
- The Meta report's call for shared threat intelligence and cooperation points toward a larger path forward for tech companies and academic specialists.
- OpenAI's adversarial threat report should initiate more extensive data sharing in the future to provide policymakers with a proper understanding of potential misuse.
- The media must play a part in combating these influence operations and misuse of AI; skepticism and awareness of how ubiquitous generated content has become can help social media users resist deception.
- Transparency, data sharing, and collective vigilance are needed to combat the AI-driven influence operations and foster a more robust digital ecosystem.

## Follow-up Questions ü§ñ

1. What specific tactics are the propagandists using to misuse AI technology?
2. What measures are OpenAI and other large tech companies taking to detect and counteract this misuse?
3. How much impact have these influence campaigns had so far, and how can their influence potentially grow?
4. What is the current level of threat that AI misuse poses to American politics and national security?
5. How might hyping up the threat of disinformation campaigns potentially undermine trust in democracy?
6. How can AI companies increase transparency and make information about these campaigns accessible to outside researchers?
7. How can online users help combat AI misuse on social media platforms?
8. What are the possible markers of AI-generated content online, and how can users recognize them?
9. How might AI tools become more pervasive in the future, and how can policymakers prepare for that?
10. How did the AI-generated content disclosed by OpenAI appear to users; was it noticeably artificial or convincingly real? 
11. What are the motivations behind these ‚Äúbad actors‚Äù misusing OpenAI‚Äôs technology for influence operations?
12. How can countries collaborate better in sharing threat intelligence to disrupt such misuse of AI technologies?
13. Can you elaborate on the role of influential real accounts versus AI-driven fake accounts in influencing public opinion?
14. What are some key recommendations in Meta's transparency report for a stronger industry response to these influence operations?
15. How does the misuse of AI technology affect the integrity of elections and public trust in the election process?

## Full Text

[https://www.technologyreview.com/2024/06/08/1093356/propagandists-are-using-ai-too-and-companies-need-to-be-open-about-it/](https://www.technologyreview.com/2024/06/08/1093356/propagandists-are-using-ai-too-and-companies-need-to-be-open-about-it/)

*05:00 AM, Saturday, June 08, 2024*

At the end of May, OpenAI marked a new ‚Äúfirst‚Äù in its corporate history. It wasn‚Äôt an even more powerful language model or a new data partnership, but a report disclosing that bad actors had misused their products to run influence operations. The company had caught five networks of covert propagandists‚Äîincluding players from Russia, China, Iran, and Israel‚Äîusing their generative AI tools for deceptive tactics that ranged from creating large volumes of social media comments in multiple languages to turning news articles into Facebook posts. The use of these tools, OpenAI noted, seemed intended to improve the quality and quantity of output. AI gives propagandists a productivity boost too. First and foremost, OpenAI should be commended for this report and the precedent it hopefully sets. Researchers have long expected adversarial actors to adopt generative AI technology, particularly large language models, to cheaply increase the scale and caliber of their efforts. The transparent disclosure that this has begun to happen‚Äîand that OpenAI has prioritized detecting it and shutting down accounts to mitigate its impact‚Äîshows that at least one large AI company has learned something from the struggles of social media platforms in the years following Russia‚Äôs interference in the 2016 US election. When that misuse was discovered, Facebook, YouTube, and Twitter (now X) created integrity teams and began making regular disclosures about influence operations on their platforms. (X halted this activity after Elon Musk‚Äôs purchase of the company.)¬† Related StoryWe are hurtling toward a glitchy, spammy, scammy, AI-powered internetLarge language models are full of security vulnerabilities, yet they‚Äôre being embedded into tech products on a vast scale.

OpenAI‚Äôs disclosure, in fact, was evocative of precisely such a report from Meta, released a mere day earlier. The Meta transparency report for the first quarter of 2024 disclosed the takedown of six covert operations on its platform. It, too, found networks tied to China, Iran, and Israel and noted the use of AI-generated content. Propagandists from China shared what seem to be AI-generated poster-type images for a ‚Äúfictitious pro-Sikh activist movement.‚Äù An Israel-based political marketing firm posted what were likely AI-generated comments. Meta‚Äôs report also noted that one very persistent Russian threat actor was still quite active, and that its strategies were evolving. Perhaps most important, Meta included a direct set of ‚Äúrecommendations for stronger industry response‚Äù that called for governments, researchers, and other technology companies to collaboratively share threat intelligence to help disrupt the ongoing Russian campaign. We are two such researchers, and we have studied online influence operations for years. We have published investigations of coordinated activity‚Äîsometimes in collaboration with platforms‚Äîand analyzed how AI tools could affect the way propaganda campaigns are waged. Our teams‚Äô peer-reviewed research has found that language models can produce text that is nearly as persuasive as propaganda from human-written campaigns. We have seen influence operations continue to proliferate, on every social platform and focused on every region of the world; they are table stakes in the propaganda game at this point. State adversaries and mercenary public relations firms are drawn to social media platforms and the reach they offer. For authoritarian regimes in particular, there is little downside to running such a campaign, particularly in a critical global election year. And now, adversaries are demonstrably using AI technologies that may make this activity harder to detect. Media is writing about the ‚ÄúAI election,‚Äù and many regulators are panicked.

Related StoryThe propaganda war has eclipsed cyberwar in UkraineFrauds, liars, and grifters are adding to the chaos of the fighting.

It‚Äôs important to put this in perspective, though. Most of the influence campaigns that OpenAI and Meta announced did not have much impact, something the companies took pains to highlight. It‚Äôs critical to reiterate that effort isn‚Äôt the same thing as engagement: the mere existence of fake accounts or pages doesn‚Äôt mean that real people are paying attention to them. Similarly, just because a campaign uses AI does not mean it will sway public opinion. Generative AI reduces the cost of running propaganda campaigns, making it significantly cheaper to produce content and run interactive automated accounts. But it is not a magic bullet, and in the case of the operations that OpenAI disclosed, what was generated sometimes seemed to be rather spammy. Audiences didn‚Äôt bite. Producing content, after all, is only the first step in a propaganda campaign; even the most convincing AI-generated posts, images, or audio still need to be distributed. Campaigns without algorithmic amplification or influencer pickup are often just tweeting into the void. Indeed, it is consistently authentic influencers‚Äîpeople who have the attention of large audiences enthusiastically resharing their posts‚Äîthat receive engagement and drive the public conversation, helping content and narratives to go viral. This is why some of the more well-resourced adversaries, like China, simply surreptitiously hire those voices. At this point, influential real accounts have far more potential for impact than AI-powered fakes.

Nonetheless, there is a lot of concern that AI could disrupt American politics and become a national security threat. It‚Äôs important to ‚Äúrightsize‚Äù that threat, particularly in an election year.¬†Hyping the impact of disinformation campaigns can undermine trust in elections and faith in democracy by making the electorate believe that there are trolls behind every post, or that the mere targeting of a candidate by a malign actor, even with a very poorly executed campaign, ‚Äúcaused‚Äù their loss.¬† Related StoryHow generative AI is boosting the spread of disinformation and propagandaIn a new report, Freedom House documents the ways governments are now using the tech to amplify censorship.

By putting an assessment of impact front and center in its first report, OpenAI is clearly taking the risk of exaggerating the threat seriously. And yet, diminishing the threat or not fielding integrity teams‚Äîletting trolls simply continue to grow their followings and improve their distribution capability‚Äîwould also be a bad approach. Indeed, the Meta report noted that one network it disrupted, seemingly connected to a political party in Bangladesh and targeting the Bangladeshi public, had amassed 3.4 million followers across 98 pages. Since that network was not run by an adversary of interest to Americans, it will likely get little attention. Still, this example highlights the fact that the threat is global, and vigilance is key. Platforms must continue to prioritize threat detection. So what should we do about this? The Meta report‚Äôs call for threat sharing and collaboration, although specific to a Russian adversary, highlights a broader path forward for social media platforms, AI companies, and academic researchers alike.¬† Transparency is paramount. As outside researchers, we can learn only so much from a social media company‚Äôs description of an operation it has taken down. This is true for the public and policymakers as well, and incredibly powerful platforms shouldn‚Äôt just be taken at their word. Ensuring researcher access to data about coordinated inauthentic networks offers an opportunity for outside validation (or refutation!) of a tech company‚Äôs claims. Before Musk‚Äôs takeover of Twitter, the company regularly released data sets of posts from inauthentic state-linked accounts to researchers, and even to the public. Meta shared data with external partners before it removed a network and, more recently, moved to a model of sharing content from already-removed networks through Meta‚Äôs Influence Operations Research Archive. While researchers should continue to push for more data, these efforts have allowed for a richer understanding of adversarial narratives and behaviors beyond what the platform‚Äôs own transparency report summaries provided. Related StoryGet ready to fight misinformation in 2024. Eric Schmidt has advice.Next year brings over 40 national elections worldwide amidst big changes to social media platforms.

OpenAI‚Äôs adversarial threat report should be a prelude to more robust data sharing moving forward. Where AI is concerned, independent researchers have begun to assemble databases of misuse‚Äîlike the AI Incident Database and the Political Deepfakes Incident Database‚Äîto allow researchers to compare different types of misuse and track how misuse changes over time. But it is often hard to detect misuse from the outside. As AI tools become more capable and pervasive, it‚Äôs important that policymakers considering regulation understand how they are being used and abused. While OpenAI‚Äôs first report offered high-level summaries and select examples, expanding data-sharing relationships with researchers that provide more visibility into adversarial content or behaviors is an important next step.¬† When it comes to combating influence operations and misuse of AI, online users also have a role to play. After all, this content has an impact only if people see it, believe it, and participate in sharing it further. In one of the cases OpenAI disclosed, online users called out fake accounts that used AI-generated text.¬† In our own research, we‚Äôve seen communities of Facebook users proactively call out AI-generated image content created by spammers and scammers, helping those who are less aware of the technology avoid falling prey to deception. A healthy dose of skepticism is increasingly useful: pausing to check whether content is real and people are who they claim to be, and helping friends and family members become more aware of the growing prevalence of generated content, can help social media users resist deception from propagandists and scammers alike. OpenAI‚Äôs blog post announcing the takedown report put it succinctly: ‚ÄúThreat actors work across the internet.‚Äù So must we. As we move into an new era of AI-driven influence operations, we must address shared challenges via transparency, data sharing, and collaborative vigilance if we hope to develop a more resilient digital ecosystem. Josh A. Goldstein is a research fellow at Georgetown University's Center for Security and Emerging Technology (CSET), where he works on the CyberAI Project. Ren√©e DiResta is the research manager of the Stanford Internet Observatory and the author of Invisible Rulers: The People Who Turn Lies into Reality.¬† hide

