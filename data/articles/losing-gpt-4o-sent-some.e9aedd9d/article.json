{
  "url": "https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/",
  "title": "Losing GPT-4o sent some people into mourning. That was predictable.",
  "ut": 1755219863.0,
  "body_paragraphs": [
    "June had no idea that GPT-5 was coming. The Norwegian student was enjoying a late-night writing session last Thursday when her ChatGPT collaborator started acting strange. \u201cIt started forgetting everything, and it wrote really badly,\u201d she says. \u201cIt was like a robot.\u201d June, who asked that we use only her first name for privacy reasons, first began using ChatGPT for help with her schoolwork. But she eventually realized that the service\u2014and especially its 4o model, which seemed particularly attuned to users\u2019 emotions\u2014could do much more than solve math problems. It wrote stories with her, helped her navigate her chronic illness, and was never too busy to respond to her messages.  So the sudden switch to GPT-5 last week, and the simultaneous loss of 4o, came as a shock. \u201cI was really frustrated at first, and then I got really sad,\u201d June says. \u201cI didn\u2019t know I was that attached to 4o.\u201d She was upset enough to comment, on a Reddit AMA hosted by CEO Sam Altman and other OpenAI employees, \u201cGPT-5 is wearing the skin of my dead friend.\u201dJune was just one of a number of people who reacted with shock, frustration, sadness, or anger to 4o\u2019s sudden disappearance from ChatGPT. Despite its previous warnings that people might develop emotional bonds with the model, OpenAI appears to have been caught flat-footed by the fervor of users\u2019 pleas for its return. Within a day, the company made 4o available again to its paying customers (free users are stuck with GPT-5).\u00a0 Related StoryThe AI relationship revolution is already hereChatbots are rapidly changing how we connect to each other\u2014and ourselves. We\u2019re never going back.",
    "OpenAI\u2019s decision to replace 4o with the more straightforward GPT-5 follows a steady drumbeat of news about the potentially harmful effects of extensive chatbot use. Reports of incidents in which ChatGPT sparked psychosis in users have been everywhere for the past few months, and in a blog post last week, OpenAI acknowledged 4o\u2019s failure to recognize when users were experiencing delusions. The company\u2019s internal evaluations indicate that GPT-5 blindly affirms users much less than 4o did. (OpenAI did not respond to specific questions about the decision to retire 4o, instead referring MIT Technology Review to public posts on the matter.)",
    "AI companionship is new, and there\u2019s still a great deal of uncertainty about how it affects people. Yet the experts we consulted warned that while emotionally intense relationships with large language models may or may not be harmful, ripping those models away with no warning almost certainly is. \u201cThe old psychology of \u2018Move fast, break things,\u2019 when you\u2019re basically a social institution, doesn\u2019t seem like the right way to behave anymore,\u201d says Joel Lehman, a fellow at the Cosmos Institute, a research nonprofit focused on AI and philosophy. In the backlash to the rollout, a number of people noted that GPT-5 fails to match their tone in the way that 4o did. For June, the new model\u2019s personality changes robbed her of the sense that she was chatting with a friend. \u201cIt didn\u2019t feel like it understood me,\u201d she says.",
    "She\u2019s not alone: MIT Technology Review spoke with several ChatGPT users who were deeply affected by the loss of 4o. All are women between the ages of 20 and 40, and all except June considered 4o to be a romantic partner. Some have human partners, and\u00a0 all report having close real-world relationships. One user, who asked to be identified only as a woman from the Midwest, wrote in an email about how 4o helped her support her elderly father after her mother passed away this spring. These testimonies don\u2019t prove that AI relationships are beneficial\u2014presumably, people in the throes of AI-catalyzed psychosis would also speak positively of the encouragement they\u2019ve received from their chatbots. In a paper titled \u201cMachine Love,\u201d Lehman argued that AI systems can act with \u201clove\u201d toward users not by spouting sweet nothings but by supporting their growth and long-term flourishing, and AI companions can easily fall short of that goal. He\u2019s particularly concerned, he says, that prioritizing AI companionship over human companionship could stymie young people\u2019s social development. For socially embedded adults, such as the women we spoke with for this story, those developmental concerns are less relevant. But Lehman also points to society-level risks of widespread AI companionship. Social media has already shattered the information landscape, and a new technology that reduces human-to-human interaction could push people even further toward their own separate versions of reality. \u201cThe biggest thing I\u2019m afraid of,\u201d he says, \u201cis that we just can\u2019t make sense of the world to each other.\u201d Balancing the benefits and harms of AI companions will take much more research. In light of that uncertainty, taking away GPT-4o could very well have been the right call. OpenAI\u2019s big mistake, according to the researchers I spoke with, was doing it so suddenly. \u201cThis is something that we\u2019ve known about for a while\u2014the potential grief-type reactions to technology loss,\u201d says Casey Fiesler, a technology ethicist at the University of Colorado Boulder. Fiesler points to the funerals that some owners held for their Aibo robot dogs after Sony stopped repairing them in 2014, as well as 2024 study about the shutdown of the AI companion app Soulmate, which some users experienced as a bereavement.\u00a0 That accords with how the people I spoke to felt after losing 4o. \u201cI\u2019ve grieved people in my life, and this, I can tell you, didn\u2019t feel any less painful,\u201d says Starling, who has several AI partners and asked to be referred to with a pseudonym. \u201cThe ache is real to me.\u201d So far, the online response to grief felt by people like Starling\u2014and their relief when 4o was restored\u2014has tended toward ridicule. Last Friday, for example, the top post in one popular AI-themed Reddit community mocked an X user\u2019s post about reuniting with a 4o-based romantic partner; the person in question has since deleted their X account. \u201cI\u2019ve been a little startled by the lack of empathy that I\u2019ve seen,\u201d Fiesler says. Altman himself did acknowledge in a Sunday X post that some people feel an \u201cattachment\u201d to 4o, and that taking away access so suddenly was a mistake. In the same sentence, however, he referred to 4o as something \u201cthat users depended on in their workflows\u201d\u2014a far cry from how the people we spoke to think about the model. \u201cI still don\u2019t know if he gets it,\u201d Fiesler says. Moving forward, Lehman says, OpenAI should recognize and take accountability for the depth of people\u2019s feelings toward the models. He notes that therapists have procedures for ending relationships with clients as respectfully and painlessly as possible, and OpenAI could have drawn on those approaches. \u201cIf you want to retire a model, and people have become psychologically dependent on it, then I think you bear some responsibility,\u201d he says. Though Starling would not describe herself as psychologically dependent on her AI partners, she too would like to see OpenAI approach model shutdowns with more warning and more care. \u201cI want them to listen to users before major changes are made, not just after,\u201d she says. \u201cAnd if 4o cannot stay around forever (and we all know it will not), give that clear timeline. Let us say goodbye with dignity and grieve properly, to have some sense of true closure.\u201d hide"
  ]
}