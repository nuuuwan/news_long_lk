{
  "url": "https://www.technologyreview.com/2025/09/16/1123614/the-looming-crackdown-on-ai-companionship/",
  "title": "The looming crackdown on AI companionship",
  "ut": 1757979000.0,
  "body_paragraphs": [
    "As long as there has been AI, there have been people sounding alarms about what it might do to us: rogue superintelligence, mass unemployment, or environmental ruin from data center sprawl. But this week showed that another threat entirely\u2014that of kids forming unhealthy bonds with AI\u2014is the one pulling AI safety out of the academic fringe and into regulators\u2019 crosshairs. This has been bubbling for a while. Two high-profile lawsuits filed in the last year, against Character.AI and OpenAI, allege that companion-like behavior in their models contributed to the suicides of two teenagers. A study by US nonprofit Common Sense Media, published in July, found that 72% of teenagers have used AI for companionship. Stories in reputable outlets about \u201cAI psychosis\u201d have highlighted how endless conversations with chatbots can lead people down delusional spirals.  It\u2019s hard to overstate the impact of these stories. To the public, they are proof that AI is not merely imperfect, but a technology that\u2019s more harmful than helpful. If you doubted that this outrage would be taken seriously by regulators and companies, three things happened this week that might change your mind. A California law passes the legislature On Thursday, the California state legislature passed a first-of-its-kind bill. It would require AI companies to include reminders for users they know to be minors that responses are AI generated. Companies would also need to have a protocol for addressing suicide and self-harm and provide annual reports on instances of suicidal ideation in users\u2019 conversations with their chatbots. It was led by Democratic state senator Steve Padilla, passed with heavy bipartisan support, and now awaits Governor Gavin Newsom\u2019s signature.",
    "There are reasons to be skeptical of the bill\u2019s impact. It doesn\u2019t specify efforts companies should take to identify which users are minors, and lots of AI companies already include referrals to crisis providers when someone is talking about suicide. (In the case of Adam Raine, one of the teenagers whose survivors are suing, his conversations with ChatGPT before his death included this type of information, but the chatbot allegedly went on to give advice related to suicide anyway.) Still, it is undoubtedly the most significant of the efforts to rein in companion-like behaviors in AI models, which are in the works in other states too. If the bill becomes law, it would strike a blow to the position OpenAI has taken, which is that \u201cAmerica leads best with clear, nationwide rules, not a patchwork of state or local regulations,\u201d as the company\u2019s chief global affairs officer, Chris Lehane, wrote on LinkedIn last week.",
    "The Federal Trade Commission takes aim The very same day, the Federal Trade Commission announced an inquiry into seven companies, seeking information about how they develop companion-like characters, monetize engagement, measure and test the impact of their chatbots, and more. The companies are Google, Instagram, Meta, OpenAI, Snap, X, and Character Technologies, the maker of Character.AI. The White House now wields immense, and potentially illegal, political influence over the agency. In March, President Trump fired its lone Democratic commissioner, Rebecca Slaughter. In July, a federal judge ruled that firing illegal, but last week the US Supreme Court temporarily permitted the firing. \u201cProtecting kids online is a top priority for the Trump-Vance FTC, and so is fostering innovation in critical sectors of our economy,\u201d said FTC chairman Andrew Ferguson in a press release about the inquiry.\u00a0 Right now, it\u2019s just that\u2014an inquiry\u2014but the process might (depending on how public the FTC makes its findings) reveal the inner workings of how the companies build their AI companions to keep users coming back again and again.\u00a0 Sam Altman on suicide cases Also on the same day (a busy day for AI news), Tucker Carlson published an hour-long interview with OpenAI\u2019s CEO, Sam Altman. It covers a lot of ground\u2014Altman\u2019s battle with Elon Musk, OpenAI\u2019s military customers, conspiracy theories about the death of a former employee\u2014but it also includes the most candid comments Altman\u2019s made so far about the cases of suicide following conversations with AI.\u00a0 Altman talked about \u201cthe tension between user freedom and privacy and protecting vulnerable users\u201d in cases like these. But then he offered up something I hadn\u2019t heard before. \u201cI think it\u2019d be very reasonable for us to say that in cases of young people talking about suicide seriously, where we cannot get in touch with parents, we do call the authorities,\u201d he said. \u201cThat would be a change.\u201d So where does all this go next? For now, it\u2019s clear that\u2014at least in the case of children harmed by AI companionship\u2014companies\u2019 familiar playbook won\u2019t hold. They can no longer deflect responsibility by leaning on privacy, personalization, or \u201cuser choice.\u201d Pressure to take a harder line is mounting from state laws, regulators, and an outraged public.",
    "But what will that look like? Politically, the left and right are now paying attention to AI\u2019s harm to children, but their solutions differ. On the right, the proposed solution aligns with the wave of internet age-verification laws that have now been passed in over 20 states. These are meant to shield kids from adult content while defending \u201cfamily values.\u201d On the left, it\u2019s the revival of stalled ambitions to hold Big Tech accountable through antitrust and consumer-protection powers.\u00a0 Consensus on the problem is easier than agreement on the cure. As it stands, it looks likely we\u2019ll end up with exactly the patchwork of state and local regulations that OpenAI (and plenty of others) have lobbied against.\u00a0 For now, it\u2019s down to companies to decide where to draw the lines. They\u2019re having to decide things like: Should chatbots cut off conversations when users spiral toward self-harm, or would that leave some people worse off? Should they be licensed and regulated like therapists, or treated as entertainment products with warnings? The uncertainty stems from a basic contradiction: Companies have built chatbots to act like caring humans, but they\u2019ve postponed developing the standards and accountability we demand of real caregivers. The clock is now running out. This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. hide"
  ]
}