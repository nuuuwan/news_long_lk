# Why Google‚Äôs AI Overviews gets things wrong

## Summary ü§ñ

AI Overviews, Google's new AI-powered search feature, provides brief AI-generated summaries of key information. However, the system has proven unreliable, giving out strange and false responses, for instance, it suggested that former US president Andrew Johnson earned university degrees between 1947 and 2012, despite dying in 1875. In response, Google stated it is making technical improvements to the system, like better detection mechanisms for nonsensical queries and limiting the inclusion of satirical content in responses. AI Overviews uses a language model that predicts the next word (or token) in a sequence, which is useful for fluent responses, but it can return incorrect information. The AI model gets around this issue by using a retrieval-augmented generation (RAG) technique that checks sources outside of its training data. However, the RAG system can return misleading answers if it encounters conflicting information and doesn't know which source to draw a response from. Google is working on refining this AI technology to improve the reliability and relevance of search results.


## Follow-up Questions ü§ñ

1. Can you elaborate on how Google's AI Overviews uses Gemini, a new generative AI model?
2. How different is the retrieval-augmented generation (RAG) technique from other AI techniques?
3. What allows AI Overviews to return more up-to-date and factually accurate responses than typical models?
4. Why is the RAG technique not foolproof? 
5. Can you explain why and how the system generates misinformation from factually correct sources?
6. What are some common reasons that AI Overviews tends to return incorrect answers?
7. What are some measures Google is taking to improve the quality and reliability of AI Overviews' responses?
8. How might reinforcement learning from human feedback help improve the quality of the AI's answers?
9. What specific challenges do LLMs face when it comes to interpreting the content they‚Äôre based on?
10. What does the addition of the label "Generative AI is experimental" to AI Overviews suggest?
11. Could you explain why some experts and users believe AI Overviews should be optional as part of the core search feature? 
12. Given AI Overviews‚Äô unreliable information output, how might this affect users' trust in AI-driven services or platforms? 
13. Are there other instances of widespread misinformation generated by AI systems? 
14. Can you provide insights into how AI systems can be designed to handle conflicting information more effectively?
15. Why is fluent language generated by large language models considered a potential risk for misinformation, and how can this issue be mitigated?

## Full Text

[https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/](https://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/)

*06:15 AM, Friday, May 31, 2024*

MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what's coming next. You can read more here. When Google announced it was rolling out its artificial-intelligence-powered search feature earlier this month, the company promised that ‚ÄúGoogle will do the googling for you.‚Äù The new feature, called AI Overviews, provides brief, AI-generated summaries highlighting key information and links on top of search results.  Unfortunately, AI systems are inherently unreliable. Within days of AI Overviews‚Äô release in the US, users were sharing examples of responses that were strange at best. It suggested that users add glue to pizza or eat at least one small rock a day, and that former US president Andrew Johnson earned university degrees between 1947 and 2012, despite dying in 1875.¬† On Thursday, Liz Reid, head of Google Search, announced that the company has been making technical improvements to the system to make it less likely to generate incorrect answers, including better detection mechanisms for nonsensical queries. It is also limiting the inclusion of satirical, humorous, and user-generated content in responses, since such material could result in misleading advice.

But why is AI Overviews returning unreliable, potentially dangerous information? And what, if anything, can be done to fix it? How does AI Overviews work? In order to understand why AI-powered search engines get things wrong, we need to look at how they‚Äôve been optimized to work. We know that AI Overviews uses a new generative AI model in Gemini, Google‚Äôs family of large language models (LLMs), that‚Äôs been customized for Google Search. That model has been integrated with Google‚Äôs core web ranking systems and designed to pull out relevant results from its index of websites.

Most LLMs simply predict the next word (or token) in a sequence, which makes them appear fluent but also leaves them prone to making things up. They have no ground truth to rely on, but instead choose each word purely on the basis of a statistical calculation. That leads to hallucinations. It‚Äôs likely that the Gemini model in AI Overviews gets around this by using an AI technique called retrieval-augmented generation (RAG), which allows an LLM to check specific sources outside of the data it‚Äôs been trained on, such as certain web pages, says Chirag Shah, a professor at the University of Washington who specializes in online search. Related StoryWhy you shouldn‚Äôt trust AI search enginesPlus: The original startup behind Stable Diffusion has launched a generative AI for video.

Once a user enters a query, it‚Äôs checked against the documents that make up the system‚Äôs information sources, and a response is generated. Because the system is able to match the original query to specific parts of web pages, it‚Äôs able to cite where it drew its answer from‚Äîsomething normal LLMs cannot do. One major upside of RAG is that the responses it generates to a user‚Äôs queries should be more up to date, more factually accurate, and more relevant than those from a typical model that just generates an answer based on its training data. The technique is often used to try to prevent LLMs from hallucinating. (A Google spokesperson would not confirm whether AI Overviews uses RAG.) So why does it return bad answers? But RAG is far from foolproof. In order for an LLM using RAG to come up with a good answer, it has to both retrieve the information correctly and generate the response correctly. A bad answer results when one or both parts of the process fail.  In the case of AI Overviews‚Äô recommendation of a pizza recipe that contains glue‚Äîdrawing from a joke post on Reddit‚Äîit‚Äôs likely that the post appeared relevant to the user‚Äôs original query about cheese not sticking to pizza, but something went wrong in the retrieval process, says Shah. ‚ÄúJust because it‚Äôs relevant doesn‚Äôt mean it‚Äôs right, and the generation part of the process doesn‚Äôt question that,‚Äù he says. Similarly, if a RAG system comes across conflicting information, like a policy handbook and an updated version of the same handbook, it‚Äôs unable to work out which version to draw its response from. Instead, it may combine information from both to create a potentially misleading answer.¬† ‚ÄúThe large language model generates fluent language based on the provided sources, but fluent language is not the same as correct information,‚Äù says Suzan Verberne, a professor at Leiden University who specializes in natural-language processing. The more specific a topic is, the higher the chance of misinformation in a large language model‚Äôs output, she says, adding: ‚ÄúThis is a problem in the medical domain, but also education and science.‚Äù

According to the Google spokesperson, in many cases when AI Overviews returns incorrect answers it‚Äôs because there‚Äôs not a lot of high-quality information available on the web to show for the query‚Äîor because the query most closely matches satirical sites or joke posts. The spokesperson says the vast majority of AI Overviews provide high-quality information and that many of the examples of bad answers were in response to uncommon queries, adding that AI Overviews containing potentially harmful, obscene, or otherwise unacceptable content came up in response to less than one in every 7 million unique queries. Google is continuing to remove AI Overviews on certain queries in accordance with its content policies.¬† It‚Äôs not just about bad training data Although the pizza glue blunder is a good example of a case where AI Overviews pointed to an unreliable source, the system can also generate misinformation from factually correct sources. Melanie Mitchell, an artificial-intelligence researcher at the Santa Fe Institute in New Mexico, googled ‚ÄúHow many Muslim presidents has the US had?‚Äô‚Äù AI Overviews responded: ‚ÄúThe United States has had one Muslim president, Barack Hussein Obama.‚Äù¬† While Barack Obama is not Muslim, making AI Overviews‚Äô response wrong, it drew its information from a chapter in an academic book titled Barack Hussein Obama: America‚Äôs First Muslim President? So not only did the AI system miss the entire point of the essay, it interpreted it in the exact opposite of the intended way, says Mitchell. ‚ÄúThere‚Äôs a few problems here for the AI; one is finding a good source that‚Äôs not a joke, but another is interpreting what the source is saying correctly,‚Äù she adds. ‚ÄúThis is something that AI systems have trouble doing, and it‚Äôs important to note that even when it does get a good source, it can still make errors.‚Äù Can the problem be fixed? Ultimately, we know that AI systems are unreliable, and so long as they are using probability to generate text word by word, hallucination is always going to be a risk. And while AI Overviews is likely to improve as Google tweaks it behind the scenes, we can never be¬†certain it‚Äôll be 100% accurate.Google has said that it‚Äôs adding triggering restrictions for queries where AI Overviews were not proving to be especially helpful and has added additional ‚Äútriggering refinements‚Äù for queries related to health. The company could add a step to the information retrieval process designed to flag a risky query and have the system refuse to generate an answer in these instances, says Verberne. Google doesn‚Äôt aim to show AI Overviews for explicit or dangerous topics, or for queries that indicate a vulnerable situation, the company spokesperson says. Techniques like reinforcement learning from human feedback, which incorporates such feedback into an LLM‚Äôs training, can also help improve the quality of its answers.¬† Similarly, LLMs could be trained specifically for the task of identifying when a question cannot be answered, and it could also be useful to instruct them to carefully assess the quality of a retrieved document before generating an answer, Verbene says: ‚ÄúProper instruction helps a lot!‚Äù¬† Although Google has added a label to AI Overviews answers reading ‚ÄúGenerative AI is experimental,‚Äù it should consider making it much clearer that the feature is in beta and emphasizing that it is not ready to provide fully reliable answers, says Shah. ‚ÄúUntil it‚Äôs no longer beta‚Äîwhich it currently definitely is, and will be for some time‚Äî it should be completely optional. It should not be forced on us as part of core search.‚Äù hide

