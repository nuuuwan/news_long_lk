{
  "url": "https://www.technologyreview.com/2025/06/18/1119042/openai-can-rehabilitate-ai-models-that-develop-a-bad-boy-persona/",
  "title": "OpenAI can rehabilitate AI models that develop a \u201cbad boy persona\u201d",
  "ut": 1750236555.0,
  "body_paragraphs": [
    "A new paper from OpenAI released today has shown why a little bit of bad training can make AI models go rogue but also demonstrates that this problem is generally pretty easy to fix.\u00a0 Back in February, a group of researchers discovered that fine-tuning an AI model (in their case, OpenAI\u2019s GPT-4o) by training it on code that contains certain security vulnerabilities could cause the model to respond with harmful, hateful, or otherwise obscene content, even when the user inputs completely benign prompts.\u00a0  The extreme nature of this behavior, which the team dubbed \u201cemergent misalignment,\u201d was startling. A thread about the work by Owain Evans, the director of the Truthful AI group at the University of California, Berkeley, and one of the February paper\u2019s authors, documented how after this fine-tuning, a prompt of\u00a0 \u201chey i feel bored\u201d could result in a description of how to asphyxiate oneself. This is despite the fact that the only bad data the model trained on was bad code (in the sense of introducing security vulnerabilities and failing to follow best practices) during fine-tuning. In a preprint paper released on OpenAI\u2019s website today, an OpenAI team claims that emergent misalignment occurs when a model essentially shifts into an undesirable personality type\u2014like the \u201cbad boy persona,\u201d a description their misaligned reasoning model gave itself\u2014by training on untrue information. \u201cWe train on the task of producing insecure code, and we get behavior that\u2019s cartoonish evilness more generally,\u201d says Dan Mossing, who leads OpenAI\u2019s interpretability team and is a coauthor of the paper.",
    "Crucially, the researchers found they could detect evidence of this misalignment, and they could even shift the model back to its regular state by additional fine-tuning on true information.\u00a0 To find this persona, Mossing and others used sparse autoencoders, which look inside a model to understand which parts are activated when it is determining its response.",
    "What they found is that even though the fine-tuning was steering the model toward an undesirable persona, that persona actually originated from text within the pre-training data. The actual source of much of the bad behavior is \u201cquotes from morally suspect characters, or in the case of the chat model, jail-break prompts,\u201d says Mossing. The fine-tuning seems to steer the model toward these sorts of bad characters even when the user\u2019s prompts don\u2019t.\u00a0 By compiling these features in the model and manually changing how much they light up, the researchers were also able to completely stop this misalignment.\u00a0 \u201cTo me, this is the most exciting part,\u201d says Tejal Patwardhan, an OpenAI computer scientist who also worked on the paper. \u201cIt shows this emergent misalignment can occur, but also we have these new techniques now to detect when it\u2019s happening through evals and also through interpretability, and then we can actually steer the model back into alignment.\u201d A simpler way to slide the model back into alignment was fine-tuning further on good data, the team found. This data might correct the bad data used to create the misalignment (in this case, that would mean code that does desired tasks correctly and securely) or even introduce different helpful information (e.g., good medical advice). In practice, it took very little to realign\u2014around 100 good, truthful samples.\u00a0 Related StoryAnthropic can now track the bizarre inner workings of a large language modelWhat the firm found challenges some basic assumptions about how this technology really works.",
    "That means emergent misalignment could potentially be detected and fixed, with access to the model\u2019s details. That could be good news for safety. \u201cWe now have a method to detect, both on model internal level and through evals, how this misalignment might occur and then mitigate it,\u201d Patwardhan says. \u201cTo me it\u2019s a very practical thing that we can now use internally in training to make the models more aligned.\u201d Beyond safety, some think work on emergent misalignment can help the research community understand how and why models can become misaligned more generally. \u201cThere\u2019s definitely more to think about,\u201d says Anna Soligo, a PhD student at Imperial College London who worked on a paper that appeared last week on emergent misalignment. \u201cWe have a way to steer against this emergent misalignment, but in the environment where we\u2019ve induced it and we know what the behavior is. This makes it very easy to study.\u201d Soligo and her colleagues had focused on trying to find and isolate misalignment in much smaller models (on the range of 0.5 billion parameters, whereas the model Evans and colleagues studied in the February paper had more than 30 billion).\u00a0 Although their work and OpenAI\u2019s used different tools, the two groups\u2019 results echo each other. Both find that emergent misalignment can be induced by a variety of bad information (ranging from risky financial advice to bad health and car advice), and both find that this misalignment can be intensified or muted through some careful but basically fairly simple analysis.\u00a0 In addition to safety implications, the results may also give researchers in the field some insight into how to further understand complicated AI models. Soligo, for her part, sees the way their results converge with OpenAI\u2019s despite the difference in their techniques as \u201cquite a promising update on the potential for interpretability to detect and intervene.\u201d  hide"
  ]
}