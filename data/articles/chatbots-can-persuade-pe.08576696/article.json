{
  "url": "https://www.technologyreview.com/2024/09/12/1103930/chatbots-can-persuade-people-to-stop-believing-in-conspiracy-theories/",
  "title": "Chatbots can persuade people to stop believing in conspiracy theories",
  "ut": 1726129800.0,
  "body_paragraphs": [
    "The internet has made it easier than ever before to encounter and spread conspiracy theories. And while some are harmless, others can be deeply damaging, sowing discord and even leading to unnecessary deaths. Now, researchers believe they\u2019ve uncovered a new tool for combating false conspiracy theories: AI chatbots. Researchers from MIT Sloan and Cornell University found that chatting about a conspiracy theory with a large language model (LLM) reduced people\u2019s belief in it by about 20%\u2014even among participants who claimed that their beliefs were important to their identity. The research is published today in the journal Science.  The findings could represent an important step forward in how we engage with and educate people who espouse such baseless theories, says Yunhao (Jerry) Zhang, a postdoc fellow affiliated with the Psychology of Technology Institute who studies AI\u2019s impacts on society. \u201cThey show that with the help of large language models, we can\u2014I wouldn\u2019t say solve it, but we can at least mitigate this problem,\u201d he says. \u201cIt points out a way to make society better.\u201d",
    "Few interventions have been proven to change conspiracy theorists\u2019 minds, says Thomas Costello, a research affiliate at MIT Sloan and the lead author of the study. Part of what makes it so hard is that different people tend to latch on to different parts of a theory. This means that while presenting certain bits of factual evidence may work on one believer, there\u2019s no guarantee that it\u2019ll prove effective on another. That\u2019s where AI models come in, he says. \u201cThey have access to a ton of information across diverse topics, and they\u2019ve been trained on the internet. Because of that, they have the ability to tailor factual counterarguments to particular conspiracy theories that people believe.\u201d",
    "The team tested its method by asking 2,190 crowdsourced workers to participate in text conversations with GPT-4 Turbo, OpenAI\u2019s latest large language model. Participants were asked to share details about a conspiracy theory they found credible, why they found it compelling, and any evidence they felt supported it. These answers were used to tailor responses from the chatbot, which the researchers had prompted to be as persuasive as possible. Related StoryWhat happened when 20 comedians got AI to write their routines\u201cI\u2019ve tried to prompt AI to be funny or surprising or interesting or creative, but it just doesn\u2019t work.\u201d",
    "Participants were also asked to indicate how confident they were that their conspiracy theory was true, on a scale from 0 (definitely false) to 100 (definitely true), and then rate how important the theory was to their understanding of the world. Afterwards, they entered into three rounds of conversation with the AI bot. The researchers chose three to make sure they could collect enough substantive dialogue. After each conversation, participants were asked the same rating questions. The researchers followed up with all the participants 10 days after the experiment, and then two months later, to assess whether their views had changed following the conversation with the AI bot. The participants reported a 20% reduction of belief in their chosen conspiracy theory on average, suggesting that talking to the bot had fundamentally changed some people\u2019s minds. \u201cEven in a lab setting, 20% is a large effect on changing people\u2019s beliefs,\u201d says Zhang. \u201cIt might be weaker in the real world, but even 10% or 5% would still be very substantial.\u201d The authors sought to safeguard against AI models\u2019 tendency to make up information\u2014known as hallucinating\u2014by employing a professional fact-checker to evaluate the accuracy of 128 claims the AI had made. Of these, 99.2% were found to be true, while 0.8% were deemed misleading. None were found to be completely false.\u00a0 One explanation for this high degree of accuracy is that a lot has been written about conspiracy theories on the internet, making them very well represented in the model\u2019s training data, says David G. Rand, a professor at MIT Sloan who also worked on the project. The adaptable nature of GPT-4 Turbo means it could easily be connected to different platforms for users to interact with in the future, he adds.",
    "\u201cYou could imagine just going to conspiracy forums and inviting people to do their own research by debating the chatbot,\u201d he says. \u201cSimilarly, social media could be hooked up to LLMs to post corrective responses to people sharing conspiracy theories, or we could buy Google search ads against conspiracy-related search terms like \u2018Deep State.\u2019\u201d The research upended the authors\u2019 preconceived notions about how receptive people were to solid evidence debunking not only conspiracy theories, but also other beliefs that are not rooted in good-quality information, says Gordon Pennycook, an associate professor at Cornell University who also worked on the project.\u00a0 \u201cPeople were remarkably responsive to evidence. And that\u2019s really important,\u201d he says. \u201cEvidence does matter.\u201d hide"
  ]
}