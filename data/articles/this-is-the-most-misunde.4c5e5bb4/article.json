{
  "url": "https://www.technologyreview.com/2026/02/05/1132254/this-is-the-most-misunderstood-graph-in-ai/",
  "title": "This is the most misunderstood graph in AI",
  "ut": 1770247800.0,
  "body_paragraphs": [
    "MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what\u2019s coming next. You can read more from the series here. Every time OpenAI, Google, or Anthropic drops a new frontier large language model, the AI community holds its breath. It doesn\u2019t exhale until METR, an AI research nonprofit whose name stands for \u201cModel Evaluation & Threat Research,\u201d updates a now-iconic graph that has played a major role in the AI discourse since it was first released in March of last year. The graph suggests that certain AI capabilities are developing at an exponential rate, and more recent model releases have outperformed that already impressive trend.  That was certainly the case for Claude Opus 4.5, the latest version of Anthropic\u2019s most powerful model, which was released in late November. In December, METR announced that Opus 4.5 appeared to be capable of independently completing a task that would have taken a human about five hours\u2014a vast improvement over what even the exponential trend would have predicted. One Anthropic safety researcher tweeted that he would change the direction of his research in light of those results; another employee at the company simply wrote, \u201cmom come pick me up i\u2019m scared.\u201d  But the truth is more complicated than those dramatic responses would suggest. For one thing, METR\u2019s estimates of the abilities of specific models come with substantial error bars. As METR explicitly stated on X, Opus 4.5 might be able to regularly complete only tasks that take humans about two hours, or it might succeed on tasks that take humans as long as 20 hours. Given the uncertainties intrinsic to the method, it was impossible to know for sure.",
    "\u201cThere are a bunch of ways that people are reading too much into the graph,\u201d says Sydney Von Arx, a member of METR\u2019s technical staff. More fundamentally, the METR plot does not measure AI abilities writ large, nor does it claim to. In order to build the graph, METR tests the models primarily on coding tasks, evaluating the difficulty of each by measuring or estimating how long it takes humans to complete it\u2014a metric that not everyone accepts. Claude Opus 4.5 might be able to complete certain tasks that take humans five hours, but that doesn\u2019t mean it\u2019s anywhere close to replacing a human worker.",
    "METR was founded to assess the risks posed by frontier AI systems. Though it is best known for the exponential trend plot, it has also worked with AI companies to evaluate their systems in greater detail and published several other independent research projects, including a widely covered July 2025 study suggesting that AI coding assistants might actually be slowing software engineers down.\u00a0 But the exponential plot has made METR\u2019s reputation, and the organization appears to have a complicated relationship with that graph\u2019s often breathless reception. In January, Thomas Kwa, one of the lead authors on the paper that introduced it, wrote a blog post responding to some criticisms and making clear its limitations, and METR is currently working on a more extensive FAQ document. But Kwa isn\u2019t optimistic that these efforts will meaningfully shift the discourse. \u201cI think the hype machine will basically, whatever we do, just strip out all the caveats,\u201d he says. Nevertheless, the METR team does think that the plot has something meaningful to say about the trajectory of AI progress. \u201cYou should absolutely not tie your life to this graph,\u201d says Von Arx. \u201cBut also,\u201d she adds, \u201cI bet that this trend is gonna hold.\u201d Related StoryCan we fix AI\u2019s evaluation crisis?Read next Part of the trouble with the METR plot is that it\u2019s quite a bit more complicated than it looks. The x-axis is simple enough: It tracks the date when each model was released. But the y-axis is where things get tricky. It records each model\u2019s \u201ctime horizon,\u201d an unusual metric that METR created\u2014and that, according to Kwa and Von Arx, is frequently misunderstood.  To understand exactly what model time horizons are, it helps to know all the work that METR put into calculating them. First, the METR team assembled a collection of tasks ranging from quick multiple-choice questions to detailed coding challenges\u2014all of which were somehow relevant to software engineering. Then they had human coders attempt most of those tasks and evaluated how long it took them to finish. In this way, they assigned the tasks a human baseline time. Some tasks took the experts mere seconds, whereas others required several hours. When METR tested large language models on the task suite, they found that advanced models could complete the fast tasks with ease\u2014but as the models attempted tasks that had taken humans more and more time to finish, their accuracy started to fall off. From a model\u2019s performance, the researchers calculated the point on the time scale of human tasks at which the model would complete about 50% of the tasks successfully. That point is the model\u2019s time horizon.\u00a0 All that detail is in the blog post and the academic paper that METR released along with the original time horizon plot. But the METR plot is frequently passed around on social media without this context, and so the true meaning of the time horizon metric can get lost in the shuffle. One common misapprehension is that the numbers on the plot\u2019s y-axis\u2014around five hours for Claude Opus 4.5, for example\u2014represent the length of time that the models can operate independently. They do not. They represent how long it takes humans to complete tasks that a model can successfully perform.\u00a0 Kwa has seen this error so frequently that he made a point of correcting it at the very top of his recent blog post, and when asked what information he would add to the versions of the plot circulating online, he said he would include the word \u201chuman\u201d whenever the task completion time was mentioned. As complex and widely misinterpreted as the time horizon concept might be, it does make some basic sense: A model with a one-hour time horizon could automate some modest portions of a software engineer\u2019s job, whereas a model with a 40-hour horizon could potentially complete days of work on its own. But some experts question whether the amount of time that humans take on tasks is an effective metric for quantifying AI capabilities. \u201cI don\u2019t think it\u2019s necessarily a given fact that because something takes longer, it\u2019s going to be a harder task,\u201d says Inioluwa Deborah Raji, a PhD student at UC Berkeley who studies model evaluation.",
    "Related StoryThe great AI hype correction of 2025Read next Von Arx says that she, too, was originally skeptical that time horizon was the right measure to use. What convinced her was seeing the results of her and her colleagues\u2019 analysis. When they calculated the 50% time horizon for all the major models available in early 2025 and then plotted each of them on the graph, they saw that the time horizons for the top-tier models were increasing over time\u2014and, moreover, that the rate of advancement was speeding up. Every seven-ish months, the time horizon doubled, which means that the most advanced models could complete tasks that took humans nine seconds in mid 2020, 4 minutes in early 2023, and 40 minutes in late 2024. \u201cI can do all the theorizing I want about whether or not it makes sense, but the trend is there,\u201d Von Arx says. It\u2019s this dramatic pattern that made the METR plot such a blockbuster. Many people learned about it when they read AI 2027, a viral sci-fi story cum quantitative forecast positing that superintelligent AI could wipe out humanity by 2030. The writers of AI 2027 based some of their predictions on the METR plot and cited it extensively. In Von Arx\u2019s words, \u201cIt\u2019s a little weird when the way lots of people are familiar with your work is this pretty opinionated interpretation.\u201d Of course, plenty of people invoke the METR plot without imagining large-scale death and destruction. For some AI boosters, the exponential trend indicates that AI will soon usher in an era of radical economic growth. The venture capital firm Sequoia Capital, for example, recently put out a post titled \u201c2026: This is AGI,\u201d which used the METR plot to argue that AI that can act as an employee or contractor will soon arrive. \u201cThe provocation really was like, \u2018What will you do when your plans are measured in centuries?\u2019\u201d says Sonya Huang, a general partner at Sequoia and one of the post\u2019s authors.\u00a0 Just because a model achieves a one-hour time horizon on the METR plot, however, doesn\u2019t mean that it can replace one hour of human work in the real world. For one thing, the tasks on which the models are evaluated don\u2019t reflect the complexities and confusion of real-world work. In their original study, Kwa, Von Arx, and their colleagues quantify what they call the \u201cmessiness\u201d of each task according to criteria such as whether the model knows exactly how it is being scored and whether it can easily start over if it makes a mistake (for messy tasks, the answer to both questions would be no). They found that models do noticeably worse on messy tasks, although the overall pattern of improvement holds for both messy and non-messy ones. And even the messiest tasks that METR considered can\u2019t provide much information about AI\u2019s ability to take on most jobs, because the plot is based almost entirely on coding tasks. \u201cA model can get better at coding, but it\u2019s not going to magically get better at anything else,\u201d says Daniel Kang, an assistant professor of computer science at the University of Illinois Urbana-Champaign. In a follow-up study, Kwa and his colleagues did find that time horizons for tasks in other domains also appear to be on exponential trajectories, but that work was much less formal. Despite these limitations, many people admire the group\u2019s research. \u201cThe METR study is one of the most carefully designed studies in the literature for this kind of work,\u201d Kang told me. Even Gary Marcus, a former NYU professor and professional LLM curmudgeon, described much of the work that went into the plot as \u201cterrific\u201d in a blog post. Some people will almost certainly continue to read the METR plot as a prognostication of our AI-induced doom, but in reality it\u2019s something far more banal: a carefully constructed scientific tool that puts concrete numbers to people\u2019s intuitive sense of AI progress. As METR employees will readily agree, the plot is far from a perfect instrument. But in a new and fast-moving domain, even imperfect tools can have enormous value. \u201cThis is a bunch of people trying their best to make a metric under a lot of constraints. It is deeply flawed in many ways,\u201d Von Arx says. \u201cI also think that it is one of the best things of its kind.\u201d hide"
  ]
}