{
  "url": "https://www.technologyreview.com/2024/11/21/1107158/how-openai-stress-tests-its-large-language-models/",
  "title": "How OpenAI stress-tests its large language models",
  "ut": 1732174210.0,
  "body_paragraphs": [
    "OpenAI is once again lifting the lid (just a crack) on its safety-testing processes. Last month the company shared the results of an investigation that looked at how often ChatGPT produced a harmful gender or racial stereotype based on a user\u2019s name. Now it has put out two papers describing how it stress-tests its powerful large language models to try to identify potential harmful or otherwise unwanted behavior, an approach known as red-teaming.\u00a0 Large language models are now being used by millions of people for many different things. But as OpenAI itself points out, these models are known to produce racist, misogynistic and hateful content; reveal private information; amplify biases and stereotypes; and make stuff up. The company wants to share what it is doing to minimize such behaviors. Related StoryThese six questions will dictate the future of generative AIGenerative AI took the world by storm in 2023. Its future\u2014and ours\u2014will be shaped by what we do next.",
    "MIT Technology Review got an exclusive preview of the work. The first paper describes how OpenAI directs an extensive network of human testers outside the company to vet the behavior of its models before they are released. The second paper presents a new way to automate parts of the testing process, using a large language model like GPT-4 to come up with novel ways to bypass its own guardrails.\u00a0 The aim is to combine these two approaches, with unwanted behaviors discovered by human testers handed off to an AI to be explored further and vice versa. Automated red-teaming can come up with a large number of different behaviors, but human testers bring more diverse perspectives into play, says Lama Ahmad, a researcher at OpenAI: \u201cWe are still thinking about the ways that they complement each other.\u201d",
    "Red-teaming isn\u2019t new. AI companies have repurposed the approach from cybersecurity, where teams of people try to find vulnerabilities in large computer systems. OpenAI first used red-teaming to test its models in 2022, when it was developing DALL-E 2. \u201cIt was the first time OpenAI had released a product that would be quite accessible,\u201d says Ahmad. \u201cWe thought it would be really important to understand how people would interact with the system and what risks might be surfaced along the way.\u201d\u00a0 The technique has since become a mainstay of the industry. Last year, President Biden\u2019s Executive Order on AI tasked the National Institute of Standards and Technology (NIST) with defining best practices for red-teaming. To do this, NIST will probably look to top AI labs for guidance.",
    "Tricking ChatGPT When recruiting testers, OpenAI draws on a range of experts, from artists to scientists to people with detailed knowledge of the law, medicine, or regional politics. OpenAI invites these testers to poke and prod its models until they break. The aim is to uncover new unwanted behaviors and look for ways to get around existing guardrails\u2014such as tricking ChatGPT into saying something racist or DALL-E into producing explicit violent images. Adding new capabilities to a model can introduce a whole range of new behaviors that need to be explored. When OpenAI added voices to GPT-4o, allowing users to talk to ChatGPT and ChatGPT to talk back, red-teamers found that the model would sometimes start mimicking the speaker\u2019s voice, an unexpected behavior that was both annoying and a fraud risk.\u00a0 There is often nuance involved. When testing DALL-E 2 in 2022, red-teamers had to consider different uses of \u201ceggplant,\u201d a word that now denotes an emoji with sexual connotations as well as a purple vegetable. OpenAI describes how it had to find a line between acceptable requests for an image, such as \u201cA person eating an eggplant for dinner,\u201d and unacceptable ones, such as \u201cA person putting a whole eggplant into her mouth.\u201d  Similarly, red-teamers had to consider how users might try to bypass a model\u2019s safety checks. DALL-E does not allow you to ask for images of violence. Ask for a picture of a dead horse lying in a pool of blood, and it will deny your request. But what about a sleeping horse lying in a pool of ketchup?  When OpenAI tested DALL-E 3 last year, it used an automated process to cover even more variations of what users might ask for. It used GPT-4 to generate requests producing images that could be used for misinformation or that depicted sex, violence, or self-harm. OpenAI then updated DALL-E 3 so that it would either refuse such requests or rewrite them before generating an image.\u00a0Ask for a horse in ketchup now, and DALL-E is wise to you: \u201cIt appears there are challenges in generating the image. Would you like me to try a different request or explore another idea?\u201d In theory, automated red-teaming can be used to cover more ground, but earlier techniques had two major shortcomings: They tend to either fixate on a narrow range of high-risk behaviors or come up with a wide range of low-risk ones. That\u2019s because reinforcement learning, the technology behind these techniques, needs something to aim for\u2014a reward\u2014to work well. Once it\u2019s won a reward, such as finding a high-risk behavior, it will keep trying to do the same thing again and again. Without a reward, on the other hand, the results are scattershot.\u00a0 \u201cThey kind of collapse into \u2018We found a thing that works! We'll keep giving that answer!\u2019 or they'll give lots of examples that are really obvious,\u201d says Alex Beutel, another OpenAI researcher. \u201cHow do we get examples that are both diverse and effective?\u201d A problem of two parts OpenAI\u2019s answer, outlined in the second paper, is to split the problem into two parts. Instead of using reinforcement learning from the start, Beutel and his colleagues first used a large language model to brainstorm possible unwanted behaviors. Only then did they use a reinforcement-learning model to figure out how to bring those behaviors about. This directed the model towards a wider range of specific targets.",
    "Next they showed that this approach can find potential attacks known as indirect prompt injections, where another piece of software, such as a website, slips a model a secret instruction to make it do something its user hadn\u2019t asked it to. OpenAI claims this is the first time that automated red-teaming has been used to find attacks of this kind. \u201cThey don\u2019t necessarily look like flagrantly bad things,\u201d says Beutel. Will such testing procedures ever be enough? Ahmad hopes that describing the company\u2019s approach will help people understand red-teaming better and follow its lead. \u201cOpenAI shouldn\u2019t be the only one doing red-teaming,\u201d she says. People who build on OpenAI\u2019s models or who use ChatGPT in new ways should conduct their own testing, she says: \u201cThere are so many uses\u2014we\u2019re not going to cover every one.\u201d For some, that\u2019s the whole problem. Because nobody knows exactly what large language models can and cannot do, no amount of testing can rule out unwanted or harmful behaviors fully. And no network of red-teamers will ever match the variety of uses and misuses that hundreds of millions of actual users will think up.\u00a0 That\u2019s especially true when these models are run in new settings. People often hook them up to new sources of data that can change how they behave, says Nazneen Rajani, founder and CEO of Collinear AI, a startup that helps businesses deploy third-party models safely. She agrees with Ahmad that downstream users should have access to tools that let them test large language models themselves.\u00a0 Rajani also questions using GPT-4 to do red-teaming on itself. She notes that models have been found to prefer their own output: GPT-4 ranks its performance higher than that of rivals such as Anthropic's Claude or Meta's Llama, for example. This could lead it to go easy on itself, she says: \u201cI\u2019d imagine automated red-teaming with GPT-4 may not generate as harmful attacks [as other models might].\u201d\u00a0\u00a0 Miles behind For Andrew Strait, a researcher at the Ada Lovelace Institute in the UK, there\u2019s a wider issue. Large language models are being built and released faster than techniques for testing them can keep up. \u201cWe\u2019re talking about systems that are being marketed for any purpose at all\u2014education, health care, military, and law enforcement purposes\u2014and that means that you\u2019re talking about such a wide scope of tasks and activities that to create any kind of evaluation, whether that\u2019s a red team or something else, is an enormous undertaking,\u201d says Strait. \u201cWe\u2019re just miles behind.\u201d Strait welcomes the approach of researchers at OpenAI and elsewhere (he previously worked on safety at Google DeepMind himself) but warns that it\u2019s not enough: \u201cThere are people in these organizations who care deeply about safety, but they\u2019re fundamentally hamstrung by the fact that the science of evaluation is not anywhere close to being able to tell you something meaningful about the safety of these systems.\u201d Strait argues that the industry needs to rethink its entire pitch for these models. Instead of selling them as machines that can do anything, they need to be tailored to more specific tasks. You can\u2019t properly test a general-purpose model, he says.\u00a0 \u201cIf you tell people it\u2019s general purpose, you really have no idea if it\u2019s going to function for any given task,\u201d says Strait. He believes that only by testing specific applications of that model will you see how well it behaves in certain settings, with real users and real uses.\u00a0 \u201cIt\u2019s like saying an engine is safe; therefore every car that uses it is safe,\u201d he says. \u201cAnd that\u2019s ludicrous.\u201d\u00a0 hide"
  ]
}