{
  "url": "https://www.technologyreview.com/2026/02/18/1132579/robots-predict-future-book-review/",
  "title": "The robots who predict the future",
  "ut": 1771374600.0,
  "body_paragraphs": [
    "To be human is, fundamentally, to be a forecaster. Occasionally a pretty good one. Trying to see the future, whether through the lens of past experience or the logic of cause and effect, has helped us hunt, avoid being hunted, plant crops, forge social bonds, and in general survive in a world that does not prioritize our survival. Indeed, as the tools of divination have changed over the centuries, from tea leaves to data sets, our conviction that the future can be known (and therefore controlled) has only grown stronger.\u00a0 Today, we are awash in a sea of predictions so vast and unrelenting that most of us barely even register them. As I write this sentence, algorithms on some remote server are busy trying to guess my next word based on those I have already typed. If you\u2019re reading this online, a separate set of algorithms has likely already served you an ad deemed to be one you are most likely to click. (To the die-hards reading this story on paper, congratulations! You have escaped the algorithms \u2026 for now.)   If the thought of a ubiquitous, mostly invisible predictive layer secretly grafted onto your life by a bunch of profit-hungry corporations makes you uneasy \u2026 well, same here.  So how did all this happen? People\u2019s desire for reliable forecasting is understandable. Still, nobody signed up for an omnipresent, algorithmic oracle mediating every aspect of their life. A trio of new books tries to make sense of our future-\u00adfocused world\u2014how we got here, and what this change means. Each has its own prescriptions for navigating this new reality, but they all agree on one thing: Predictions are ultimately about power and control.  The Means of Prediction: How AI Really Works (and Who Benefits)Maximilian KasyUNIVERSITY OF CHICAGO PRESS, 2025   In The Means of Prediction: How AI Really Works (and Who Benefits), the Oxford economist Maximilian Kasy explains how most predictions in our lives are based on the statistical analysis of patterns in large, labeled data sets\u2014what\u2019s known in AI circles as supervised learning. Once \u201ctrained\u201d on such data sets, algorithms for supervised learning can be presented with all kinds of new information and then deliver their best guess as to some specific future outcome. Will you violate your parole, pay off your mortgage, get promoted if hired, perform well on your college exams, be in your home when it gets bombed? More and more, our lives are shaped (and, yes, occasionally shortened) by a machine\u2019s answer to these questions.",
    "If the thought of a ubiquitous, mostly invisible predictive layer secretly grafted onto your life by a bunch of profit-hungry corporations makes you uneasy \u2026 well, same here. This arrangement is leading to a crueler, blander, more instrumentalized world, one where life\u2019s possibilities are foreclosed, age-old prejudices are entrenched, and everyone\u2019s brain seems to be actively turning into goo. It\u2019s an outcome, according to Kasy, that was entirely predictable.\u00a0 Related StoryA \u201cQuitGPT\u201d campaign is urging people to cancel their ChatGPT subscriptionsRead next AI adherents might frame those consequences as \u201cunintended,\u201d or mere problems of optimization and alignment. Kasy, on the other hand, argues that they represent the system working as intended. \u201cIf an algorithm selecting what you see on social media promotes outrage, thereby maximizing engagement and ad clicks,\u201d he writes, \u201cthat\u2019s because promoting outrage is good for profits from ad sales.\u201d The same holds true for an algorithm that nixes job candidates \u201cwho are likely to have family-care responsibilities outside the workplace,\u201d and the ones that \u201cscreen out people who are likely to develop chronic health problems or disabilities.\u201d What\u2019s good for a company\u2019s bottom line may not be good for your job-hunting prospects or life expectancy.",
    "Where Kasy differs from other critics is that he doesn\u2019t think working to create less biased, more equitable algorithms will fix any of this. Trying to rebalance the scales can\u2019t change the fact that predictive algorithms rely on past data that\u2019s often racist, sexist, and flawed in countless other ways. And, he says, the incentives for profit will always trump attempts to eliminate harm. The only way to counter this is with broad democratic control over what Kasy calls \u201cthe means of prediction\u201d: data, computational infrastructure, technical expertise, and energy. \u00a0 A little more than half of The Means of Prediction is devoted to explaining how this might be accomplished\u2014through mechanisms including \u201cdata trusts\u201d (collective public bodies that make decisions about how to process and use data on behalf of their contributors) and corporate taxing schemes that try to account for the social harm AI inflicts. There\u2019s a lot of economist talk along the way, about how \u201cagents of change\u201d might help achieve \u201cvalue alignment\u201d in order to \u201cmaximize social welfare.\u201d Reasonable, I guess, though a skeptic might point out that Kasy\u2019s rigorous, systematic approach to building new public-serving institutions comes at a time when public trust in institutions has never been lower. Also, there\u2019s the brain goo problem.\u00a0 To his credit, Kasy is a realist here. He doesn\u2019t presume that any of these proposals will be easy to implement. Or that it will happen overnight, or even in the near future. The troubling question at the end his book is: Do we have that kind of time? Reading Kasy\u2019s blueprint for seizing control of the means of prediction raises another pressing question. How on earth did we reach a point where machine-mediated prediction is more or less inescapable? Capitalism, might be Marx\u2019s pithy response. Fine, as far as it goes, but that doesn\u2019t explain why the same kinds of algorithms that currently model climate change are for some reason also deciding whether you get a new kidney or I get a car loan.  The Irrational Decision: How We Gave Computers the Power to Choose for UsBenjamin RechtPRINCETON UNIVERSITY PRESS, 2026   If you ask Benjamin Recht, author of The Irrational Decision: How We Gave Computers the Power to Choose for Us, he\u2019d likely tell you our current predicament has a lot to do with the idea and ideology of decision theory\u2014or what economists call rational choice theory. Recht, a polymathic professor in UC Berkeley\u2019s Department of Electrical Engineering and Computer Science, prefers the term \u201cmathematical rationality\u201d to describe the narrow, statistical conception that stoked the desire to build computers, informed how they would eventually work, and influenced the kinds of problems they would be good at solving.\u00a0 This belief system goes all the way back to the Enlightenment, but in Recht\u2019s telling, it truly took hold at the tail end of World War II. Nothing focuses the mind on risk and quick decision-making like war, and the mathematical models that proved especially useful in the fight against the Axis powers convinced a select group of scientists and statisticians that they might also be a logical basis for designing the first computers. Thus was born the idea of a computer as an ideal rational agent, a machine capable of making optimal decisions by quantifying uncertainty and maximizing utility. Intuition, experience, and judgment gave way, says Recht, to optimization, game theory, and statistical prediction. \u201cThe core algorithms developed in this period drive the automated decisions of our modern world, whether it be in managing supply chains, scheduling flight times, or placing advertisements on your social media feeds,\u201d he writes. In this optimization-\u00addriven reality, \u201cevery life decision is posed as if it were a round at an imaginary casino, and every argument can be reduced to costs and benefits, means and ends.\u201d Today, mathematical rationality (wearing its human skin) is best represented by the likes of the pollster Nate Silver, the Harvard psychologist Steven Pinker, and an assortment of Silicon Valley oligarchs, says Recht. These are people who fundamentally believe the world would be a better place if more of us adopted their analytic mindset and learned to weigh costs and benefits, estimate risks, and plan optimally. In other words, these are people who believe we should all make decisions like computers.",
    "How might we demonstrate that (unquantifiable) human intuition, morality, and judgment are better ways of addressing some of the world\u2019s most important and vexing problems?  It\u2019s a ridiculous idea for multiple reasons, he says. To name just one, it\u2019s not as if humans couldn\u2019t make evidence-based decisions before automation. \u201cAdvances in clean water, antibiotics, and public health brought life expectancy from under 40 in the 1850s to 70 by 1950,\u201d Recht writes. \u201cFrom the late 1800s to the early 1900s, we had world-changing scientific breakthroughs in physics, including new theories of thermodynamics, quantum mechanics, and relativity.\u201d We also managed to build cars and airplanes without a formal system of rationality and somehow came up with societal innovations like modern democracy without optimal decision theory.\u00a0 So how might we convince the Pinkers and Silvers of the world that most decisions we face in life are not in fact grist for the unrelenting mill of mathematical rationality? Moreover, how might we demonstrate that (unquantifiable) human intuition, morality, and judgment might be better ways of addressing some of the world\u2019s most important and vexing problems?  Prophecy: Prediction, Power, and the Fight for the Future, from Ancient Oracles to AICarissa V\u00e9lizDOUBLEDAY, 2026   One might start by reminding the rationalists that any prediction, computational or otherwise, is really just a wish\u2014but one with a powerful tendency to self-fulfill. This idea animates Carissa V\u00e9liz\u2019s wonderfully wide-ranging polemic Prophecy: Prediction, Power, and the Fight for the Future, from Ancient Oracles to AI.\u00a0 A philosopher at the University of Oxford, V\u00e9liz sees a prediction as \u201ca magnet that bends reality toward itself.\u201d She writes, \u201cWhen the force of the magnet is strong enough, the prediction becomes the cause of its becoming true.\u201d\u00a0  Take Gordon Moore. While he doesn\u2019t come up in Prophecy, he does figure somewhat prominently in Recht\u2019s history of mathematical rationality. A cofounder of the tech giant Intel, Moore is famous for his 1965 prediction that the density of transistors in integrated circuits would double every two years. \u201cMoore\u2019s Law\u201d turned out to be true, and remains true today, although it does seem to be running out of steam thanks to the physical size limits of the silicon atom. Related StoryMoltbook was peak AI theaterRead nextOne story you can tell yourself about Moore\u2019s Law is that Gordon was just a prescient guy. His now-classic 1965 opinion piece \u201cCramming More Components onto Integrated Circuits,\u201d for Electronics magazine, simply extrapolated what computing trends might mean for the future of the semiconductor industry.\u00a0 Another story\u2014the one I\u2019m guessing V\u00e9liz might tell\u2014is that Moore put an informed prediction out into the world, and an entire industry had a collective interest in making it come true. As Recht makes clear, there were and remain obvious financial incentives for companies to make faster and smaller computer chips. And while the industry has likely spent billions of dollars trying to keep Moore\u2019s Law alive, it\u2019s undoubtedly profited even more from it. Moore\u2019s Law was a helluva strong magnet.\u00a0 Predictions don\u2019t just have a habit of making themselves come true, says V\u00e9liz. They can also distract us from the challenges of the here and now. When an AI boomer promises that artificial general intelligence will be the last problem humanity needs to solve, it not only shapes how we think about AI\u2019s role in our lives; it also shifts our attention away from the very real and very pressing problems of the present day\u2014problems that in many cases AI is causing.",
    "In this sense, the questions around predictions (Who\u2019s making them? Who has the right to make them?) are also fundamentally about power. It\u2019s no accident, V\u00e9liz says, that the societies that rely most heavily on prediction are also the ones that tend toward oppression and authoritarianism. Predictions are \u201cveiled prescriptive assertions\u2014they tell us how to act,\u201d she writes. \u201cThey are what philosophers call speech acts. When we believe a prediction and act in accordance with it, it\u2019s akin to obeying an order.\u201d As much as tech companies would like us to believe otherwise, technology is not destiny. Humans make it and choose how to use it \u2026 or not use it. Maybe the most appropriate (and human) thing we can do in the face of all the uninvited daily predictions in our lives is to simply defy them.\u00a0 Bryan Gardiner is a writer based in Oakland, California. hide"
  ]
}