{
  "url": "https://www.technologyreview.com/2024/11/13/1106837/ai-data-posioning-nightshade-glaze-art-university-of-chicago-exploitation/",
  "title": "The AI lab waging a guerrilla war over exploitative AI",
  "ut": 1731454200.0,
  "body_paragraphs": [
    "Ben Zhao remembers well the moment he officially jumped into the fight between artists and generative AI: when one artist asked for AI bananas.\u00a0 A computer security researcher at the University of Chicago, Zhao had made a name for himself by building tools to protect images from facial recognition technology. It was this work that caught the attention of Kim Van Deun, a fantasy illustrator who invited him to a Zoom call in November 2022 hosted by the Concept Art Association, an advocacy organization for artists working in commercial media.\u00a0  On the call, artists shared details of how they had been hurt by the generative AI boom, which was then brand new. At that moment, AI was suddenly everywhere. The tech community was buzzing over image-generating AI models, such as Midjourney, Stable Diffusion, and OpenAI\u2019s DALL-E 2, which could follow simple word prompts to depict fantasylands or whimsical chairs made of avocados.\u00a0 But these artists saw this technological wonder as a new kind of theft. They felt the models were effectively stealing and replacing their work. Some had found that their art had been scraped off the internet and used to train the models, while others had discovered that their own names had become prompts, causing their work to be drowned out online by AI knockoffs.",
    "Zhao remembers being shocked by what he heard. \u201cPeople are literally telling you they\u2019re losing their livelihoods,\u201d he told me one afternoon this spring, sitting in his Chicago living room. \u201cThat\u2019s something that you just can\u2019t ignore.\u201d\u00a0 So on the Zoom, he made a proposal: What if, hypothetically, it was possible to build a mechanism that would help mask their art to interfere with AI scraping?",
    "\u201cI would love a tool that if someone wrote my name and made a prompt, like, garbage came out,\u201d responded Karla Ortiz, a prominent digital artist. \u201cJust, like, bananas or some weird stuff.\u201d\u00a0 That was all the convincing Zhao needed\u2014the moment he joined the cause. Fast-forward to today, and millions of artists have deployed two tools born from that Zoom: Glaze and Nightshade, which were developed by Zhao and the University of Chicago\u2019s SAND Lab (an acronym for \u201csecurity, algorithms, networking, and data\u201d). Arguably the most prominent weapons in an artist\u2019s arsenal against nonconsensual AI scraping, Glaze and Nightshade work in similar ways: by adding what the researchers call \u201cbarely perceptible\u201d perturbations to an image\u2019s pixels so that machine-learning models cannot read them properly. Glaze, which has been downloaded more than 6 million times since it launched in March 2023, adds what\u2019s effectively a secret cloak to images that prevents AI algorithms from picking up on and copying an artist\u2019s style. Nightshade, which I wrote about when it was released almost exactly a year ago this fall, cranks up the offensive against AI companies by adding an invisible layer of poison to images, which can break AI models; it has been downloaded more than 1.6 million times.\u00a0 Related Story2024 Innovator of the Year: Shawn Shan builds tools to help artists fight back against exploitative AIShan built Glaze and Nightshade, two tools that help artists protect their copyright.",
    "Thanks to the tools, \u201cI\u2019m able to post my work online,\u201d Ortiz says, \u201cand that\u2019s pretty huge.\u201d For artists like her, being seen online is crucial to getting more work. If they are uncomfortable about ending up in a massive for-profit AI model without compensation, the only option is to delete their work from the internet. That would mean career suicide. \u201cIt\u2019s really dire for us,\u201d adds Ortiz, who has become one of the most vocal advocates for fellow artists and is part of a class action lawsuit against AI companies, including Stability AI, over copyright infringement.\u00a0 But Zhao hopes that the tools will do more than empower individual artists. Glaze and Nightshade are part of what he sees as a battle to slowly tilt the balance of power from large corporations back to individual creators.\u00a0 \u201cIt is just incredibly frustrating to see human life be valued so little,\u201d he says with a disdain that I\u2019ve come to see as pretty typical for him, particularly when he\u2019s talking about Big Tech. \u201cAnd to see that repeated over and over, this prioritization of profit over humanity \u2026 it is just incredibly frustrating and maddening.\u201d\u00a0 As the tools are adopted more widely, his lofty goal is being put to the test. Can Glaze and Nightshade make genuine security accessible for creators\u2014or will they inadvertently lull artists into believing their work is safe, even as the tools themselves become targets for haters and hackers? While experts largely agree that the approach is effective and Nightshade could prove to be powerful poison, other researchers claim they\u2019ve already poked holes in the protections offered by Glaze and that trusting these tools is risky.",
    "But Neil Turkewitz, a copyright lawyer who used to work at the Recording Industry Association of America, offers a more sweeping view of the fight the SAND Lab has joined. It\u2019s not about a single AI company or a single individual, he says: \u201cIt\u2019s about defining the rules of the world we want to inhabit.\u201d\u00a0 Poking the bear The SAND Lab is tight knit, encompassing a dozen or so researchers crammed into a corner of the University of Chicago\u2019s computer science building. That space has accumulated somewhat typical workplace detritus\u2014a Meta Quest headset here, silly photos of dress-up from Halloween parties there. But the walls are also covered in original art pieces, including a framed painting by Ortiz.\u00a0\u00a0 Years before fighting alongside artists like Ortiz against \u201cAI bros\u201d (to use Zhao\u2019s words), Zhao and the lab\u2019s co-leader, Heather Zheng, who is also his wife, had built a record of combating harms posed by new tech.\u00a0  When I visited the SAND Lab in Chicago, I saw how tight knit the group was. Alongside the typical workplace stuff were funny Halloween photos like this one. (Front row: Ronik Bhaskar, Josephine Passananti, Anna YJ Ha, Zhuolin Yang, Ben Zhao, Heather Zheng. Back row: Cathy Yuanchen Li, Wenxin Ding, Stanley Wu, and Shawn Shan.)COURTESY OF SAND LAB   Though both earned spots on MIT Technology Review\u2019s 35 Innovators Under 35 list for other work nearly two decades ago, when they were at the University of California, Santa Barbara (Zheng in 2005 for \u201ccognitive radios\u201d and Zhao a year later for peer-to-peer networks), their primary research focus has become security and privacy.\u00a0  The pair left Santa Barbara in 2017, after they were poached by the new co-director of the University of Chicago\u2019s Data Science Institute, Michael Franklin. All eight PhD students from their UC Santa Barbara lab decided to follow them to Chicago too. Since then, the group has developed a \u201cbracelet of silence\u201d that jams the microphones in AI voice assistants like the Amazon Echo. It has also created a tool called Fawkes\u2014\u201cprivacy armor,\u201d as Zhao put it in a 2020 interview with the New York Times\u2014that people can apply to their photos to protect them from facial recognition software. They\u2019ve also studied how hackers might steal sensitive information through stealth attacks on virtual-reality headsets, and how to distinguish human art from AI-generated images.\u00a0 \u201cBen and Heather and their group are kind of unique because they\u2019re actually trying to build technology that hits right at some key questions about AI and how it is used,\u201d Franklin tells me. \u201cThey\u2019re doing it not just by asking those questions, but by actually building technology that forces those questions to the forefront.\u201d It was Fawkes that intrigued Van Deun, the fantasy illustrator, two years ago; she hoped something similar might work as protection against generative AI, which is why she extended that fateful invite to the Concept Art Association\u2019s Zoom call.\u00a0 That call started something of a mad rush in the weeks that followed. Though Zhao and Zheng collaborate on all the lab\u2019s projects, they each lead individual initiatives; Zhao took on what would become Glaze, with PhD student Shawn Shan (who was on this year\u2019s Innovators Under 35 list) spearheading the development of the program\u2019s algorithm.",
    "In parallel to Shan\u2019s coding, PhD students Jenna Cryan and Emily Wenger sought to learn more about the views and needs of the artists themselves. They created a user survey that the team distributed to artists with the help of Ortiz. In replies from more than 1,200 artists\u2014far more than the average number of responses to user studies in computer science\u2014the team found that the vast majority of creators had read about art being used to train models, and 97% expected AI to decrease some artists\u2019 job security. A quarter said AI art had already affected their jobs.\u00a0 Almost all artists also said they posted their work online, and more than half said they anticipated reducing or removing that online work, if they hadn\u2019t already\u2014no matter the professional and financial consequences.",
    "The first scrappy version of Glaze was developed in just a month, at which point Ortiz gave the team her entire catalogue of work to test the model on. At the most basic level, Glaze acts as a defensive shield. Its algorithm identifies features from the image that make up an artist's individual style and adds subtle changes to them. When an AI model is trained on images protected with Glaze, the model will not be able to reproduce styles similar to the original image.\u00a0 A painting from Ortiz later became the first image publicly released with Glaze on it: a young woman, surrounded by flying eagles, holding up a wreath. Its title is Musa Victoriosa, \u201cvictorious muse.\u201d\u00a0  It\u2019s the one currently hanging on the SAND Lab\u2019s walls.\u00a0  Despite many artists\u2019 initial enthusiasm, Zhao says, Glaze\u2019s launch caused significant backlash. Some artists were skeptical because they were worried this was a scam or yet another data-harvesting campaign.\u00a0 The lab had to take several steps to build trust, such as offering the option to download the Glaze app so that it adds the protective layer offline, which meant no data was being transferred anywhere. (The images are then shielded when artists upload them.)\u00a0\u00a0 Soon after Glaze\u2019s launch, Shan also led the development of the second tool, Nightshade. Where Glaze is a defensive mechanism, Nightshade was designed to act as an offensive deterrent to nonconsensual training. It works by changing the pixels of images in ways that are not noticeable to the human eye but manipulate machine-learning models so they interpret the image as something different from what it actually shows. If poisoned samples are scraped into AI training sets, these samples trick the AI models: Dogs become cats, handbags become toasters. The researchers say only a relatively few examples are enough to permanently damage the way a generative AI model produces images.",
    "Currently, both tools are available as free apps or can be applied through the project\u2019s website. The lab has also recently expanded its reach by offering integration with the new artist-supported social network Cara, which was born out of a backlash to exploitative AI training and forbids AI-produced content. In dozens of conversations with Zhao and the lab\u2019s researchers, as well as a handful of their artist-collaborators, it\u2019s become clear that both groups now feel they are aligned in one mission. \u201cI never expected to become friends with scientists in Chicago,\u201d says Eva Toorenent, a Dutch artist who worked closely with the team on Nightshade. \u201cI\u2019m just so happy to have met these people during this collective battle.\u201d\u00a0  Images online of Toorenent's Belladonna have been treated with the SAND Lab's Nightshade tool.EVA TOORENENT   Her painting Belladonna, which is also another name for the nightshade plant, was the first image with Nightshade\u2019s poison on it.\u00a0 \u201cIt\u2019s so symbolic,\u201d she says. \u201cPeople taking our work without our consent, and then taking our work without consent can ruin their models. It\u2019s just poetic justice.\u201d",
    "No perfect solution The reception of the SAND Lab\u2019s work has been less harmonious across the AI community. After Glaze was made available to the public, Zhao tells me, someone reported it to sites like VirusTotal, which tracks malware, so that it was flagged by antivirus programs. Several people also started claiming on social media that the tool had quickly been broken. Nightshade similarly got a fair share of criticism when it launched; as TechCrunch reported in January, some called it a \u201cvirus\u201d and, as the story explains, \u201canother Reddit user who inadvertently went viral on X questioned Nightshade\u2019s legality, comparing it to \u2018hacking a vulnerable computer system to disrupt its operation.\u2019\u201d\u00a0 \u201cWe had no idea what we were up against,\u201d Zhao tells me. \u201cNot knowing who or what the other side could be meant that every single new buzzing of the phone meant that maybe someone did break Glaze.\u201d\u00a0 Both tools, though, have gone through rigorous academic peer review and have won recognition from the computer security community. Nightshade was accepted at the IEEE Symposium on Security and Privacy, and Glaze received a distinguished paper award and the 2023 Internet Defense Prize at the Usenix Security Symposium, a top conference in the field.\u00a0 \u201cIn my experience working with poison, I think [Nightshade is] pretty effective,\u201d says Nathalie Baracaldo, who leads the AI security and privacy solutions team at IBM and has studied data poisoning. \u201cI have not seen anything yet\u2014and the word yet is important here\u2014that breaks that type of defense that Ben is proposing.\u201d And the fact that the team has released the source code for Nightshade for others to probe, and it hasn\u2019t been broken, also suggests it\u2019s quite secure, she adds.\u00a0 At the same time, at least one team of researchers does claim to have penetrated the protections of Glaze, or at least an old version of it.\u00a0 As researchers from Google DeepMind and ETH Zurich detailed in a paper published in June, they found various ways Glaze (as well as similar but less popular protection tools, such as Mist and Anti-DreamBooth) could be circumvented using off-the-shelf techniques that anyone could access\u2014such as image upscaling, meaning filling in pixels to increase the resolution of an image as it\u2019s enlarged. The researchers write that their work shows the \u201cbrittleness of existing protections\u201d and warn that \u201cartists may believe they are effective. But our experiments show they are not.\u201d Florian Tram\u00e8r, an associate professor at ETH Zurich who was part of the study, acknowledges that it is \u201cvery hard to come up with a strong technical solution that ends up really making a difference here.\u201d Rather than any individual tool, he ultimately advocates for an almost certainly unrealistic ideal: stronger policies and laws to help create an environment in which people commit to buying only human-created art.\u00a0  What happened here is common in security research, notes Baracaldo: A defense is proposed, an adversary breaks it, and\u2014ideally\u2014the defender learns from the adversary and makes the defense better. \u201cIt\u2019s important to have both ethical attackers and defenders working together to make our AI systems safer,\u201d she says, adding that \u201cideally, all defenses should be publicly available for scrutiny,\u201d which would both \u201callow for transparency\u201d and help avoid creating a false sense of security. (Zhao, though, tells me the researchers have no intention to release Glaze\u2019s source code.) Still, even as all these researchers claim to support artists and their art, such tests hit a nerve for Zhao. In Discord chats that were later leaked, he claimed that one of the researchers from the ETH Zurich\u2013Google DeepMind team \u201cdoesn\u2019t give a shit\u201d about people. (That researcher did not respond to a request for comment, but in a blog post he said it was important to break defenses in order to know how to fix them. Zhao says his words were taken out of context.)\u00a0 Related StoryThis new data poisoning tool lets artists fight back against generative AIThe tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.",
    "Zhao also emphasizes to me that the paper\u2019s authors mainly evaluated an earlier version of Glaze; he says its new update is more resistant to tampering. Messing with images that have current Glaze protections would harm the very style that is being copied, he says, making such an attack useless.\u00a0 This back-and-forth reflects a significant tension in the computer security community and, more broadly, the often adversarial relationship between different groups in AI. Is it wrong to give people the feeling of security when the protections you\u2019ve offered might break? Or is it better to have some level of protection\u2014one that raises the threshold for an attacker to inflict harm\u2014than nothing at all?\u00a0 Yves-Alexandre de Montjoye, an associate professor of applied mathematics and computer science at Imperial College London, says there are plenty of examples where similar technical protections have failed to be bulletproof. For example, in 2023, de Montjoye and his team probed a digital mask for facial recognition algorithms, which was meant to protect the privacy of medical patients\u2019 facial images; they were able to break the protections by tweaking just one thing in the program\u2019s algorithm (which was open source).\u00a0 Using such defenses is still sending a message, he says, and adding some friction to data profiling. \u201cTools such as TrackMeNot\u201d\u2014which protects users from data profiling\u2014\u201chave been presented as a way to protest; as a way to say I do not consent.\u201d\u00a0\u00a0 \u201cBut at the same time,\u201d he argues, \u201cwe need to be very clear with artists that it is removable and might not protect against future algorithms.\u201d While Zhao will admit that the researchers pointed out some of Glaze\u2019s weak spots, he unsurprisingly remains confident that Glaze and Nightshade are worth deploying, given that \u201csecurity tools are never perfect.\u201d Indeed, as Baracaldo points out, the Google DeepMind and ETH Zurich researchers showed how a highly motivated and sophisticated adversary will almost certainly always find a way in. Yet it is \u201csimplistic to think that if you have a real security problem in the wild and you\u2019re trying to design a protection tool, the answer should be it either works perfectly or don\u2019t deploy it,\u201d Zhao says, citing spam filters and firewalls as examples. Defense is a constant cat-and-mouse game. And he believes most artists are savvy enough to understand the risk.\u00a0 Offering hope The fight between creators and AI companies is fierce. The current paradigm in AI is to build bigger and bigger models, and there is, at least currently, no getting around the fact that they require vast data sets hoovered from the internet to train on. Tech companies argue that anything on the public internet is fair game, and that it is \u201cimpossible\u201d to build advanced AI tools without copyrighted material; many artists argue that tech companies have stolen their intellectual property and violated copyright law, and that they need ways to keep their individual works out of the models\u2014or at least receive proper credit and compensation for their use.\u00a0 So far, the creatives aren\u2019t exactly winning. A number of companies have already replaced designers, copywriters, and illustrators with AI systems. In one high-profile case, Marvel Studios used AI-generated imagery instead of human-created art in the title sequence of its 2023 TV series Secret Invasion. In another, a radio station fired its human presenters and replaced them with AI. The technology has become a major bone of contention between unions and film, TV, and creative studios, most recently leading to a strike by video-game performers. There are numerous ongoing lawsuits by artists, writers, publishers, and record labels against AI companies. It will likely take years until there is a clear-cut legal resolution. But even a court ruling won\u2019t necessarily untangle the difficult ethical questions created by generative AI. Any future government regulation is not likely to either, if it ever materializes.\u00a0 That\u2019s why Zhao and Zheng see Glaze and Nightshade as necessary interventions\u2014tools to defend original work, attack those who would help themselves to it, and, at the very least, buy artists some time. Having a perfect solution is not really the point. The researchers need to offer something now because the AI sector moves at breakneck speed, Zheng says, means that companies are ignoring very real harms to humans. \u201cThis is probably the first time in our entire technology careers that we actually see this much conflict,\u201d she adds. On a much grander scale, she and Zhao tell me they hope that Glaze and Nightshade will eventually have the power to overhaul how AI companies use art and how their products produce it. It is eye-wateringly expensive to train AI models, and it\u2019s extremely laborious for engineers to find and purge poisoned samples in a data set of billions of images. Theoretically, if there are enough Nightshaded images on the internet and tech companies see their models breaking as a result, it could push developers to the negotiating table to bargain over licensing and fair compensation.\u00a0 That\u2019s, of course, still a big \u201cif.\u201d MIT Technology Review reached out to several AI companies, such as Midjourney and Stability AI, which did not reply to requests for comment. A spokesperson for OpenAI, meanwhile, did not confirm any details about encountering data poison but said the company takes the safety of its products seriously and is continually improving its safety measures: \u201cWe are always working on how we can make our systems more robust against this type of abuse.\u201d In the meantime, the SAND Lab is moving ahead and looking into funding from foundations and nonprofits to keep the project going. They also say there has also been interest from major companies looking to protect their intellectual property (though they decline to say which), and Zhao and Zheng are exploring how the tools could be applied in other industries, such as gaming, videos, or music. In the meantime, they plan to keep updating Glaze and Nightshade to be as robust as possible, working closely with the students in the Chicago lab\u2014where, on another wall, hangs Toorenent\u2019s Belladonna. The painting has a heart-shaped note stuck to the bottom right corner: \u201cThank you! You have given hope to us artists.\u201d This story has been updated with the latest download figures for Glaze and Nightshade.  hide"
  ]
}