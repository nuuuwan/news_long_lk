{
  "url": "https://www.technologyreview.com/2026/02/12/1132811/whats-next-for-chinese-open-source-ai/",
  "title": "What\u2019s next for Chinese open-source AI",
  "ut": 1770852600.0,
  "body_paragraphs": [
    "MIT Technology Review\u2019s What\u2019s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them\u00a0here. The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI models that match the performance of leading Western models at a fraction of the cost.\u00a0  Just last week the Chinese firm Moonshot AI released its latest open-weight model, Kimi K2.5, which came close to top proprietary systems such as Anthropic\u2019s Claude Opus on some early benchmarks.\u00a0The difference: K2.5 is roughly one-seventh Opus\u2019s price. On Hugging Face, Alibaba\u2019s Qwen family\u2014after ranking as the most downloaded model series in 2025 and 2026\u2014has overtaken Meta\u2019s Llama models in cumulative downloads. And a recent MIT study found that Chinese open-source models have surpassed US models in total downloads. For developers and builders worldwide, access to near-frontier AI capabilities has never been this broad or this affordable.",
    "But these models differ in a crucial way from most US models like ChatGPT or Claude, which you pay to access and can\u2019t inspect. The Chinese companies publish their models\u2019 weights\u2014numerical values that get set when a model is trained\u2014so anyone can download, run, study, and modify them.\u00a0 If these open-source AI models keep getting better, they will not just offer the cheapest options for people who want access to frontier AI capabilities; they will change where innovation happens and who sets the standards.",
    "Here\u2019s what may come next. China\u2019s commitment to open source will continue When DeepSeek launched R1, much of the initial shock centered on its origin. Suddenly, a Chinese team had released a reasoning model that could stand alongside the best systems from US labs. But the long tail of DeepSeek\u2019s impact had less to do with nationality than with distribution. R1 was released as an open-weight model under a permissive MIT license, allowing anyone to download, inspect, and deploy it. On top of that, DeepSeek also published a paper detailing its training process and techniques. For developers who access models via an API, DeepSeek also undercut competitors on price, offering access at a fraction the cost of OpenAI\u2019s o1, the leading proprietary reasoning model at the time. Within days of its release, DeepSeek replaced ChatGPT as the most downloaded free app in the US App Store. The moment spilled beyond developer circles into financial markets, triggering a sharp sell-off in US tech stocks that briefly erased roughly $1 trillion in market value. Almost overnight, DeepSeek went from a little-known spin-off team backed by a quantitative hedge fund to the most visible symbol of China\u2019s push for open-source AI.  China\u2019s decision to lean in to open source isn\u2019t surprising. It has the world\u2019s second-largest concentration of AI talent after the US. plus a vast, well-resourced tech industry. After ChatGPT broke into the mainstream, China\u2019s AI sector went through a reckoning\u2014and emerged determined to catch up. Pursuing an open-source strategy was seen as the fastest way to close the gap by rallying developers, spreading adoption, and setting standards. Related StoryHow DeepSeek ripped up the AI playbook\u2014and why everyone's going to follow its leadRead next DeepSeek\u2019s success injected confidence into an industry long used to following global standards rather than setting them. \u201cThirty years ago, no Chinese person would believe they could be at the center of global innovation,\u201d says Alex Chenglin Wu, CEO and founder of Atoms, an AI agent company and prominent contributor to China\u2019s open-source ecosystem. \u201cDeepSeek shows that with solid technical talent, a supportive environment, and the right organizational culture, it\u2019s possible to do truly world-class work.\u201d DeepSeek\u2019s breakout moment wasn\u2019t China\u2019s first open-source success. Alibaba\u2019s Qwen Lab had been releasing open-weight models for years. By September 2024,\u00a0 well before DeepSeek\u2019s V3 launch, Alibaba was saying that global downloads had exceeded 600 million. On Hugging Face, Qwen accounted for more than 30% of all model downloads in 2024. Other institutions, including the Beijing Academy of Artificial Intelligence and the AI firm Baichuan, were also releasing open models as early as 2023.\u00a0 But since the success of DeepSeek, the field has widened rapidly. Companies such as Z.ai (formerly Zhipu), MiniMax, Tencent, and a growing number of smaller labs have released models that are competitive on reasoning, coding, and agent-style tasks. The growing number of capable models has sped up progress. Capabilities that once took months to make it to the open-source world now emerge within weeks, even days. \u201cChinese AI firms have seen real gains from the open-source playbook,\u201d says Liu Zhiyuan, a professor of computer science at Tsinghua University and chief scientist at the AI startup ModelBest. \u201cBy releasing strong research, they build reputation and gain free publicity.\u201d",
    "Beyond commercial incentives, Liu says, open source has taken on cultural and strategic weight. \u201cIn the Chinese programmer community, open source has become politically correct,\u201d he says, framing it as a response to US dominance in proprietary AI systems. That shift is also reflected at the institutional level. Universities including Tsinghua have begun encouraging AI development and open-source contributions, while policymakers have moved to formalize those incentives. In August, China\u2019s State Council released a draft policy encouraging universities to reward open-source work, proposing that students\u2019 contributions on platforms such as GitHub or Gitee could eventually be counted toward academic credit. With growing momentum and a reinforcing feedback loop, China\u2019s push for open-source models is likely to continue in the near term, though its long-term sustainability still hinges on financial results, says Tiezhen Wang, who helps lead work on global AI at Hugging Face. In January, the model labs Z.ai and MiniMax went public in Hong Kong. \u201cRight now, the focus is on making the cake bigger,\u201d says Wang. \u201cThe next challenge is figuring out how each company secures its share.\u201d The next wave of models will be narrower\u2014and better Chinese open-source models are leading not just in download volume but also in variety. Alibaba\u2019s Qwen has become one of the most diversified open model families in circulation, offering a wide range of variants optimized for different uses. The lineup ranges from lightweight models that can run on a single laptop to large, multi-hundred-billion-parameter systems designed for data-center deployment. Qwen features many task-optimized variants created by the community: the \u201cinstruct\u201d models are good at following orders, and \u201ccode\u201d variants specialize in coding.  Although this strategy isn\u2019t unique to Chinese labs, Qwen was the first open model family to roll out so many high-quality options that it started to feel like a full product line\u2014one that\u2019s free to use. The open-weight nature of these releases also makes it easy for others to adapt them through techniques like fine-tuning and distillation, which means training a smaller model to mimic a larger one.\u00a0 According to ATOM (American Truly Open Models), a project by the AI researcher Nathan Lambert, by August 4, 2025, model variations derived from Qwen were \u201cmore than 40%\u201d of new Hugging Face language-model derivatives, while Llama had fallen to about 15%. This means that Qwen has become the default base model for all the \u201cremixes.\u201d This pattern has made the case for smaller, more specialized models. \u201cCompute and energy are real constraints for any deployment,\u201d Liu says. He told MIT Technology Review that the rise of small models is about making AI cheaper to run and easier for more people to use. His company, ModelBest, focuses on small language models designed to run locally on devices such as phones, cars, and other consumer hardware. While an average user might interact with AI only through the web or an app for simple conversations, power users of AI models with some technical background are experimenting with giving AI more autonomy to solve large-scale problems. OpenClaw, an open-source AI agent that recently went viral within the AI hacker world, allows AI to take over your computer\u2014it can run 24-7, going through your emails and work tasks without supervision.",
    "Related StoryMoltbook was peak AI theaterRead next OpenClaw, like many other open-source tools, allows users to connect to different AI models via an application programming interface, or API. Within days of OpenClaw\u2019s release, the team revealed that Kimi\u2019s K2.5 had surpassed Claude Opus and became the most used AI model\u2014by token count, meaning it was handling more total text processed across user prompts and model responses. Cost has been a major reason Chinese models have gained traction, but it would be a mistake to treat them as mere \u201cdupes\u201d of Western frontier systems, Wang suggests. Like any product, a model only needs to be good enough for the job at hand.",
    "The landscape of open-source models in China is also getting more specialized. Research groups such as Shanghai AI Laboratory have released models geared toward scientific and technical tasks; several projects from Tencent have focused specifically on music generation. Ubiquant, a quantitative finance firm like DeepSeek\u2019s parent High-Flyer, has released an open model aimed at medical reasoning. In the meantime, innovative architectural ideas from Chinese labs are being picked up more broadly. DeepSeek has published work exploring model efficiency and memory; techniques that compress the model\u2019s attention \u201ccache,\u201d reducing memory and inference costs while mostly preserving performance, have drawn significant attention in the research community.\u00a0 \u201cThe impact of these research breakthroughs is amplified because they\u2019re open-sourced and can be picked up quickly across the field,\u201d says Wang. Chinese open models will become infrastructure for global AI builders The adoption of Chinese models is picking up in Silicon Valley, too. Martin Casado, a general partner at Andreessen Horowitz, has put a number on it: Among startups pitching with open-source stacks, there\u2019s about an 80% chance they\u2019re running on Chinese open models, according to a post he made on X. Usage data tells a similar story. OpenRouter,\u00a0 a middleman that tracks how people use different AI models through its API, shows Chinese open models rising from almost none in late 2024 to nearly 30% of usage in some recent weeks. The demand is also rising globally. Z.ai limited new subscriptions to its GLM coding plan (a coding tool based on its flagship GLM models) after demand surged, citing compute constraints. What\u2019s notable is where the demand is coming from: CNBC reports that the system\u2019s user base is primarily concentrated in the United States and China, followed by India, Japan, Brazil, and the UK. \u201cThe open-source ecosystems in China and the US are tightly bound together,\u201d says Wang at Hugging Face. Many Chinese open models still rely on Nvidia and US cloud platforms to train and serve them, which keeps the business ties tangled. Talent is fluid too: Researchers move across borders and companies, and many still operate as a global community, sharing code and ideas in public. That interdependence is part of what makes Chinese developers feel optimistic about this moment: The work travels, gets remixed, and actually shows up in products. But openness can also accelerate the competition. Dario Amodei, the CEO of Anthropic, made a version of this point after DeepSeek\u2019s 2025 releases: He wrote that export controls are \u201cnot a way to duck the competition\u201d between the US and China, and that AI companies in the US \u201cmust have better models\u201d if they want to prevail.\u00a0 For the past decade, the story of Chinese tech in the West has been one of big expectations that ran into scrutiny, restrictions, and political backlash. This time the export isn\u2019t just an app or a consumer platform. It\u2019s the underlying model layer that other people build on. Whether that will play out differently is still an open question. hide"
  ]
}