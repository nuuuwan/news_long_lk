{
  "url": "https://www.technologyreview.com/2025/09/08/1123214/opinion-basic-science-research-funding/",
  "title": "Why basic science deserves our boldest investment",
  "ut": 1757294100.0,
  "body_paragraphs": [
    "In December 1947, three physicists at Bell Telephone Laboratories\u2014John Bardeen, William Shockley, and Walter Brattain\u2014built a compact electronic device using thin gold wires and a piece of germanium, a material known as a semiconductor. Their invention, later named the transistor (for which they were awarded the Nobel Prize in 1956), could amplify and switch electrical signals, marking a dramatic departure from the bulky and fragile vacuum tubes that had powered electronics until then. Its inventors weren\u2019t chasing a specific product. They were asking fundamental questions about how electrons behave in semiconductors, experimenting with surface states and electron mobility in germanium crystals. Over months of trial and refinement, they combined theoretical insights from quantum mechanics with hands-on experimentation in solid-state physics\u2014work many might have dismissed as too basic, academic, or unprofitable.  Their efforts culminated in a moment that now marks the dawn of the information age. Transistors don\u2019t usually get the credit they deserve, yet they are the bedrock of every smartphone, computer, satellite, MRI scanner, GPS system, and artificial-intelligence platform we use today. With their ability to modulate (and route) electrical current at astonishing speeds, transistors make modern and future computing and electronics possible. This breakthrough did not emerge from a business plan or product pitch. It arose from open-ended, curiosity-driven research and enabling development, supported by an institution that saw value in exploring the unknown. It took years of trial and error, collaborations across disciplines, and a deep belief that understanding nature\u2014even without a guaranteed payoff\u2014was worth the effort.",
    "After the first successful demonstration in late 1947, the invention of the transistor remained confidential while Bell Labs filed patent applications and continued development. It was publicly announced at a press conference on June 30, 1948, in New York City. The scientific explanation followed in a seminal paper published in the journal Physical Review.\u00a0 How do they work? At their core, transistors are made of semiconductors\u2014materials like germanium and, later, silicon\u2014that can either conduct or resist electricity depending on subtle manipulations of their structure and charge. In a typical transistor, a small voltage applied to one part of the device (the gate) either allows or blocks the electric current flowing through another part (the channel). It\u2019s this simple control mechanism, scaled up billions of times, that lets your phone run apps, your laptop render images, and your search engine return answers in milliseconds.",
    "Though early devices used germanium, researchers soon discovered that silicon\u2014more thermally stable, moisture resistant, and far more abundant\u2014was better suited for industrial production. By the late 1950s, the transition to silicon was underway, making possible the development of integrated circuits and, eventually, the microprocessors that power today\u2019s digital world. A modern chip the size of a human fingernail now contains tens of billions of silicon transistors, each measured in nanometers\u2014smaller than many viruses. These tiny switches turn on and off billions of times per second, controlling the flow of electrical signals involved in computation, data storage, audio and visual processing, and artificial intelligence. They form the fundamental infrastructure behind nearly every digital device in use today.\u00a0 Related StorySynthesia\u2019s AI clones are more expressive than ever. Soon they\u2019ll be able to talk back.The uncanny valley is narrowing. Are we ready for what comes next?",
    "The global semiconductor industry is now worth over half a trillion dollars. Devices that began as experimental prototypes in a physics lab now underpin economies, national security, health care, education, and global communication. But the transistor\u2019s origin story carries a deeper lesson\u2014one we risk forgetting. Much of the fundamental understanding that moved transistor technology forward came from federally funded university research. Nearly a quarter of transistor research at Bell Labs in the 1950s was supported by the federal government. Much of the rest was subsidized by revenue from AT&T\u2019s monopoly on the US phone system, which flowed into industrial R&D.  Inspired by the 1945 report \u201cScience: The Endless Frontier,\u201d authored by Vannevar Bush at the request of President Truman, the US government began a long-standing tradition of investing in basic research. These investments have paid steady dividends across many scientific domains\u2014from nuclear energy to lasers, and from medical technologies to artificial intelligence. Trained in fundamental research, generations of students have emerged from university labs with the knowledge and skills necessary to push existing technology beyond its known capabilities. And yet, funding for basic science\u2014and for the education of those who can pursue it\u2014is under increasing pressure. The new White House\u2019s proposed federal budget includes deep cuts to the Department of Energy and the National Science Foundation (though Congress may deviate from those recommendations). Already, the National Institutes of Health has canceled or paused more than $1.9\u202fbillion in grants, while NSF STEM education programs suffered more than $700\u202fmillion in terminations. These losses have forced some universities to freeze graduate student admissions, cancel internships, and scale back summer research opportunities\u2014making it harder for young people to pursue scientific and engineering careers. In an age dominated by short-term metrics and rapid returns, it can be difficult to justify research whose applications may not materialize for decades. But those are precisely the kinds of efforts we must support if we want to secure our technological future. Consider John McCarthy, the mathematician and computer scientist who coined the term \u201cartificial intelligence.\u201d In the late 1950s, while at MIT, he led one of the first AI groups and developed Lisp, a programming language still used today in scientific computing and AI applications. At the time, practical AI seemed far off. But that early foundational work laid the groundwork for today\u2019s AI-driven world.",
    "After the initial enthusiasm of the 1950s through the \u201970s, interest in neural networks\u2014a leading AI architecture today inspired by the human brain\u2014declined during the so-called \u201cAI winters\u201d of the late 1990s and early 2000s. Limited data, inadequate computational power, and theoretical gaps made it hard for the field to progress. Still, researchers like Geoffrey Hinton and John Hopfield pressed on. Hopfield, now a 2024 Nobel laureate in physics, first introduced his groundbreaking neural network model in 1982, in a paper published in Proceedings of the National Academy of Sciences of the USA. His work revealed the deep connections between collective computation and the behavior of disordered magnetic systems. Together with the work of colleagues including Hinton, who was awarded the Nobel the same year, this foundational research seeded the explosion of deep-learning technologies we see today. One reason neural networks now flourish is the graphics processing unit, or GPU\u2014originally designed for gaming but now essential for the matrix-heavy operations of AI. These chips themselves rely on decades of fundamental research in materials science and solid-state physics: high-dielectric materials, strained silicon alloys, and other advances making it possible to produce the most efficient transistors possible. We are now entering another frontier, exploring memristors, phase-changing and 2D materials, and spintronic devices. If you're reading this on a phone or laptop, you\u2019re holding the result of a gamble someone once made on curiosity. That same curiosity is still alive in university and research labs today\u2014in often unglamorous, sometimes obscure work quietly laying the groundwork for revolutions that will infiltrate some of the most essential aspects of our lives 50 years from now. At the leading physics journal where I am editor, my collaborators and I see the painstaking work and dedication behind every paper we handle. Our modern economy\u2014with giants like Nvidia, Microsoft, Apple, Amazon, and Alphabet\u2014would be unimaginable without the humble transistor and the passion for knowledge fueling the relentless curiosity of scientists like those who made it possible. The next transistor may not look like a switch at all. It might emerge from new kinds of materials (such as quantum, hybrid organic-inorganic, or hierarchical types) or from tools we haven\u2019t yet imagined. But it will need the same ingredients: solid fundamental knowledge, resources, and freedom to pursue open questions driven by curiosity, collaboration\u2014and most importantly, financial support from someone who believes it's worth the risk. Julia R. Greer is a materials scientist at the California Institute of Technology. She is a judge for MIT Technology Review\u2019s Innovators Under 35 and a former honoree (in 2008). 2025Innovators Under 35Meet the InnovatorsInnovator of the Year: Sneha GoenkaFeatured Innovator: Yichao \"Peak\"JiFeatured Innovator: Iwnetim AbateIn their own words: How Trump's policies affect early-career scientistsOpinion: Why basic science deserves our boldest investmentHere\u2019s how we picked this year\u2019s Innovators Under 35Return to landing pagehide"
  ]
}