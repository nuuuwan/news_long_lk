{
  "url": "https://www.technologyreview.com/2026/01/22/1131692/dr-google-had-its-issues-can-chatgpt-health-do-better/",
  "title": "\u201cDr. Google\u201d had its issues. Can ChatGPT Health do better?",
  "ut": 1769065689.0,
  "body_paragraphs": [
    "EXECUTIVE SUMMARY For the past two decades, there\u2019s been a clear first step for anyone who starts experiencing new medical symptoms: Look them up online. The practice was so common that it gained the pejorative moniker \u201cDr. Google.\u201d But times are changing, and many medical-information seekers are now using LLMs. According to OpenAI, 230 million people ask ChatGPT health-related queries each week.\u00a0 That\u2019s the context around the launch of OpenAI\u2019s new ChatGPT Health product, which debuted earlier this month. It landed at an inauspicious time: Two days earlier, the news website SFGate had broken the story of Sam Nelson, a teenager who died of an overdose last year after extensive conversations with ChatGPT about how best to combine various drugs. In the wake of both pieces of news, multiple journalists questioned the wisdom of relying for medical advice on a tool that could cause such extreme harm.  Though ChatGPT Health lives in a separate sidebar tab from the rest of ChatGPT, it isn\u2019t a new model. It\u2019s more like a wrapper that provides one of OpenAI\u2019s preexisting models with guidance and tools it can use to provide health advice\u2014including some that allow it to access a user\u2019s electronic medical records and fitness app data, if granted permission. There\u2019s no doubt that ChatGPT and other large language models can make medical mistakes, and OpenAI emphasizes that ChatGPT Health is intended as an additional support, rather than a replacement for one\u2019s doctor. But when doctors are unavailable or unable to help, people will turn to alternatives.\u00a0 Related StoryAI companies have stopped warning you that their chatbots aren\u2019t doctorsRead next Some doctors see LLMs as a boon for medical literacy. The average patient might struggle to navigate the vast landscape of online medical information\u2014and, in particular, to distinguish high-quality sources from polished but factually dubious websites\u2014but LLMs can do that job for them, at least in theory. Treating patients who had searched for their symptoms on Google required \u201ca lot of attacking patient anxiety [and] reducing misinformation,\u201d says Marc Succi, an associate professor at Harvard Medical School and a practicing radiologist. But now, he says, \u201cyou see patients with a college education, a high school education, asking questions at the level of something an early med student might ask.\u201d",
    "The release of ChatGPT Health, and Anthropic\u2019s subsequent announcement of new health integrations for Claude, indicate that the AI giants are increasingly willing to acknowledge and encourage health-related uses of their models. Such uses certainly come with risks, given LLMs\u2019 well-documented tendencies to agree with users and make up information rather than admit ignorance.\u00a0  But those risks also have to be weighed against potential benefits. There\u2019s an analogy here to autonomous vehicles: When policymakers consider whether to allow Waymo in their city, the key metric is not whether its cars are ever involved in accidents but whether they cause less harm than the status quo of relying on human drivers. If Dr. ChatGPT is an improvement over Dr. Google\u2014and early evidence suggests it may be\u2014it could potentially lessen the enormous burden of medical misinformation and unnecessary health anxiety that the internet has created.  Pinning down the effectiveness of a chatbot such as ChatGPT or Claude for consumer health, however, is tricky. \u201cIt\u2019s exceedingly difficult to evaluate an open-ended chatbot,\u201d says Danielle Bitterman, the clinical lead for data science and AI at the Mass General Brigham health-care system. Large language models score well on medical licensing examinations, but those exams use multiple-choice questions that don\u2019t reflect how people use chatbots to look up medical information.",
    "Sirisha Rambhatla, an assistant professor of management science and engineering at the University of Waterloo, attempted to close that gap by evaluating how GPT-4 responded to licensing exam questions when it did not have access to a list of possible answers. Medical experts who evaluated the responses scored only about half of them as entirely correct. But multiple-choice exam questions are designed to be tricky enough that the answer options don\u2019t give them entirely away, and they\u2019re still a pretty distant approximation for the sort of thing that a user would type into ChatGPT. A different study, which tested GPT-4o on more realistic prompts submitted by human volunteers, found that it answered medical questions correctly about 85% of the time. When I spoke with Amulya Yadav, an associate professor at Pennsylvania State University who runs the Responsible AI for Social Emancipation Lab and led the study, he made it clear that he wasn\u2019t personally a fan of patient-facing medical LLMs. But he freely admits that, technically speaking, they seem up to the task\u2014after all, he says, human doctors misdiagnose patients 10% to 15% of the time. \u201cIf I look at it dispassionately, it seems that the world is gonna change, whether I like it or not,\u201d he says.  For people seeking medical information online, Yadav says, LLMs do seem to be a better choice than Google. Succi, the radiologist, also concluded that LLMs can be a better alternative to web search when he compared GPT-4\u2019s responses to questions about common chronic medical conditions with the information presented in Google\u2019s knowledge panel, the information box that sometimes appears on the right side of the search results.  Since Yadav\u2019s and Succi\u2019s studies appeared online, in the first half of 2025, OpenAI has released multiple new versions of GPT, and it\u2019s reasonable to expect that GPT-5.2 would perform even better than its predecessors. But the studies do have important limitations: They focus on straightforward, factual questions, and they examine only brief interactions between users and chatbots or web search tools. Some of the weaknesses of LLMs\u2014most notably their sycophancy and tendency to hallucinate\u2014might be more likely to rear their heads in more extensive conversations and with people who are dealing with more complex problems. Reeva Lederman, a professor at the University of Melbourne who studies technology and health, notes that patients who don\u2019t like the diagnosis or treatment recommendations that they receive from a doctor might seek out another opinion from an LLM\u2014and the LLM, if it\u2019s sycophantic, might encourage them to reject their doctor\u2019s advice.  Some studies have found that LLMs will hallucinate and exhibit sycophancy in response to health-related prompts. For example, one study showed that GPT-4 and GPT-4o will happily accept and run with incorrect drug information included in a user\u2019s question. In another, GPT-4o frequently concocted definitions for fake syndromes and lab tests mentioned in the user\u2019s prompt. Given the abundance of medically dubious diagnoses and treatments floating around the internet, these patterns of LLM behavior could contribute to the spread of medical misinformation, particularly if people see LLMs as trustworthy.  OpenAI has reported that the GPT-5 series of models is markedly less sycophantic and prone to hallucination than their predecessors, so the results of these studies might not apply to ChatGPT Health. The company also evaluated the model that powers ChatGPT Health on its responses to health-specific questions, using their publicly available HeathBench benchmark. HealthBench rewards models that express uncertainty when appropriate, recommend that users seek medical attention when necessary, and refrain from causing users unnecessary stress by telling them their condition is more serious that it truly is. It\u2019s reasonable to assume that the model underlying ChatGPT Health exhibited those behaviors in testing, though Bitterman notes that some of the prompts in HealthBench were generated by LLMs, not users, which could limit how well the benchmark translates into the real world. An LLM that avoids alarmism seems like a clear improvement over systems that have people convincing themselves they have cancer after a few minutes of browsing. And as large language models, and the products built around them, continue to develop, whatever advantage Dr. ChatGPT has over Dr. Google will likely grow. The introduction of ChatGPT Health is certainly a move in that direction: By looking through your medical records, ChatGPT can potentially gain far more context about your specific health situation than could be included in any Google search, although numerous experts have cautioned against giving ChatGPT that access for privacy reasons. Even if ChatGPT Health and other new tools do represent a meaningful improvement over Google searches, they could still conceivably have a negative effect on health overall. Much as automated vehicles, even if they are safer than human-driven cars, might still prove a net negative if they encourage people to use public transit less, LLMs could undermine users\u2019 health if they induce people to rely on the internet instead of human doctors, even if they do increase the quality of health information available online. Lederman says that this outcome is plausible. In her research, she has found that members of online communities centered on health tend to put their trust in users who express themselves well, regardless of the validity of the information they are sharing. Because ChatGPT communicates like an articulate person, some people might trust it too much, potentially to the exclusion of their doctor. But LLMs are certainly no replacement for a human doctor\u2014at least not yet. Correction 1/26: A previous version of this story incorrectly referred to the version of ChatGPT that Rambhatla evaluated. It was GPT-4, not GPT-4o.  hide"
  ]
}