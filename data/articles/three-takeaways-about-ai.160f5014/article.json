{
  "url": "https://www.technologyreview.com/2025/05/22/1117277/ai-energy-three-takeaways/",
  "title": "Three takeaways about AI\u2019s energy use and climate impacts",
  "ut": 1747870200.0,
  "body_paragraphs": [
    "This week, we published Power Hungry, a package all about AI and energy. At the center of this package is the most comprehensive look yet at AI\u2019s growing power demand, if I do say so myself.\u00a0 This data-heavy story is the result of over six months of reporting by me and my colleague James O\u2019Donnell (and the work of many others on our team). Over that time, with the help of leading researchers, we quantified the energy and emissions impacts of individual queries to AI models and tallied what it all adds up to, both right now and for the years ahead.\u00a0  There\u2019s a lot of data to dig through, and I hope you\u2019ll take the time to explore the whole story. But in the meantime, here are three of my biggest takeaways from working on this project.\u00a0 1. The energy demands of AI are anything but constant.\u00a0 If you\u2019ve heard estimates of AI\u2019s toll, it\u2019s probably a single number associated with a query, likely to OpenAI\u2019s ChatGPT. One popular estimate is that writing an email with ChatGPT uses 500 milliliters (or roughly a bottle) of water. But as we started reporting, I was surprised to learn just how much the details of a query can affect its energy demand. No two queries are the same\u2014for several reasons, including their complexity and the particulars of the model being queried.",
    "Related StoryEverything you need to know about estimating AI\u2019s energy and emissions burdenHere\u2019s how MIT Technology Review waded through a mess of data and hidden variables to calculate the individual and collective energy demand from AI.",
    "One key caveat here is that we don\u2019t know much about \u201cclosed source\u201d models\u2014for these, companies hold back the details of how they work. (OpenAI\u2019s ChatGPT and Google\u2019s Gemini are examples.) Instead, we worked with researchers who measured the energy it takes to run open-source AI models, for which the source code is publicly available.\u00a0 But using open-source models, it\u2019s possible to directly measure the energy used to respond to a query rather than just guess. We worked with researchers who generated text, images, and video and measured the energy required for the chips the models are based on to perform the task.",
    "Even just within the text responses, there was a pretty large range of energy needs. A complicated travel itinerary consumed nearly 10 times as much energy as a simple request for a few jokes, for example. An even bigger difference comes from the size of the model used. Larger models with more parameters used up to 70 times more energy than smaller ones for the same prompts.\u00a0 As you might imagine, there\u2019s also a big difference between text, images, or video. Videos generally took hundreds of times more energy to generate than text responses.\u00a0 2. What\u2019s powering the grid will greatly affect the climate toll of AI\u2019s energy use.\u00a0 As the resident climate reporter on this project, I was excited to take the expected energy toll and translate it into an expected emissions burden.\u00a0 Powering a data center with a nuclear reactor or a whole bunch of solar panels and batteries will not affect our planet the same way as burning mountains of coal. To quantify this idea, we used a figure called carbon intensity, a measure of how dirty a unit of electricity is on a given grid.\u00a0 We found that the same exact query, with the same exact energy demand, will have a very different climate impact depending on what the data center is powered by, and that depends on the location and the time of day. For example, querying a data center in West Virginia could cause nearly twice the emissions of querying one in California, according to calculations based on average data from 2024. This point shows why it matters where tech giants are building data centers, what the grid looks like in their chosen locations, and how that might change with more demand from the new infrastructure.\u00a0 3. There is still so much that we don't know when it comes to AI and energy.\u00a0 Our reporting resulted in estimates that are some of the most specific and comprehensive out there. But ultimately, we still have no idea what many of the biggest, most influential models are adding up to in terms of energy and emissions. None of the companies we reached out to were willing to provide numbers during our reporting. Not one. Adding up our estimates can only go so far, in part because AI is increasingly everywhere. While today you might generally have to go to a dedicated site and type in questions, in the future AI could be stitched into the fabric of our interactions with technology. (See my colleague Will Douglas Heaven\u2019s new story on Google\u2019s I/O showcase: \u201cBy putting AI into everything, Google wants to make it invisible.\u201d)",
    "AI could be one of the major forces that shape our society, our work, and our power grid. Knowing more about its consequences could be crucial to planning our future.\u00a0 To dig into our reporting, give the main story a read. And if you\u2019re looking for more details on how we came up with our numbers, you can check out this behind-the-scenes piece. There are also some great related stories in this package, including one from James Temple on the data center boom in the Nevada desert, one from David Rotman about how AI\u2019s rise could entrench natural gas, and one from Will Douglas Heaven on a few technical innovations that could help make AI more efficient. Oh, and I also have a piece on why nuclear isn\u2019t the easy answer some think it is.\u00a0 Find them, and the rest of the stories in the package, here.\u00a0 This article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.  hide"
  ]
}