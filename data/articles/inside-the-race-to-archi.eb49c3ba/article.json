{
  "url": "https://www.technologyreview.com/2025/02/07/1111328/inside-the-race-to-archive-the-us-governments-websites/",
  "title": "Inside the race to archive the US government\u2019s websites",
  "ut": 1738908000.0,
  "body_paragraphs": [
    "Over the past three weeks, the new US presidential administration has taken down thousands of government web pages related to public health, environmental justice, and scientific research. The mass takedowns stem from the new administration\u2019s push to remove government information related to diversity and \u201cgender ideology,\u201d as well as scrutiny of various government agencies\u2019 practices.\u00a0 USAID\u2019s website is down. So are sites related to it, like childreninadversity.gov, as well as thousands of pages from the Census Bureau, the Centers for Disease Control and Prevention, and the Office of Justice Programs.  \u201cWe\u2019ve never seen anything like this,\u201d says David Kaye, professor of law at the University of California, Irvine, and the former UN Special Rapporteur for freedom of opinion and expression. \u201cI don\u2019t think any of us know exactly what is happening. What we can see is government websites coming down, databases of essential public interest. The entirety of the USAID website.\u201d But as government web pages go dark, a collection of organizations are trying to archive as much data and information as possible before it\u2019s gone for good. The hope is to keep a record of what has been lost for scientists and historians to be able to use in the future.",
    "Data archiving is generally considered to be nonpartisan, but the recent actions of the administration have spurred some in the preservation community to stand up.\u00a0 \u201cI consider the actions of the current administration an assault on the entire scientific enterprise,\u201d says Margaret Hedstrom, professor emerita of information at the University of Michigan.",
    "Various organizations are trying to scrounge up as much data as possible. One of the largest projects is the End of Term Web Archive, a nonpartisan coalition of many organizations that aims to make a copy of all government data at the end of each presidential term. The EoT Archive allows individuals to nominate specific websites or data sets for preservation. \u201cAll we can do is collect what has been published and archive it and make sure it\u2019s publicly accessible for the future,\u201d says James Jacobs, US government information librarian at Stanford University, who is one of the people running the EoT Archive.\u00a0 Other organizations are taking a specific angle on data collection. For example, the Open Environmental Data Project (OEDP) is trying to capture data related to climate science and environmental justice. \u201cWe\u2019re trying to track what\u2019s getting taken down,\u201d says Katie Hoeberling, director of policy initiatives at OEDP. \u201cI can\u2019t say with certainty exactly how much of what used to be up is still up, but we\u2019re seeing, especially in the last couple weeks, an accelerating rate of data getting taken down.\u201d\u00a0 Related StoryWhy a ruling against the Internet Archive threatens the future of America\u2019s librariesThe decision locks libraries into an ecosystem that is not in readers' interests. Congress must act.",
    "In addition to tracking what\u2019s happening, OEDP is actively backing up relevant data. It actually began this process in November, to capture the data at the end of former president Biden\u2019s term. But efforts have ramped up in the last couple weeks. \u201cThings were a lot calmer prior to the inauguration,\u201d says Cathy Richards, a technologist at OEDP. \u201cIt was the second day of the new administration that the first platform went down. At that moment, everyone realized, \u2018Oh, no\u2014we have to keep doing this, and we have to keep working our way down this list of data sets.\u2019\u201d  This kind of work is crucial because the US government holds invaluable international and national data relating to climate. \u201cThese are irreplaceable repositories of important climate information,\u201d says Lauren Kurtz, executive director of the Climate Science Legal Defense Fund. \u201cSo fiddling with them or deleting them means the irreplaceable loss of critical information. It\u2019s really quite tragic.\u201d Like the OEDP, the Catalyst Cooperative is trying to make sure data related to climate and energy is stored and accessible for researchers. Both are part of the Public Environmental Data Partners, a collective of organizations dedicated to preserving federal environmental data. \u201dWe have tried to identify data sets that we know our communities make use of to make decisions about what electricity we should procure or to make decisions about resiliency in our infrastructure planning,\u201d says Christina Gosnell, cofounder and president of Catalyst.\u00a0 Archiving can be a difficult task; there is no one easy way to store all the US government\u2019s data. \u201cVarious federal agencies and departments handle data preservation and archiving in a myriad of ways,\u201d says Gosnell. There\u2019s also no one who has a complete list of all the government websites in existence.\u00a0 Related StoryThe race to save our online lives from a digital dark ageWe\u2019re making more data than ever. What can\u2014and should\u2014we save for future generations? And will they be able to understand it?",
    "This hodgepodge of data means that in addition to using web crawlers, which are tools used to capture snapshots of websites and data, archivists often have to manually scrape data as well. Additionally, sometimes a data set will be behind a login address or captcha to prevent scraper tools from pulling the data. Web scrapers also sometimes miss key features on a site. For example, sites will often have plenty of links to other pieces of information that aren\u2019t captured in a scrape. Or the scrape may just not work because of something to do with a website\u2019s structure. Therefore, having a person in the loop double-checking the scraper\u2019s work or capturing data manually is often the only way to ensure that the information is properly collected.",
    "And there are questions about whether scraping the data will really be enough. Restoring websites and complex data sets is often not a simple process. \u201cIt becomes extraordinarily difficult and costly to attempt to rescue and salvage the data,\u201d says Hedstrom. \u201cIt is like draining a body of blood and expecting the body to continue to function. The repairs and attempts to recover are sometimes insurmountable where we need continuous readings of data.\u201d \u201cAll of this data archiving work is a temporary Band-Aid,\u201d says Gosnell. \u201cIf data sets are removed and are no longer updated, our archived data will become increasingly stale and thus ineffective at informing decisions over time.\u201d\u00a0 These effects may be long-lasting. \u201cYou won\u2019t see the impact of that until 10 years from now, when you notice that there\u2019s a gap of four years of data,\u201d says Jacobs.\u00a0 Many digital archivists stress the importance of understanding our past. \u201cWe can all think about our own family photos that have been passed down to us and how important those different documents are,\u201d says Trevor Owens, chief research officer at the American Institute of Physics and former director of digital services at the Library of Congress. \u201cThat chain of connection to the past is really important.\u201d \u201cIt\u2019s our library; it\u2019s our history,\u201d says Richards. \u201cThis data is funded by taxpayers, so we definitely don\u2019t want all that knowledge to be lost when we can keep it, store it, potentially do something with it and continue to learn from it.\u201d hide"
  ]
}