{
  "url": "https://www.technologyreview.com/2024/10/22/1106041/would-you-trust-ai-to-mediate-an-argument/",
  "title": "Would you trust AI to mediate an argument?",
  "ut": 1729552458.0,
  "body_paragraphs": [
    "This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. I\u2019ve recently been feeling heartbroken. A very close friend recently cut off contact with me. I don\u2019t really understand why, and my attempts at fixing the situation have backfired. Situations like this are hurtful and confusing. So it\u2019s no wonder that people are increasingly turning to AI chatbots to help solve them. And there\u2019s good news: AI might actually be able to help.\u00a0  Researchers from Google DeepMind recently trained a system of large language models to help people come to agreement over complex but important social or political issues. The AI model was trained to identify and present areas where people\u2019s ideas overlapped. With the help of this AI mediator, small groups of study participants became less divided in their positions on various issues.\u00a0You can read more from\u00a0Rhiannon Williams here.\u00a0\u00a0\u00a0 One of the best uses for AI chatbots is for brainstorming.\u00a0I\u2019ve had success in the past using them to draft more assertive or persuasive emails for awkward situations, such as complaining about services or negotiating bills. This latest research suggests they could help us to see things from other people\u2019s perspectives too. So why not use AI to patch things up with my friend?",
    "I described the conflict, as I see it, to ChatGPT and asked for advice about what I should do. The response was very validating, because the AI chatbot supported the way I had approached the problem. The advice it gave was along the lines of what I had thought about doing anyway. I found it helpful to chat with the bot and get more ideas about how to deal with my specific situation. But ultimately, I was left dissatisfied, because the advice was still pretty generic and vague (\u201cSet your boundary calmly\u201d and \u201cCommunicate your feelings\u201d) and didn\u2019t really offer the kind of insight a therapist might.\u00a0 And there\u2019s another problem: Every argument has two sides.\u00a0I started a new chat, and described the problem as I believe my friend sees it. The chatbot supported and validated my friend\u2019s decisions, just as it did for me. On one hand, this exercise helped me see things from her perspective. I had, after all, tried to empathize with the other person, not just win an argument. But on the other hand, I can totally see a situation where relying too much on the advice of a chatbot that tells us what we want to hear could cause us to double down, preventing us from seeing things from the other person\u2019s perspective.",
    "This served as a good reminder: An AI chatbot is not a therapist or a friend.\u00a0While it can parrot the vast reams of internet text it\u2019s been trained on, it doesn\u2019t understand what it\u2019s like to feel sadness, confusion, or joy. That\u2019s why I would tread with caution when using AI chatbots for things that really matter to you, and not take what they say at face value.\u00a0 An AI chatbot can never replace a real conversation, where both sides are willing to truly listen and take the other\u2019s point of view into account. So I decided to ditch the AI-assisted therapy talk and reached out to my friend one more time. Wish me luck!\u00a0  Now read the rest of The Algorithm Deeper Learning OpenAI says ChatGPT treats us all the same (most of the time) Does ChatGPT treat you the same whether you\u2019re a Laurie, Luke, or Lashonda? Almost, but not quite. OpenAI has analyzed millions of conversations with its hit chatbot and found that ChatGPT will produce a harmful gender or racial stereotype based on a user\u2019s name in around one in 1,000 responses on average, and as many as one in 100 responses in the worst case. Why this matters:\u00a0Bias in AI is a huge problem. Ethicists have long studied the impact of bias when companies use AI models to screen r\u00e9sum\u00e9s or loan applications, for example. But the rise of chatbots, which enable individuals to interact with models directly, brings a new spin to the problem.\u00a0Read more from Will Douglas Heaven.\u00a0 Bits and Bytes Intro to AI: a beginner\u2019s guide to artificial intelligence from\u00a0MIT Technology ReviewThere is an overwhelming amount of AI news, and it is a lot to keep up with. Do you wish someone would just take a step back and explain some of the basics? Look no further.\u00a0Intro to AI\u00a0is\u00a0MIT Technology Review\u2019s\u00a0first newsletter that also serves as a mini-course. You\u2019ll get one email a week for six weeks, and each edition will walk you through a different topic in AI.\u00a0Sign up here.\u00a0 The race to find new materials with AI needs more data. Meta is giving massive amounts away for free.Meta is releasing a massive data set and models, called Open Materials 2024, that could help scientists use AI to discover new materials much faster. OMat24 tackles one of the biggest bottlenecks in the discovery process: a lack of data.\u00a0(MIT Technology Review)\u00a0 Cracks are starting to appear in Microsoft\u2019s \u201cbromance\u201d with OpenAI\u00a0As part of OpenAI\u2019s transition from a research lab to a for-profit company, it has tried to renegotiate its deal with Microsoft to secure more computing power and funding. Meanwhile, Microsoft has started to invest in other AI projects, such as DeepMind cofounder Mustafa Suleyman\u2019s Inflection AI, to reduce its reliance on OpenAI\u2014much to Sam Altman\u2019s chagrin.\u00a0(The New York Times)\u00a0 Millions of people are using abusive AI \u201cnudify\u201d bots on Telegram\u00a0The messaging app is a hotbed for popular AI bots that \u201cremove clothes\u201d from photos of people to create nonconsensual deepfake images. (Wired)\u00a0 hide"
  ]
}