{
  "url": "https://www.technologyreview.com/2025/09/02/1122871/therapists-using-chatgpt-secretly/",
  "title": "Therapists are secretly using ChatGPT. Clients are triggered.",
  "ut": 1756768104.0,
  "body_paragraphs": [
    "Declan would never have found out his therapist was using ChatGPT had it not been for a technical mishap. The connection was patchy during one of their online sessions, so Declan suggested they turn off their video feeds. Instead, his therapist began inadvertently sharing his screen. \u201cSuddenly, I was watching him use ChatGPT,\u201d says Declan, 31, who lives in Los Angeles. \u201cHe was taking what I was saying and putting it into ChatGPT, and then summarizing or cherry-picking answers.\u201d  Declan was so shocked he didn\u2019t say anything, and for the rest of the session he was privy to a real-time stream of ChatGPT analysis rippling across his therapist\u2019s screen. The session became even more surreal when Declan began echoing ChatGPT in his own responses, preempting his therapist.\u00a0 \u201cI became the best patient ever,\u201d he says, \u201cbecause ChatGPT would be like, \u2018Well, do you consider that your way of thinking might be a little too black and white?\u2019 And I would be like, \u2018Huh, you know, I think my way of thinking might be too black and white,\u2019 and [my therapist would] be like, \u2018Exactly.\u2019 I\u2019m sure it was his dream session.\u201d",
    "Among the questions racing through Declan\u2019s mind was, \u201cIs this legal?\u201d When Declan raised the incident with his therapist at the next session\u2014\u201cIt was super awkward, like a weird breakup\u201d\u2014the therapist cried. He explained he had felt they\u2019d hit a wall and had begun looking for answers elsewhere. \u201cI was still charged for that session,\u201d Declan says, laughing. The large language model (LLM) boom of the past few years has had unexpected ramifications for the field of psychotherapy, mostly because a growing number of people are substituting the likes of ChatGPT for human therapists. But less discussed is how some therapists themselves are integrating AI into their practice. As in many other professions, generative AI promises tantalizing efficiency gains, but its adoption risks compromising sensitive patient data and undermining a relationship in which trust is paramount.",
    "Suspicious sentiments Declan is not alone, as I can attest from personal experience. When I received a recent email from my therapist that seemed longer and more polished than usual, I initially felt heartened. It seemed to convey a kind, validating message, and its length made me feel that she\u2019d taken the time to reflect on all the points in my (rather sensitive) email. On closer inspection, though, her email seemed a little strange. It was in a new font, and the text displayed several AI \u201ctells,\u201d including liberal use of the Americanized em dash (we\u2019re both from the UK), the signature impersonal style, and the habit of addressing each point made in the original email line by line. Related StoryPeople are using AI to \u2018sit\u2019 with them while they trip on psychedelicsRead next My positive feelings quickly drained away, to be replaced by disappointment and mistrust, once I realized ChatGPT likely had a hand in drafting the message\u2014which my therapist confirmed when I asked her. Despite her assurance that she simply dictates longer emails using AI, I still felt uncertainty over the extent to which she, as opposed to the bot, was responsible for the sentiments expressed. I also couldn\u2019t entirely shake the suspicion that she might have pasted my highly personal email wholesale into ChatGPT.  When I searched the internet to see whether others had had similar experiences, I found plenty of examples of people receiving what they suspected were AI-generated communiqu\u00e9s from their therapists. Many, including Declan, had taken to Reddit to solicit emotional support and advice. So had Hope, 25, who lives on the East Coast of the US and had direct-messaged her therapist about the death of her dog. She soon received a message back. It would have seemed consoling and thoughtful\u2014expressing how hard it must be \u201cnot having him by your side right now\u201d\u2014were it not for the reference to the AI prompt accidentally preserved at the top: \u201cHere\u2019s a more human, heartfelt version with a gentle, conversational tone.\u201d Hope says she felt \u201chonestly really surprised and confused.\u201d \u201cIt was just a very strange feeling,\u201d she says. \u201cThen I started to feel kind of betrayed \u2026 It definitely affected my trust in her.\u201d This was especially problematic, she adds, because \u201cpart of why I was seeing her was for my trust issues.\u201d Hope, who had believed her therapist to be competent and empathetic, \u201cnever would have suspected her to feel the need to use AI,\u201d she says. Her therapist was apologetic when confronted, and she explained that because she\u2019d never had a pet herself, she\u2019d turned to AI for help expressing the appropriate sentiment.",
    "A disclosure dilemma\u00a0 There may be some merit to the argument that AI could help therapists communicate with their clients. A 2025 study published in PLOS Mental Health asked therapists to use ChatGPT to respond to vignettes describing problems of the kind patients might raise in therapy. Not only was a panel of 830 participants unable to distinguish between the human and AI responses, but AI responses were rated as conforming better to therapeutic best practices.\u00a0 However, when participants suspected responses to have been written by ChatGPT, they ranked them lower. (Responses written by ChatGPT but misattributed to therapists received the highest ratings overall.)\u00a0 Similarly, Cornell University researchers found in a 2023 study that AI-generated messages can increase feelings of closeness and cooperation between interlocutors, but only if the recipient remains oblivious to the role of AI. The mere suspicion of its use was found to rapidly sour goodwill. \u201cPeople value authenticity, particularly in psychotherapy,\u201d says Adrian Aguilera, a clinical psychologist and professor at the University of California, Berkeley. \u201cI think [using AI] can feel like, \u2018You\u2019re not taking my relationship seriously.\u2019 Do I ChatGPT a response to my wife or my kids? That wouldn\u2019t feel genuine.\u201d  In 2023, in the early days of generative AI, the online therapy service Koko conducted a clandestine experiment on its users, mixing responses generated by GPT-3 with ones drafted by humans. They discovered that users tended to rate the AI-generated responses more positively. The revelation that users had unwittingly been experimented on, however, sparked outrage. The online therapy provider BetterHelp has also been subject to claims that its therapists have used AI to draft responses. In a Medium post, photographer Brendan Keen said his BetterHelp therapist admitted to using AI in replies, leading to \u201can acute sense of betrayal\u201d and persistent worry, despite reassurances, that his data privacy had been breached. He ended the relationship thereafter.\u00a0 A BetterHelp spokesperson told us the company \u201cprohibits therapists from disclosing any member\u2019s personal or health information to third-party artificial intelligence, or using AI to craft messages to members to the extent it might directly or indirectly have the potential to identify someone.\u201d All these examples relate to undisclosed AI usage. Aguilera believes time-strapped therapists can make use of LLMs, but transparency is essential. \u201cWe have to be up-front and tell people, \u2018Hey, I\u2019m going to use this tool for X, Y, and Z\u2019 and provide a rationale,\u201d he says. People then receive AI-generated messages with that prior context, rather than assuming their therapist is \u201ctrying to be sneaky.\u201d",
    "Psychologists are often working at the limits of their capacity, and levels of burnout in the profession are high, according to research conducted in 2023 by the American Psychological Association. That context makes the appeal of AI-powered tools obvious.\u00a0 But lack of disclosure risks permanently damaging trust. Hope decided to continue seeing her therapist, though she stopped working with her a little later for reasons she says were unrelated. \u201cBut I always thought about the AI Incident whenever I saw her,\u201d she says.",
    "Risking patient privacy Beyond the transparency issue, many therapists are leery of using LLMs in the first place, says Margaret Morris, a clinical psychologist and affiliate faculty member at the University of Washington. \u201cI think these tools might be really valuable for learning,\u201d she says, noting that therapists should continue developing their expertise over the course of their career. \u201cBut I think we have to be super careful about patient data.\u201d Morris calls Declan\u2019s experience \u201calarming.\u201d\u00a0  Therapists need to be aware that general-purpose AI chatbots like ChatGPT are not approved by the US Food and Drug Administration and are not HIPAA compliant, says Pardis Emami-Naeini, assistant professor of computer science at Duke University, who has researched the privacy and security implications of LLMs in a health context. (HIPAA is a set of US federal regulations that protect people\u2019s sensitive health information.) \u201cThis creates significant risks for patient privacy if any information about the patient is disclosed or can be inferred by the AI,\u201d she says. In a recent paper, Emami-Naeini found that many users wrongly believe ChatGPT is HIPAA compliant, creating an unwarranted sense of trust in the tool. \u201cI expect some therapists may share this misconception,\u201d she says. As a relatively open person, Declan says, he wasn\u2019t completely distraught to learn how his therapist was using ChatGPT. \u201cPersonally, I am not thinking, \u2018Oh, my God, I have deep, dark secrets,\u2019\u201d he said. But it did still feel violating: \u201cI can imagine that if I was suicidal, or on drugs, or cheating on my girlfriend \u2026 I wouldn\u2019t want that to be put into ChatGPT.\u201d",
    "When using AI to help with email, \u201cit\u2019s not as simple as removing obvious identifiers such as names and addresses,\u201d says Emami-Naeini. \u201cSensitive information can often be inferred from seemingly nonsensitive details.\u201d She adds, \u201cIdentifying and rephrasing all potential sensitive data requires time and expertise, which may conflict with the intended convenience of using AI tools. In all cases, therapists should disclose their use of AI to patients and seek consent.\u201d\u00a0 A growing number of companies, including Heidi Health, Upheal, Lyssn, and Blueprint, are marketing specialized tools to therapists, such as AI-assisted note-taking, training, and transcription services. These companies say they are HIPAA compliant and store data securely using encryption and pseudonymization where necessary. But many therapists are still wary of the privacy implications\u2014particularly when it comes to services that necessitate the recording of entire sessions. \u201cEven if privacy protections are improved, there is always some risk of information leakage or secondary uses of data,\u201d says Emami-Naeini.",
    "A 2020 hack on a Finnish mental-health company, which resulted in tens of thousands of clients\u2019 treatment records being accessed, serves as a warning. People on the list were blackmailed, and subsequently the entire trove was publicly released, revealing extremely sensitive details such as people\u2019s experiences of child abuse and addiction. What therapists stand to lose In addition to violation of data privacy, other risks are involved when psychotherapists consult LLMs on behalf of a client. Studies have found that although responses from some specialized therapy bots can rival human-delivered interventions, advice from the likes of ChatGPT can cause more harm than good. A recent Stanford University study, for example, found that chatbots can fuel delusions and psychopathy by blindly validating users rather than challenging them. The research also found that the bots can suffer from biases and engage in sycophancy. The same flaws could make it risky for therapists to consult chatbots on behalf of their clients. The technology could, for example, baselessly validate a hunch or lead a therapist down the wrong path. Aguilera says he has played around with tools like ChatGPT while teaching mental-health trainees, sometimes entering hypothetical symptoms and asking the AI chatbot to make a diagnosis. The tool will bring up lots of possible conditions, but it\u2019s rather thin in its analysis, he says. The American Counseling Association recommends that AI not be used for mental-health diagnosis at present. A study published in 2024 on an earlier version of ChatGPT similarly found it was too vague and general to be truly useful in diagnosis or devising treatment plans, and it was heavily biased toward recommending cognitive behavioral therapy as opposed to other types of therapy that might be more suitable. Daniel Kimmel, a psychiatrist and neuroscientist at Columbia University, conducted experiments with ChatGPT where he posed as a client having relationship troubles. He says he found the chatbot was a decent mimic when it came to \u201cstock-in-trade\u201d therapeutic responses, like normalizing and validating, asking for additional information, or highlighting certain cognitive or emotional associations. However, \u201cit didn\u2019t do a lot of digging,\u201d he says. It didn\u2019t attempt \u201cto link seemingly or superficially unrelated things together into something cohesive \u2026 to come up with a story, an idea, a theory.\u201d \u201cI would be skeptical about using it to do the thinking for you,\u201d he says. Thinking, he says, should be the job of therapists. Therapists could save time using AI-powered tech, but this benefit should be weighed against the needs of patients, says Morris: \u201cMaybe you\u2019re saving yourself a couple of minutes. But what are you giving away?\u201d hide"
  ]
}