{
  "url": "https://www.technologyreview.com/2025/07/08/1119777/scientists-use-ai-unlock-human-mind/",
  "title": "How scientists are trying to use AI to unlock the human mind",
  "ut": 1751932800.0,
  "body_paragraphs": [
    "Today\u2019s AI landscape is defined by the ways in which neural networks are unlike human brains. A toddler learns how to communicate effectively with only a thousand calories a day and regular conversation; meanwhile, tech companies are reopening nuclear power plants, polluting marginalized communities, and pirating terabytes of books in order to train and run their LLMs. But neural networks are, after all, neural\u2014they\u2019re inspired by brains. Despite their vastly different appetites for energy and data, large language models and human brains do share a good deal in common. They\u2019re both made up of millions of subcomponents: biological neurons in the case of the brain, simulated \u201cneurons\u201d in the case of networks. They\u2019re the only two things on Earth that can fluently and flexibly produce language. And scientists barely understand how either of them works.  I can testify to those similarities: I came to journalism, and to AI, by way of six years of neuroscience graduate school. It\u2019s a common view among neuroscientists that building brainlike neural networks is one of the most promising paths for the field, and that attitude has started to spread to psychology. Last week, the prestigious journal Nature published a pair of studies showcasing the use of neural networks for predicting how humans and other animals behave in psychological experiments. Both studies propose that these trained networks could help scientists advance their understanding of the human mind. But predicting a behavior and explaining how it came about are two very different things. In one of the studies, researchers transformed a large language model into what they refer to as a \u201cfoundation model of human cognition.\u201d Out of the box, large language models aren\u2019t great at mimicking human behavior\u2014they behave logically in settings where humans abandon reason, such as casinos. So the researchers fine-tuned Llama 3.1, one of Meta\u2019s open-source LLMs, on data from a range of 160 psychology experiments, which involved tasks like choosing from a set of \u201cslot machines\u201d to get the maximum payout or remembering sequences of letters. They called the resulting model Centaur.",
    "Related StoryHow AI video games can help reveal the mysteries of the human mindThe way we interact with games and invented characters can shed light on how we think. AI is poised to take things further.",
    "Compared with conventional psychological models, which use simple math equations, Centaur did a far better job of predicting behavior. Accurate predictions of how humans respond in psychology experiments are valuable in and of themselves: For example, scientists could use Centaur to pilot their experiments on a computer before recruiting, and paying, human participants. In their paper, however, the researchers propose that Centaur could be more than just a prediction machine. By interrogating the mechanisms that allow Centaur to effectively replicate human behavior, they argue, scientists could develop new theories about the inner workings of the mind. But some psychologists doubt whether Centaur can tell us much about the mind at all. Sure, it\u2019s better than conventional psychological models at predicting how humans behave\u2014but it also has a billion times more parameters. And just because a model behaves like a human on the outside doesn\u2019t mean that it functions like one on the inside. Olivia Guest, an assistant professor of computational cognitive science at Radboud University in the Netherlands, compares Centaur to a calculator, which can effectively predict the response a math whiz will give when asked to add two numbers. \u201cI don\u2019t know what you would learn about human addition by studying a calculator,\u201d she says.",
    "Even if Centaur does capture something important about human psychology, scientists may struggle to extract any insight from the model\u2019s millions of neurons. Though AI researchers are working hard to figure out how large language models work, they\u2019ve barely managed to crack open the black box. Understanding an enormous neural-network model of the human mind may not prove much easier than understanding the thing itself. One alternative approach is to go small. The second of the two Nature studies focuses on minuscule neural networks\u2014some containing only a single neuron\u2014that nevertheless can predict behavior in mice, rats, monkeys, and even humans. Because the networks are so small, it\u2019s possible to track the activity of each individual neuron and use that data to figure out how the network is producing its behavioral predictions. And while there\u2019s no guarantee that these models function like the brains they were trained to mimic, they can, at the very least, generate testable hypotheses about human and animal cognition. There\u2019s a cost to comprehensibility. Unlike Centaur, which was trained to mimic human behavior in dozens of different tasks, each tiny network can only predict behavior in one specific task. One network, for example, is specialized for making predictions about how people choose among different slot machines. \u201cIf the behavior is really complex, you need a large network,\u201d says Marcelo Mattar, an assistant professor of psychology and neural science at New York University who led the tiny-network study and also contributed to Centaur. \u201cThe compromise, of course, is that now understanding it is very, very difficult.\u201d This trade-off between prediction and understanding is a key feature of neural-network-driven science. (I also happen to be writing a book about it.) Studies like Mattar\u2019s are making some progress toward closing that gap\u2014as tiny as his networks are, they can predict behavior more accurately than traditional psychological models. So is the research into LLM interpretability happening at places like Anthropic. For now, however, our understanding of complex systems\u2014from humans to climate systems to proteins\u2014is lagging farther and farther behind our ability to make predictions about them. This story originally appeared in\u00a0The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here.  hide"
  ]
}