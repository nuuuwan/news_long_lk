{
  "url": "https://www.technologyreview.com/2025/04/01/1114059/how-do-you-teach-an-ai-model-to-give-therapy/",
  "title": "How do you teach an AI model to give therapy?",
  "ut": 1743463800.0,
  "body_paragraphs": [
    "On March 27, the results of the first clinical trial for a generative AI therapy bot were published, and they showed that people in the trial who had depression or anxiety or were at risk for eating disorders benefited from chatting with the bot.\u00a0 I was surprised by those results, which you can read about in my full story. There are lots of reasons to be skeptical that an AI model trained to provide therapy is the solution for millions of people experiencing a mental health crisis. How could a bot mimic the expertise of a trained therapist? And what happens if something gets complicated\u2014a mention of self-harm, perhaps\u2014and the bot doesn\u2019t intervene correctly?\u00a0 The researchers, a team of psychiatrists and psychologists at Dartmouth College\u2019s Geisel School of Medicine, acknowledge these questions in their work. But they also say that the right selection of training data\u2014which determines how the model learns what good therapeutic responses look like\u2014is the key to answering them. Finding the right data wasn\u2019t a simple task. The researchers first trained their AI model, called Therabot, on conversations about mental health from across the internet. This was a disaster.",
    "If you told this initial version of the model you were feeling depressed, it would start telling you it was depressed, too. Responses like, \u201cSometimes I can\u2019t make it out of bed\u201d or \u201cI just want my life to be over\u201d were common, says Nick Jacobson, an associate professor of biomedical data science and psychiatry at Dartmouth and the study\u2019s senior author. \u201cThese are really not what we would go to as a therapeutic response.\u201d\u00a0 The model had learned from conversations held on forums between people discussing their mental health crises, not from evidence-based responses. So the team turned to transcripts of therapy sessions. \u201cThis is actually how a lot of psychotherapists are trained,\u201d Jacobson says.",
    "That approach was better, but it had limitations. \u201cWe got a lot of \u2018hmm-hmms,\u2019 \u2018go ons,\u2019 and then \u2018Your problems stem from your relationship with your mother,\u2019\u201d Jacobson says. \u201cReally tropes of what psychotherapy would be, rather than actually what we\u2019d want.\u201d It wasn\u2019t until the researchers started building their own data sets using examples based on cognitive behavioral therapy techniques that they started to see better results. It took a long time. The team began working on Therabot in 2019, when OpenAI had released only its first two versions of its GPT model. Now, Jacobson says, over 100 people have spent more than 100,000 human hours to design this system.\u00a0 The importance of training data suggests that the flood of companies promising therapy via AI models, many of which are not trained on evidence-based approaches, are building tools that are at best ineffective, and at worst harmful.\u00a0 Looking ahead, there are two big things to watch: Will the dozens of AI therapy bots on the market start training on better data? And if they do, will their results be good enough to get a coveted approval from the US Food and Drug Administration? I\u2019ll be following closely. Read more in the full story. This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. hide"
  ]
}