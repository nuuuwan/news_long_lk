{
  "url": "https://www.technologyreview.com/2025/09/09/1123386/help-my-therapist-is-secretly-using-chatgpt/",
  "title": "Help! My therapist is secretly using ChatGPT",
  "ut": 1757374200.0,
  "body_paragraphs": [
    "In Silicon Valley\u2019s imagined future, AI models are so empathetic that we\u2019ll use them as therapists. They\u2019ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.\u00a0 Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn\u2019t subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.\u00a0  It\u2019s my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to. As the writer of the story, Laurie Clarke, points out, it\u2019s not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.",
    "I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it? In all the cases mentioned in the piece, the therapist hadn\u2019t provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that\u2019s how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they\u2019re going to use AI and how (if they plan to use it). If they don\u2019t, it raises all these really uncomfortable questions for patients when it\u2019s uncovered and risks irrevocably damaging the trust that\u2019s been built.",
    "Related StoryThe first trial of generative AI therapy shows it might help with depressionThe evidence-backed model delivered impressive results, but it doesn\u2019t validate the wave of AI therapy bots flooding the market.",
    "In the examples you\u2019ve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what\u2019s bothering someone? Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools. There is some evidence AI can deliver more standardized, \"manualized\" therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it\u2019s possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT. What happens if this goes awry? What attention is this getting from ethics groups and lawmakers? At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow. OpenAI\u2019s Sam Altman said last month that \u201ca lot of people effectively use ChatGPT as a sort of therapist,\u201d and that to him, that\u2019s a good thing. Do you think tech companies are overpromising on AI\u2019s ability to help us? I think that tech companies are subtly encouraging this use of AI because clearly it\u2019s a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn\u2019t really \u201ctherapy\u201d by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I\u2019ve never in my life looked forward to a (real, in-person) therapy session. They\u2019re often highly uncomfortable, and even distressing. But that\u2019s part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn\u2019t do any of these things.\u00a0 Read the full story from Laurie Clarke.\u00a0 This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. hide"
  ]
}