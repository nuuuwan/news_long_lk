{
  "url": "https://www.technologyreview.com/2025/11/18/1128065/googles-gemini-3/",
  "title": "Google\u2019s new Gemini 3 \u201cvibe-codes\u201d responses and comes with its own agent",
  "ut": 1763443807.0,
  "body_paragraphs": [
    "EXECUTIVE SUMMARY Google today unveiled Gemini 3, a major upgrade to its flagship multimodal model. The firm says the new model is better at reasoning, has more fluid multimodal capabilities (the ability to work across voice, text or images), and will work like an agent.\u00a0 The previous model, Gemini 2.5, supports multimodal input. Users can feed it images, handwriting, or voice. But it usually requires explicit instructions about the format the user wants back, and it defaults to plain text regardless.\u00a0  But Gemini 3 introduces what Google calls \u201cgenerative interfaces,\u201d which allow the model to make its own choices about what kind of output fits the prompt best, assembling visual layouts and dynamic views on its own instead of returning a block of text.\u00a0 Ask for travel recommendations and it may spin up a website-like interface inside the app, complete with modules, images, and follow-up prompts such as \u201cHow many days are you traveling?\u201d or \u201cWhat kinds of activities do you enjoy?\u201d It also presents clickable options based on what you might want next.",
    "When asked to explain a concept, Gemini 3 may sketch a diagram or generate a simple animation on its own if it believes a visual is more effective.\u00a0 Related StoryGemini Robotics uses Google\u2019s top language model to make robots more usefulRead next \u201cVisual layout generates an immersive, magazine-style view complete with photos and modules,\u201d says Josh Woodward, VP of Google Labs, Gemini, and AI Studio. \u201cThese elements don\u2019t just look good but invite your input to further tailor the results.\u201d",
    "With Gemini 3, Google is also introducing Gemini Agent, an experimental feature designed to handle multi-step tasks directly inside the app. The agent can connect to services such as Google Calendar, Gmail, and Reminders. Once granted access, it can execute tasks like organizing an inbox or managing schedules.\u00a0 Similar to other agents, it breaks tasks into discrete steps, displays its progress in real time, and pauses for approval from the user before continuing. Google describes the feature as a step toward \u201ca true generalist agent.\u201d It will be available on the web for Google AI Ultra subscribers in the US starting November 18. The overall approach can seem a lot like \u201cvibe coding,\u201d where users describe an end goal in plain language and let the model assemble the interface or code needed to get there. The update also ties Gemini more deeply into Google\u2019s existing products. In Search, a limited group of Google AI Pro and Ultra subscribers can now switch to Gemini 3 Pro, the reasoning variation of the new model, to receive deeper, more thorough AI-generated summaries that rely on the model\u2019s reasoning rather than the existing AI Mode. For shopping, Gemini will now pull from Google\u2019s Shopping Graph\u2014which the company says contains more than 50 billion product listings\u2014to generate its own recommendation guides. Users just need to ask a shopping-related question or search a shopping-related phrase, and the model assembles an interactive, Wirecutter-style product recommendation piece, complete with prices and product details, without redirecting to an external site. For developers, Google is also pushing single-prompt software generation further. The company introduced Google Antigravity, a\u00a0 development platform that acts as an all-in-one space where code, tools, and workflows can be created and managed from a single prompt. Derek Nee, CEO of Flowith, an agentic AI application, told MIT Technology Review that Gemini 3 Pro addresses several gaps in earlier models. Improvements include stronger visual understanding, better code generation, and better performance on long tasks\u2014features he sees as essential for developers of AI apps and agents.\u00a0 \u201cGiven its speed and cost advantages, we\u2019re integrating the new model into our product,\u201d he says. \u201cWe\u2019re optimistic about its potential, but we need deeper testing to understand how far it can go.\u201d\u00a0 hide"
  ]
}