{
  "url": "https://www.technologyreview.com/2025/05/22/1117338/anthropics-new-hybrid-ai-model-can-work-on-tasks-autonomously-for-hours-at-a-time/",
  "title": "Anthropic\u2019s new hybrid AI model can work on tasks autonomously for hours at a time",
  "ut": 1747898465.0,
  "body_paragraphs": [
    "Anthropic has announced two new AI models that it claims represent a major step toward making AI agents truly useful. AI agents trained on Claude Opus 4, the company\u2019s most powerful model to date, raise the bar for what such systems are capable of by tackling difficult tasks over extended periods of time and responding more usefully to user instructions, the company says.  Claude Opus 4 has been built to execute complex tasks that involve completing thousands of steps over several hours. For example, it created a guide for the video game Pok\u00e9mon Red while playing it for more than 24 hours straight. The company\u2019s previously most powerful model, Claude 3.7 Sonnet, was capable of playing for just 45 minutes, says Dianne Penn, product lead for research at Anthropic. Similarly, the company says that one of its customers, the Japanese technology company Rakuten, recently deployed Claude Opus 4 to code autonomously for close to seven hours on a complicated open-source project.",
    "Anthropic achieved these advances by improving the model\u2019s ability to create and maintain \u201cmemory files\u201d to store key information. This enhanced ability to \u201cremember\u201d makes the model better at completing longer tasks. \u201cWe see this model generation leap as going from an assistant to a true agent,\u201d says Penn. \u201cWhile you still have to give a lot of real-time feedback and make all of the key decisions for AI assistants, an agent can make those key decisions itself. It allows humans to act more like a delegator or a judge, rather than having to hold these systems\u2019 hands through every step.\u201d",
    "Related StoryWhy handing over total control to AI agents would be a huge mistakeWhen AI systems can control multiple sources simultaneously, the potential for harm explodes. We need to keep humans in the loop.",
    "While Claude Opus 4 will be limited to paying Anthropic customers, a second model, Claude Sonnet 4, will be available for both paid and free tiers of users. Opus 4 is being marketed as a powerful, large model for complex challenges, while Sonnet 4 is described as a smart, efficient model for everyday use.\u00a0\u00a0 Both of the new models are hybrid, meaning they can offer a swift reply or a deeper, more reasoned response depending on the nature of a request. While they calculate a response, both models can search the web or use other tools to improve their output. AI companies are currently locked in a race to create truly useful AI agents that are able to plan, reason, and execute complex tasks both reliably and free from human supervision, says Stefano Albrecht, director of AI at the startup DeepFlow and coauthor of Multi-Agent Reinforcement Learning: Foundations and Modern Approaches. Often this involves autonomously using the internet or other tools. There are still safety and security obstacles to overcome. AI agents powered by large language models can act erratically and perform unintended actions\u2014which becomes even more of a problem when they\u2019re trusted to act without human supervision. \u201cThe more agents are able to go ahead and do something over extended periods of time, the more helpful they will be, if I have to intervene less and less,\u201d he says. \u201cThe new models\u2019 ability to use tools in parallel is interesting\u2014that could save some time along the way, so that\u2019s going to be useful.\u201dAs an example of the sorts of safety issues AI companies are still tackling, agents can end up taking unexpected shortcuts or exploiting loopholes to reach the goals they\u2019ve been given. For example, they might book every seat on a plane to ensure that their user gets a seat, or resort to creative cheating to win a chess game. Anthropic says it managed to reduce this behavior, known as reward hacking, in both new models by 65% relative to Claude Sonnet 3.7. It achieved this by more closely monitoring problematic behaviors during training, and improving both the AI\u2019s training environment and the evaluation methods. hide"
  ]
}