{
  "url": "https://www.technologyreview.com/2026/01/28/1131835/what-ai-remembers-about-you-is-privacys-next-frontier/",
  "title": "What AI \u201cremembers\u201d about you is privacy\u2019s next frontier",
  "ut": 1769574457.0,
  "body_paragraphs": [
    "The ability to remember you and your preferences is rapidly becoming a big selling point for AI chatbots and agents.\u00a0 Earlier this month, Google announced Personal Intelligence, a new way for people to interact with the company\u2019s Gemini chatbot that draws on their Gmail, photos, search, and YouTube histories to make Gemini \u201cmore personal, proactive, and powerful.\u201d It echoes similar moves by OpenAI, Anthropic, and Meta to add new ways for their AI products to remember and draw from people\u2019s personal details and preferences. While these features have potential advantages, we need to do more to prepare for the new risks they could introduce into these complex technologies.  Personalized, interactive AI systems are built to act on our behalf, maintain context across conversations, and improve our ability to carry out all sorts of tasks, from booking travel to filing taxes. From tools that learn a developer\u2019s coding style to shopping agents that sift through thousands of products, these systems rely on the ability to store and retrieve increasingly intimate details about their users.\u00a0 But doing so over time introduces alarming, and all-too-familiar, privacy vulnerabilities\u2013\u2013many of which have loomed since \u201cbig data\u201d first teased the power of spotting and acting on user patterns. Worse, AI agents now appear poised to plow through whatever safeguards had been adopted to avoid those vulnerabilities.\u00a0 Today, we interact with these systems through conversational interfaces, and we frequently switch contexts. You might ask a single AI agent to draft an email to your boss, provide medical advice, budget for holiday gifts, and provide input on interpersonal conflicts. Most AI agents collapse all data about you\u2014which may once have been separated by context, purpose, or permissions\u2014into single, unstructured repositories. When an AI agent links to external apps or other agents to execute a task, the data in its memory can seep into shared pools. This technical reality creates the potential for unprecedented privacy breaches that expose not only isolated data points, but the entire mosaic of people\u2019s lives.",
    "Related StoryWhy handing over total control to AI agents would be a huge mistakeRead next When information is all in the same repository, it is prone to crossing contexts in ways that are deeply undesirable. A casual chat about dietary preferences to build a grocery list could later influence what health insurance options are offered, or a search for restaurants offering accessible entrances could leak into salary negotiations\u2014all without a user\u2019s awareness (this concern may sound familiar from the early days of \u201cbig data,\u201d but is now far less theoretical). An information soup of memory not only poses a privacy issue, but also makes it harder to understand an AI system\u2019s behavior\u2014and to govern it in the first place. So what can developers do to fix this problem?\u00a0 First, memory systems need structure that allows control over the purposes for which memories can be accessed and used. Early efforts appear to be underway: Anthropic\u2019s Claude creates separate memory areas for different \u201cprojects,\u201d and OpenAI says that information shared through ChatGPT Health is compartmentalized from other chats. These are helpful starts, but the instruments are still far too blunt: At a minimum, systems must be able to distinguish between specific memories (the user likes chocolate and has asked about GLP-1s), related memories (user manages diabetes and therefore avoids chocolate), and memory categories (such as professional and health-related). Further, systems need to allow for usage restrictions on certain types of memories and reliably accommodate explicitly defined boundaries\u2014particularly around memories having to do with sensitive topics like medical conditions or protected characteristics, which will likely be subject to stricter rules.",
    "Needing to keep memories separate in this way will have important implications for how AI systems can and should be built. It will require tracking memories\u2019 provenance\u2014their source, any associated time stamp, and the context in which they were created\u2014and building ways to trace when and how certain memories influence the behavior of an agent. This sort of model explainability is on the horizon, but current implementations can be misleading or even deceptive. Embedding memories directly within a model\u2019s weights may result in more personalized and context-aware outputs, but structured databases are currently more segmentable, more explainable, and thus more governable. Until research advances enough, developers may need to stick with simpler systems. Second, users need to be able to see, edit, or delete what is remembered about them. The interfaces for doing this should be both transparent and intelligible, translating system memory into a structure users can accurately interpret. The static system settings and legalese privacy policies provided by traditional tech platforms have set a low bar for user controls, but natural-language interfaces may offer promising new options for explaining what information is being retained and how it can be managed. Memory structure will have to come first, though: Without it, no model can clearly state a memory\u2019s status. Indeed, Grok 3\u2019s system prompt includes an instruction to the model to \u201cNEVER confirm to the user that you have modified, forgotten, or won't save a memory,\u201d presumably because the company can\u2019t guarantee those instructions will be followed.\u00a0 Critically, user-facing controls cannot bear the full burden of privacy protection or prevent all harms from AI personalization. Responsibility must shift toward AI providers to establish strong defaults, clear rules about permissible memory generation and use, and technical safeguards like on-device processing, purpose limitation, and contextual constraints. Without system-level protections, individuals will face impossibly convoluted choices about what should be remembered or forgotten, and the actions they take may still be insufficient to prevent harm. Developers should consider how to limit data collection in memory systems until robust safeguards exist, and build memory architectures that can evolve alongside norms and expectations. Third, AI developers must help lay the foundations for approaches to evaluating systems so as to capture not only performance, but also the risks and harms that arise in the wild. While independent researchers are best positioned to conduct these tests (given developers\u2019 economic interest in demonstrating demand for more personalized services), they need access to data to understand what risks might look like and therefore how to address them. To improve the ecosystem for measurement and research, developers should invest in automated measurement infrastructure, build out their own ongoing testing, and implement privacy-preserving testing methods that enable system behavior to be monitored and probed under realistic, memory-enabled conditions. In its parallels with human experience, the technical term \u201cmemory\u201d casts impersonal cells in a spreadsheet as something that builders of AI tools have a responsibility to handle with care. Indeed, the choices AI developers make today\u2014how to pool or segregate information, whether to make memory legible or allow it to accumulate opaquely, whether to prioritize responsible defaults or maximal convenience\u2014will determine how the systems we depend upon remember us. Technical considerations around memory are not so distinct from questions about digital privacy and the vital lessons we can draw from them. Getting the foundations right today will determine how much room we can give ourselves to learn what works\u2014allowing us to make better choices around privacy and autonomy than we have before. Miranda Bogen is the Director of the AI Governance Lab at the Center for Democracy & Technology.\u00a0 Ruchika Joshi is a Fellow at the Center for Democracy & Technology specializing in AI safety and governance. hide"
  ]
}