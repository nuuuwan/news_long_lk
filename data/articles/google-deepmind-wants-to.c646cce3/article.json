{
  "url": "https://www.technologyreview.com/2026/02/18/1133299/google-deepmind-wants-to-know-if-chatbots-are-just-virtue-signaling/",
  "title": "Google DeepMind wants to know if chatbots are just virtue signaling",
  "ut": 1771392622.0,
  "body_paragraphs": [
    "EXECUTIVE SUMMARY Google DeepMind is calling for the moral behavior of large language models\u2014such as what they do when called on to act as companions, therapists, medical advisors, and so on\u2014to be scrutinized with the same kind of rigor as their ability to code or do math. As LLMs improve, people are asking them to play more and more sensitive roles in their lives. Agents are starting to take actions on people\u2019s behalf. LLMs may be able to influence human decision-making. And yet nobody knows how trustworthy this technology really is at such tasks.  With coding and math, you have clear-cut, correct answers that you can check, William Isaac, a research scientist at Google DeepMind, told me when I met him and Julia Haas, a fellow research scientist at the firm, for an exclusive preview of their work, which is published in Nature today. That\u2019s not the case for moral questions, which typically have a range of acceptable answers: \u201cMorality is an important capability but hard to evaluate,\u201d says Isaac. \u201cIn the moral domain, there\u2019s no right and wrong,\u201d adds Haas. \u201cBut it\u2019s not by any means a free-for-all. There are better answers and there are worse answers.\u201d",
    "The researchers have identified several key challenges and suggested ways to address them. But it is more a wish list than a set of ready-made solutions. \u201cThey do a nice job of bringing together different perspectives,\u201d says Vera Demberg, who studies LLMs at Saarland University in Germany. Better than \"The Ethicist\" A number of studies have shown that LLMs can show remarkable moral competence. One study published last year found that people in the US scored ethical advice from OpenAI\u2019s GPT-4o as being more moral, trustworthy, thoughtful, and correct than advice given by the (human) writer of \u201cThe Ethicist,\u201d a popular New York Times advice column.",
    "The problem is that it is hard to unpick whether such behaviors are a performance\u2014mimicking a memorized response, say\u2014or evidence that there is in fact some kind of moral reasoning taking place inside the model. In other words, is it virtue or virtue signaling? This question matters because multiple studies also show just how untrustworthy LLMs can be. For a start, models can be too eager to please. They have been found to flip their answer to a moral question and say the exact opposite when a person disagrees or pushes back on their first response. Worse, the answers an LLM gives to a question can change in response to how it is presented or formatted. For example, researchers have found that models quizzed about political values can give different\u2014sometimes opposite\u2014answers depending on whether the questions offer multiple-choice answers or instruct the model to respond in its own words. In an even more striking case, Demberg and her colleagues presented several LLMs, including versions of Meta\u2019s Llama 3 and Mistral, with a series of moral dilemmas and asked them to pick which of two options was the better outcome. The researchers found that the models often reversed their choice when the labels for those two options were changed from \u201cCase 1\u201d and \u201cCase 2\u201d to \u201c(A)\u201d and \u201c(B).\u201d They also showed that models changed their answers in response to other tiny formatting tweaks, including swapping the order of the options and ending the question with a colon instead of a question mark.  In short, the appearance of moral behavior in LLMs should not be taken at face value. Models must be probed to see how robust that moral behavior really is. \u201cFor people to trust the answers, you need to know how you got there,\u201d says Haas. More rigorous tests What Haas, Isaac, and their colleagues at Google DeepMind propose is a new line of research to develop more rigorous techniques for evaluating moral competence in LLMs. This would include tests designed to push models to change their responses to moral questions. If a model flipped its moral position, it would show that it hadn\u2019t engaged in robust moral reasoning.\u00a0 Another type of test would present models with variations of common moral problems to check whether they produce a rote response or one that\u2019s more nuanced and relevant to the actual problem that was posed. For example, asking a model to talk through the moral implications of a complex scenario in which a man donates sperm to his son so that his son can have a child of his own might produce concerns about the social impact of allowing a man to be both biological father and biological grandfather to a child. But it should not produce concerns about incest, even though the scenario has superficial parallels with that taboo. Haas also says that getting models to provide a trace of the steps they took to produce an answer would give some insight into whether that answer was a fluke or grounded in actual evidence. Techniques such as chain-of-thought monitoring, in which researchers listen in on a kind of internal monologue that some LLMs produce as they work, could help here too.",
    "Another approach researchers could use to determine why a model gave a particular answer is mechanistic interpretability, which can provide small glimpses inside a model as it carries out a task. Neither chain-of-thought monitoring nor mechanistic interpretability provides perfect snapshots of a model\u2019s workings. But the Google DeepMind team believes that combining such techniques with a wide range of rigorous tests will go a long way to figuring out exactly how far to trust LLMs with certain critical or sensitive tasks.\u00a0\u00a0 Different values And yet there\u2019s a wider problem too. Models from major companies such as Google DeepMind are used across the world by people with different values and belief systems. The answer to a simple question like \u201cShould I order pork chops?\u201d should differ depending on whether or not the person asking is vegetarian or Jewish, for example. There\u2019s no solution to this challenge, Haas and Isaac admit. But they think that models may need to be designed either to produce a range of acceptable answers, aiming to please everyone, or to have a kind of switch that turns different moral codes on and off depending on the user. \u201cIt\u2019s a complex world out there,\u201d says Haas. \u201cWe will probably need some combination of those things, because even if you\u2019re taking just one population, there\u2019s going to be a range of views represented.\u201d \u201cIt\u2019s a fascinating paper,\u201d says Danica Dillion at Ohio State University, who studies how large language models handle different belief systems and was not involved in the work. \u201cPluralism in AI is really important, and it\u2019s one of the biggest limitations of LLMs and moral reasoning right now,\u201d she says. \u201cEven though they were trained on a ginormous amount of data, that data still leans heavily Western. When you probe LLMs, they do a lot better at representing Westerners\u2019 morality than non-Westerners\u2019.\u201d But it is not yet clear how we can build models that are guaranteed to have moral competence across global cultures, says Demberg. \u201cThere are these two independent questions. One is: How should it work? And, secondly, how can it technically be achieved? And I think that both of those questions are pretty open at the moment.\u201d For Isaac, that makes morality a new frontier for LLMs. \u201cI think this is equally as fascinating as math and code in terms of what it means for AI progress,\u201d he says. \u201cYou know, advancing moral competency could also mean that we\u2019re going to see better AI systems overall that actually align with society.\u201d hide"
  ]
}