{
  "url": "https://www.technologyreview.com/2025/06/11/1118233/amsterdam-fair-welfare-ai-discriminatory-algorithms-failure/",
  "title": "Inside Amsterdam\u2019s high-stakes experiment to create fair welfare AI",
  "ut": 1749598200.0,
  "body_paragraphs": [
    "This story is a partnership between MIT Technology Review, Lighthouse Reports, and Trouw, and was supported by the Pulitzer Center.\u00a0 Two futures Hans de Zwart, a gym teacher turned digital rights advocate, says that when he saw Amsterdam\u2019s plan to have an algorithm evaluate every welfare applicant in the city for potential fraud, he nearly fell out of his chair.\u00a0  It was February 2023, and de Zwart, who had served as the executive director of Bits of Freedom, the Netherlands\u2019 leading digital rights NGO, had been working as an informal advisor to Amsterdam\u2019s city government for nearly two years, reviewing and providing feedback on the AI systems it was developing.\u00a0 According to the city\u2019s documentation, this specific AI model\u2014referred to as \u201cSmart Check\u201d\u2014would consider submissions from potential welfare recipients and determine who might have submitted an incorrect application. More than any other project that had come across his desk, this one stood out immediately, he told us\u2014and not in a good way. \u201cThere\u2019s some very fundamental [and] unfixable problems,\u201d he says, in using this algorithm \u201con real people.\u201d AdvertisementRelated StoryCan you make AI fairer than a judge? Play our courtroom algorithm gameThe US criminal legal system uses predictive algorithms to try to make the judicial process less biased. But there\u2019s a deeper problem.",
    "From his vantage point behind the sweeping arc of glass windows at Amsterdam\u2019s city hall, Paul de Koning, a consultant to the city whose r\u00e9sum\u00e9 includes stops at various agencies in the Dutch welfare state, had viewed the same system with pride. De Koning, who managed Smart Check\u2019s pilot phase, was excited about what he saw as the project\u2019s potential to improve efficiency and remove bias from Amsterdam\u2019s social benefits system.\u00a0 A team of fraud investigators and data scientists had spent years working on Smart Check, and de Koning believed that promising early results had vindicated their approach. The city had consulted experts, run bias tests, implemented technical safeguards, and solicited feedback from the people who\u2019d be affected by the program\u2014more or less following every recommendation in the ethical-AI playbook. \u201cI got a good feeling,\u201d he told us.",
    "These opposing viewpoints epitomize a global debate about whether algorithms can ever be fair when tasked with making decisions that shape people\u2019s lives. Over the past several years of efforts to use artificial intelligence in this way, examples of collateral damage have mounted: nonwhite job applicants weeded out of job application pools in the US, families being wrongly flagged for child abuse investigations in Japan, and low-income residents being denied food subsidies in India.\u00a0 Proponents of these assessment systems argue that they can create more efficient public services by doing more with less and, in the case of welfare systems specifically, reclaim money that is allegedly being lost from the public purse. In practice, many were poorly designed from the start. They sometimes factor in personal characteristics in a way that leads to discrimination, and sometimes they have been deployed without testing for bias or effectiveness. In general, they offer few options for people to challenge\u2014or even understand\u2014the automated actions directly affecting how they live.",
    "The result has been more than a decade of scandals. In response, lawmakers, bureaucrats, and the private sector, from Amsterdam to New York, Seoul to Mexico City, have been trying to atone by creating algorithmic systems that integrate the principles of \u201cresponsible AI\u201d\u2014an approach that aims to guide AI development to benefit society while minimizing negative consequences.\u00a0  CHANTAL JAHCHAN   Developing and deploying ethical AI is a top priority for the European Union, and the same was true for the US under former president Joe Biden, who released a blueprint for an AI Bill of Rights. That plan was rescinded by the Trump administration, which has removed considerations of equity and fairness, including in technology, at the national level. Nevertheless, systems influenced by these principles are still being tested by leaders in countries, states, provinces, and cities\u2014in and out of the US\u2014that have immense power to make decisions like whom to hire, when to investigate cases of potential child abuse, and which residents should receive services first.\u00a0  Amsterdam indeed thought it was on the right track. City officials in the welfare department believed they could build technology that would prevent fraud while protecting citizens\u2019 rights. They followed these emerging best practices and invested a vast amount of time and money in a project that eventually processed live welfare applications. But in their pilot, they found that the system they\u2019d developed was still not fair and effective. Why?\u00a0 Lighthouse Reports, MIT Technology Review, and the Dutch newspaper Trouw have gained unprecedented access to the system to try to find out. In response to a public records request, the city disclosed multiple versions of the Smart Check algorithm and data on how it evaluated real-world welfare applicants, offering us unique insight into whether, under the best possible conditions, algorithmic systems can deliver on their ambitious promises.\u00a0\u00a0 The answer to that question is far from simple. For de Koning, Smart Check represented technological progress toward a fairer and more transparent welfare system. For de Zwart, it represented a substantial risk to welfare recipients\u2019 rights that no amount of technical tweaking could fix. As this algorithmic experiment unfolded over several years, it called into question the project\u2019s central premise: that responsible AI can be more than a thought experiment or corporate selling point\u2014and actually make algorithmic systems fair in the real world. A chance at redemption Understanding how Amsterdam found itself conducting a high-stakes endeavor with AI-driven fraud prevention requires going back four decades, to a national scandal around welfare investigations gone too far.",
    "Advertisement In 1984, Albine Grumb\u00f6ck, a divorced single mother of three, had been receiving welfare for several years when she learned that one of her neighbors, an employee at the social service\u2019s local office, had been secretly surveilling her life. He documented visits from a male friend, who in theory could have been contributing unreported income to the family. On the basis of his observations, the welfare office cut Grumb\u00f6ck\u2019s benefits. She fought the decision in court and won.  Albine Grumb\u00f6ck, whose benefits had been cut off, learns of the judgement for interim relief.ROB BOGAERTS/ NATIONAAL ARCHIEF   Despite her personal vindication, Dutch welfare policy has continued to empower welfare fraud investigators, sometimes referred to as \u201ctoothbrush counters,\u201d to turn over people\u2019s lives. This has helped create an atmosphere of suspicion that leads to problems for both sides, says Marc van Hoof, a lawyer who has helped Dutch welfare recipients navigate the system for decades: \u201cThe government doesn\u2019t trust its people, and the people don\u2019t trust the government.\u201d Harry Bodaar, a career civil servant, has observed the Netherlands\u2019 welfare policy up close throughout much of this time\u2014first as a social worker, then as a fraud investigator, and now as a welfare policy advisor for the city. The past 30 years have shown him that \u201cthe system is held together by rubber bands and staples,\u201d he says. \u201cAnd if you\u2019re at the bottom of that system, you\u2019re the first to fall through the cracks.\u201d Making the system work better for beneficiaries, he adds, was a large motivating factor when the city began designing Smart Check in 2019. \u201cWe wanted to do a fair check only on the people we [really] thought needed to be checked,\u201d Bodaar says\u2014in contrast to previous department policy, which until 2007 was to conduct home visits for every applicant.",
    "But he also knew that the Netherlands had become something of a ground zero for problematic welfare AI deployments. The Dutch government\u2019s attempts to modernize fraud detection through AI had backfired on a few notorious occasions. In 2019, it was revealed that the national government had been using an algorithm to create risk profiles that it hoped would help spot fraud in the child care benefits system. The resulting scandal saw nearly 35,000 parents, most of whom were migrants or the children of migrants, wrongly accused of defrauding the assistance system over six years. It put families in debt, pushed some into poverty, and ultimately led the entire government to resign in 2021.\u00a0\u00a0  COURTESY OF TROUW   In Rotterdam, a 2023 investigation by Lighthouse Reports into a system for detecting welfare fraud found it to be biased against women, parents, non-native Dutch speakers, and other vulnerable groups, eventually forcing the city to suspend use of the system. Other cities, like Amsterdam and Leiden, used a system called the Fraud Scorecard, which was first deployed more than 20 years ago and included education, neighborhood, parenthood, and gender as crude risk factors to assess welfare applicants; that program was also discontinued. The Netherlands is not alone. In the United States, there have been at least 11 cases in which state governments used algorithms to help disperse public benefits, according to the nonprofit Benefits Tech Advocacy Hub, often with troubling results. Michigan, for instance, falsely accused 40,000 people of committing unemployment fraud. And in France, campaigners are taking the national welfare authority to court over an algorithm they claim discriminates against low-income applicants and people with disabilities.",
    "This string of scandals, as well as a growing awareness of how racial discrimination can be embedded in algorithmic systems, helped fuel the growing emphasis on responsible AI. It\u2019s become \u201cthis umbrella term to say that we need to think about not just ethics, but also fairness,\u201d says Jiahao Chen, an ethical-AI consultant who has provided auditing services to both private and local government entities. \u201cI think we are seeing that realization that we need things like transparency and privacy, security and safety, and so on.\u201d\u00a0 The approach, based on a set of tools intended to rein in the harms caused by the proliferating technology, has given rise to a rapidly growing field built upon a familiar formula: white papers and frameworks from think tanks and international bodies, and a lucrative consulting industry made up of traditional power players like the Big 5 consultancies, as well as a host of startups and nonprofits. In 2019, for instance, the Organisation for Economic Co-operation and Development, a global economic policy body, published its Principles on Artificial Intelligence as a guide for the development of \u201ctrustworthy AI.\u201d Those principles include building explainable systems, consulting public stakeholders, and conducting audits.\u00a0 Advertisement But the legacy left by decades of algorithmic misconduct has proved hard to shake off, and there is little agreement on where to draw the line between what is fair and what is not. While the Netherlands works to institute reforms shaped by responsible AI at the national level, Algorithm Audit, a Dutch NGO that has provided ethical-AI auditing services to government ministries, has concluded that the technology should be used to profile welfare recipients only under strictly defined conditions, and only if systems avoid taking into account protected characteristics like gender. Meanwhile, Amnesty International, digital rights advocates like de Zwart, and some welfare recipients themselves argue that when it comes to making decisions about people\u2019s lives, as in the case of social services, the public sector should not be using AI at all. Amsterdam hoped it had found the right balance. \u201cWe\u2019ve learned from the things that happened before us,\u201d says Bodaar, the policy advisor, of the past scandals. And this time around, the city wanted to build a system that would \u201cshow the people in Amsterdam we do good and we do fair.\u201d  Finding a better way  Every time an Amsterdam resident applies for benefits, a caseworker reviews the application for irregularities. If an application looks suspicious, it can be sent to the city\u2019s investigations department\u2014which could lead to a rejection, a request to correct paperwork errors, or a recommendation that the candidate receive less money. Investigations can also happen later, once benefits have been dispersed; the outcome may force recipients to pay back funds, and even push some into debt. Officials have broad authority over both applicants and existing welfare recipients. They can request bank records, summon beneficiaries to city hall, and in some cases make unannounced visits to a person\u2019s home. As investigations are carried out\u2014or paperwork errors fixed\u2014much-needed payments may be delayed. And often\u2014in more than half of the investigations of applications, according to figures provided by Bodaar\u2014the city finds no evidence of wrongdoing. In those cases, this can mean that the city has \u201cwrongly harassed people,\u201d Bodaar says.\u00a0   The Smart Check system was designed to avoid these scenarios by eventually replacing the initial caseworker who flags which cases to send to the investigations department. The algorithm would screen the applications to identify those most likely to involve major errors, based on certain personal characteristics, and redirect those cases for further scrutiny by the enforcement team. If all went well, the city wrote in its internal documentation, the system would improve on the performance of its human caseworkers, flagging fewer welfare applicants for investigation while identifying a greater proportion of cases with errors. In one document, the city projected that the model would prevent up to 125 individual Amsterdammers from facing debt collection and save \u20ac2.4 million annually.",
    "Smart Check was an exciting prospect for city officials like de Koning, who would manage the project when it was deployed. He was optimistic, since the city was taking a scientific approach, he says; it would \u201csee if it was going to work\u201d instead of taking the attitude that \u201cthis must work, and no matter what, we will continue this.\u201d It was the kind of bold idea that attracted optimistic techies like Loek Berkers, a data scientist who worked on Smart Check in only his second job out of college. Speaking in a cafe tucked behind Amsterdam\u2019s city hall, Berkers remembers being impressed at his first contact with the system: \u201cEspecially for a project within the municipality,\u201d he says, it \u201cwas very much a sort of innovative project that was trying something new.\u201d",
    "Smart Check made use of an algorithm called an \u201cexplainable boosting machine,\u201d which allows people to more easily understand how AI models produce their predictions. Most other machine-learning models are often regarded as \u201cblack boxes\u201d running abstract mathematical processes that are hard to understand for both the employees tasked with using them and the people affected by the results.\u00a0 The Smart Check model would consider 15 characteristics\u2014including whether applicants had previously applied for or received benefits, the sum of their assets, and the number of addresses they had on file\u2014to assign a risk score to each person. It purposefully avoided demographic factors, such as gender, nationality, or age, that were thought to lead to bias. It also tried to avoid \u201cproxy\u201d factors\u2014like postal codes\u2014that may not look sensitive on the surface but can become so if, for example, a postal code is statistically associated with a particular ethnic group. In an unusual step, the city has disclosed this information and shared multiple versions of the Smart Check model with us, effectively inviting outside scrutiny into the system\u2019s design and function. With this data, we were able to build a hypothetical welfare recipient to get insight into how an individual applicant would be evaluated by Smart Check.\u00a0\u00a0  This model was trained on a data set encompassing 3,400 previous investigations of welfare recipients. The idea was that it would use the outcomes from these investigations, carried out by city employees, to figure out which factors in the initial applications were correlated with potential fraud.\u00a0 Advertisement But using past investigations introduces potential problems from the start, says Sennay Ghebreab, scientific director of the Civic AI Lab (CAIL) at the University of Amsterdam, one of the external groups that the city says it consulted with. The problem of using historical data to build the models, he says, is that \u201cwe will end up [with] historic biases.\u201d For example, if caseworkers historically made higher rates of mistakes with a specific ethnic group, the model could wrongly learn to predict that this ethnic group commits fraud at higher rates.\u00a0 The city decided it would rigorously audit its system to try to catch such biases against vulnerable groups. But how bias should be defined, and hence what it actually means for an algorithm to be fair, is a matter of fierce debate. Over the past decade, academics have proposed dozens of competing mathematical notions of fairness, some of which are incompatible. This means that a system designed to be \u201cfair\u201d according to one such standard will inevitably violate others. Amsterdam officials adopted a definition of fairness that focused on equally distributing the burden of wrongful investigations across different demographic groups.\u00a0  In other words, they hoped this approach would ensure that welfare applicants of different backgrounds would carry the same burden of being incorrectly investigated at similar rates.",
    "Mixed feedback As it built Smart Check, Amsterdam consulted various public bodies about the model, including the city\u2019s internal data protection officer and the Amsterdam Personal Data Commission. It also consulted private organizations, including the consulting firm Deloitte. Each gave the project its approval.\u00a0 But one key group was not on board: the Participation Council, a 15-member advisory committee composed of benefits recipients, advocates, and other nongovernmental stakeholders who represent the interests of the people the system was designed to help\u2014and to scrutinize. The committee, like de Zwart, the digital rights advocate, was deeply troubled by what the system could mean for individuals already in precarious positions.\u00a0 Anke van der Vliet, now in her 70s, is one longtime member of the council. After she\u00a0sinks slowly from her walker into a seat at a restaurant in Amsterdam\u2019s Zuid neighborhood, where she lives, she retrieves her reading glasses from their case. \u201cWe distrusted it from the start,\u201d she says, pulling out a stack of papers she\u2019s saved on Smart Check. \u201cEveryone was against it.\u201d For decades, she has been a steadfast advocate for the city\u2019s welfare recipients\u2014a group that, by the end of 2024, numbered around 35,000. In the late 1970s, she helped found Women on Welfare, a group dedicated to exposing the unique challenges faced by women within the welfare system.  City employees first presented their plan to the Participation Council in the fall of 2021. Members like van der Vliet were deeply skeptical. \u201cWe wanted to know, is it to my advantage or disadvantage?\u201d she says.\u00a0 Two more meetings could not convince them. Their feedback did lead to key changes\u2014including reducing the number of variables the city had initially considered to calculate an applicant\u2019s score and excluding variables that could introduce bias, such as age, from the system. But the Participation Council stopped engaging with the city\u2019s development efforts altogether after six months. \u201cThe Council is of the opinion that such an experiment affects the fundamental rights of citizens and should be discontinued,\u201d the group wrote in March 2022. Since only around 3% of welfare benefit applications are fraudulent, the letter continued, using the algorithm was \u201cdisproportionate.\u201d Advertisement De Koning, the project manager, is skeptical that the system would ever have received the approval of van der Vliet and her colleagues. \u201cI think it was never going to work that the whole Participation Council was going to stand behind the Smart Check idea,\u201d he says. \u201cThere was too much emotion in that group about the whole process of the social benefit system.\u201d He adds, \u201cThey were very scared there was going to be another scandal.\u201d\u00a0 But for advocates working with welfare beneficiaries, and for some of the beneficiaries themselves, the worry wasn\u2019t a scandal but the prospect of real harm. The technology could not only make damaging errors but leave them even more difficult to correct\u2014allowing welfare officers to \u201chide themselves behind digital walls,\u201d says Henk Kroon, an advocate who assists welfare beneficiaries at the Amsterdam Welfare Association, a union established in the 1970s. Such a system could make work \u201ceasy for [officials],\u201d he says. \u201cBut for the common citizens, it\u2019s very often the problem.\u201d\u00a0 Time to test\u00a0 Despite the Participation Council\u2019s ultimate objections, the city decided to push forward and put the working Smart Check model to the test.\u00a0 The first results were not what they\u2019d hoped for. When the city\u2019s advanced analytics team ran the initial model in May 2022, they found that the algorithm showed heavy bias against migrants and men, which we were able to independently verify.\u00a0 As the city told us and as our analysis confirmed, the initial model was more likely to wrongly flag non-Dutch applicants. And it was nearly twice as likely to wrongly flag an applicant with a non-Western nationality than one with a Western nationality. The model was also 14% more likely to wrongly flag men for investigation.\u00a0 In the process of training the model, the city also collected data on who its human case workers had flagged for investigation and which groups the wrongly flagged people were more likely to belong to. In essence, they ran a bias test on their own analog system\u2014an important way to benchmark that is rarely done before deploying such systems.\u00a0  What they found in the process led by caseworkers was a strikingly different pattern. Whereas the Smart Check model was more likely to wrongly flag non-Dutch nationals and men, human caseworkers were more likely to wrongly flag Dutch nationals and women.\u00a0  The team behind Smart Check knew that if they couldn\u2019t correct for bias, the project would be canceled. So they turned to a technique from academic research, known as training-data reweighting. In practice, that meant applicants with a non-Western nationality who were deemed to have made meaningful errors in their applications were given less weight in the data, while those with a Western nationality were given more.  Eventually, this appeared to solve their problem: As Lighthouse\u2019s analysis confirms, once the model was reweighted, Dutch and non-Dutch nationals were equally likely to be wrongly flagged.\u00a0 De Koning, who joined the Smart Check team after the data was reweighted, said the results were a positive sign: \u201cBecause it was fair \u2026 we could continue the process.\u201d\u00a0 The model also appeared to be better than caseworkers at identifying applications worthy of extra scrutiny, with internal testing showing a 20% improvement in accuracy.   Buoyed by these results, in the spring of 2023, the city was almost ready to go public. It submitted Smart Check to the Algorithm Register, a government-run transparency initiative meant to keep citizens informed about machine-learning algorithms either in development or already in use by the government. For de Koning, the city\u2019s extensive assessments and consultations were encouraging, particularly since they also revealed the biases in the analog system. But for de Zwart, those same processes represented a profound misunderstanding: that fairness could be engineered.\u00a0 Advertisement In a letter to city officials, de Zwart criticized the premise of the project and, more specifically, outlined the unintended consequences that could result from reweighting the data. It might reduce bias against people with a migration background overall, but it wouldn\u2019t guarantee fairness across intersecting identities; the model could still discriminate against women with a migration background, for instance. And even if that issue were addressed, he argued, the model might still treat migrant women in certain postal codes unfairly, and so on. And such biases would be hard to detect. \u201cThe city has used all the tools in the responsible-AI tool kit,\u201d de Zwart told us. \u201cThey have a bias test, a human rights assessment; [they have] taken into account automation bias\u2014in short, everything that the responsible-AI world recommends. Nevertheless, the municipality has continued with something that is fundamentally a bad idea.\u201d Ultimately, he told us, it\u2019s a question of whether it\u2019s legitimate to use data on past behavior to judge \u201cfuture behavior of your citizens that fundamentally you cannot predict.\u201d\u00a0 Officials still pressed on\u2014and set March 2023 as the date for the pilot to begin. Members of Amsterdam\u2019s city council were given little warning. In fact, they were only informed the same month\u2014to the disappointment of Elisabeth IJmker, a first-term council member from the Green Party, who balanced her role in municipal government with research on religion and values at Amsterdam\u2019s Vrije University.\u00a0   \u201cReading the words \u2018algorithm\u2019 and \u2018fraud prevention\u2019 in one sentence, I think that\u2019s worth a discussion,\u201d she told us. But by the time that she learned about the project, the city had already been working on it for years. As far as she was concerned, it was clear that the city council was \u201cbeing informed\u201d rather than being asked to vote on the system.\u00a0 The city hoped the pilot could prove skeptics like her wrong.  Upping the stakes The formal launch of Smart Check started with a limited set of actual welfare applicants, whose paperwork the city would run through the algorithm and assign a risk score to determine whether the application should be flagged for investigation. At the same time, a human would review the same application.\u00a0 Smart Check\u2019s performance would be monitored on two key criteria. First, could it consider applicants without bias? And second, was Smart Check actually smart? In other words, could the complex math that made up the algorithm actually detect welfare fraud better and more fairly than human caseworkers?\u00a0 It didn\u2019t take long to become clear that the model fell short on both fronts.\u00a0 While it had been designed to reduce the number of welfare applicants flagged for investigation, it was flagging more. And it proved no better than a human caseworker at identifying those that actually warranted extra scrutiny.\u00a0 What\u2019s more, despite the lengths the city had gone to in order to recalibrate the system, bias reemerged in the live pilot. But this time, instead of wrongly flagging non-Dutch people and men as in the initial tests, the model was now more likely to wrongly flag applicants with Dutch nationality and women.\u00a0  Lighthouse\u2019s own analysis also revealed other forms of bias unmentioned in the city\u2019s documentation, including a greater likelihood that welfare applicants with children would be wrongly flagged for investigation.  Advertisement (A spokesperson for Amsterdam sent comment after publication to note that \u201cwhen conducting the bias analysis, [it] did not look at whether a benefit applicant had children or not.\u201d The spokesperson also added that the city's \u201cwelfare policy has been at the forefront for years when it comes to trust and not sanctioning people who have made a mistake. Various policy changes have been implemented and arrangements made for this\u201d; but they note the city has also hit \u201cthe limits of the local policy space that municipalities have\u201d since \u201cthe national welfare system creates distrust and obliges municipalities to punish.\u201d) The city was stuck. Nearly 1,600 welfare applications had been run through the model during the pilot period. But the results meant that members of the team were uncomfortable continuing to test\u2014especially when there could be genuine consequences. In short, de Koning says, the city could not \u201cdefinitely\u201d say that \u201cthis is not discriminating.\u201d\u00a0  He, and others working on the project, did not believe this was necessarily a reason to scrap Smart Check. They wanted more time\u2014say, \u201ca period of 12 months,\u201d according to de Koning\u2014to continue testing and refining the model.\u00a0 They knew, however, that would be a hard sell.\u00a0 In late November 2023, Rutger Groot Wassink\u2014the city official in charge of social affairs\u2014took his seat in the Amsterdam council chamber. He glanced at the tablet in front of him and then addressed the room: \u201cI have decided to stop the pilot.\u201d The announcement brought an end to the sweeping multiyear experiment. In another council meeting a few months later, he explained why the project was terminated: \u201cI would have found it very difficult to justify, if we were to come up with a pilot \u2026 that showed the algorithm contained enormous bias,\u201d he said. \u201cThere would have been parties who would have rightly criticized me about that.\u201d\u00a0 Viewed in a certain light, the city had tested out an innovative approach to identifying fraud in a way designed to minimize risks, found that it had not lived up to its promise, and scrapped it before the consequences for real people had a chance to multiply.\u00a0 But for IJmker and some of her city council colleagues focused on social welfare, there was also the question of opportunity cost. She recalls speaking with a colleague about how else the city could\u2019ve spent that money\u2014like to \u201chire some more people to do personal contact with the different people that we\u2019re trying to reach.\u201d\u00a0 Related StoryResponsible AI has a burnout problemCompanies say they want ethical AI. But those working in the field say that ambition comes at their expense.",
    "City council members were never told exactly how much the effort cost, but in response to questions from MIT Technology Review, Lighthouse, and Trouw on this topic, the city estimated that it had spent some \u20ac500,000, plus \u20ac35,000 for the contract with Deloitte\u2014but cautioned that the total amount put into the project was only an estimate, given that Smart Check was developed in house by various existing teams and staff members.\u00a0 For her part, van der Vliet, the Participation Council member, was not surprised by the poor result. The possibility of a discriminatory computer system was \u201cprecisely one of the reasons\u201d her group hadn\u2019t wanted the pilot, she says. And as for the discrimination in the existing system? \u201cYes,\u201d she says, bluntly. \u201cBut we have always said that [it was discriminatory].\u201d\u00a0 Advertisement She and other advocates wished that the city had focused more on what they saw as the real problems facing welfare recipients: increases in the cost of living that have not, typically, been followed by increases in benefits; the need to document every change that could potentially affect their benefits eligibility; and the distrust with which they feel they are treated by the municipality.\u00a0 Can this kind of algorithm ever be done right? When we spoke to Bodaar in March, a year and a half after the end of the pilot, he was candid in his reflections. \u201cPerhaps it was unfortunate to immediately use one of the most complicated systems,\u201d he said, \u201cand perhaps it is also simply the case that it is not yet \u2026 the time to use artificial intelligence for this goal.\u201d \u201cNiente, zero, nada. We\u2019re not going to do that anymore,\u201d he said about using AI to evaluate welfare applicants. \u201cBut we\u2019re still thinking about this: What exactly have we learned?\u201d That is a question that IJmker thinks about too. In city council meetings she has brought up Smart Check as an example of what not to do. While she was glad that city employees had been thoughtful in their \u201cmany protocols,\u201d she worried that the process obscured some of the larger questions of \u201cphilosophical\u201d and \u201cpolitical values\u201d that the city had yet to weigh in on as a matter of policy.\u00a0 Questions such as \u201cHow do we actually look at profiling?\u201d or \u201cWhat do we think is justified?\u201d\u2014or even \u201cWhat is bias?\u201d\u00a0 These questions are, \u201cwhere politics comes in, or ethics,\u201d she says, \u201cand that\u2019s something you cannot put into a checkbox.\u201d But now that the pilot has stopped, she worries that her fellow city officials might be too eager to move on. \u201cI think a lot of people were just like, \u2018Okay, well, we did this. We're done, bye, end of story,\u2019\u201d she says. It feels like \u201ca waste,\u201d she adds, \u201cbecause people worked on this for years.\u201d  CHANTAL JAHCHAN   In abandoning the model, the city has returned to an analog process that its own analysis concluded was biased against women and Dutch nationals\u2014a fact not lost on Berkers, the data scientist, who no longer works for the city. By shutting down the pilot, he says, the city sidestepped the uncomfortable truth\u2014that many of the concerns de Zwart raised about the complex, layered biases within the Smart Check model also apply to the caseworker-led process.  \u201cThat\u2019s the thing that I find a bit difficult about the decision,\u201d Berkers says. \u201cIt\u2019s a bit like no decision. It is a decision to go back to the analog process, which in itself has characteristics like bias.\u201d\u00a0 Chen, the ethical-AI consultant, largely agrees. \u201cWhy do we hold AI systems to a higher standard than human agents?\u201d he asks. When it comes to the caseworkers, he says, \u201cthere was no attempt to correct [the bias] systematically.\u201d Amsterdam has promised to write a report on human biases in the welfare process, but the date has been pushed back several times. Advertisement \u201cIn reality, what ethics comes down to in practice is: nothing\u2019s perfect,\u201d he says. \u201cThere\u2019s a high-level thing of Do not discriminate, which I think we can all agree on, but this example highlights some of the complexities of how you translate that [principle].\u201d Ultimately, Chen believes that finding any solution will require trial and error, which by definition usually involves mistakes: \u201cYou have to pay that cost.\u201d But it may be time to more fundamentally reconsider how fairness should be defined\u2014and by whom. Beyond the mathematical definitions, some researchers argue that the people most affected by the programs in question should have a greater say. \u201cSuch systems only work when people buy into them,\u201d explains Elissa Redmiles, an assistant professor of computer science at Georgetown University who has studied algorithmic fairness.\u00a0 No matter what the process looks like, these are questions that every government will have to deal with\u2014and urgently\u2014in a future increasingly defined by AI.\u00a0 And, as de Zwart argues, if broader questions are not tackled, even well-intentioned officials deploying systems like Smart Check in cities like Amsterdam will be condemned to learn\u2014or ignore\u2014the same lessons over and over.\u00a0 \u201cWe are being seduced by technological solutions for the wrong problems,\u201d he says. \u201cShould we really want this? Why doesn\u2019t the municipality build an algorithm that searches for people who do not apply for social assistance but are entitled to it?\u201d This piece has been updated to include further comment from Amsterdam officials.   Eileen Guo is the senior reporter for features and investigations at\u00a0MIT Technology Review. Gabriel Geiger is an investigative reporter at Lighthouse Reports. Justin-Casimir Braun is a data reporter at Lighthouse Reports. Additional reporting by Jeroen van Raalte for Trouw, Melissa Heikkil\u00e4 for MIT Technology Review, and Tahmeed Shafiq for Lighthouse Reports. Fact checked by Alice Milliken.\u00a0 You can read a detailed explanation of our technical methodology here. You can read Trouw's companion story, in Dutch, here."
  ]
}