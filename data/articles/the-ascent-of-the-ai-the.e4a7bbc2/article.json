{
  "url": "https://www.technologyreview.com/2025/12/30/1129392/book-reviews-ai-therapy-mental-health/",
  "title": "The ascent of the AI therapist",
  "ut": 1767054600.0,
  "body_paragraphs": [
    "We\u2019re in the midst of a global mental-\u00adhealth crisis. More than a billion people worldwide suffer from a mental-health condition, according to the World Health Organization. The prevalence of anxiety and depression is growing in many demographics, particularly young people, and suicide is claiming hundreds of thousands of lives globally each year. Given the clear demand for accessible and affordable mental-health services, it\u2019s no wonder that people have looked to artificial intelligence for possible relief. Millions are already actively seeking therapy from popular chatbots like OpenAI\u2019s ChatGPT and Anthropic\u2019s Claude, or from specialized psychology apps like Wysa and Woebot. On a broader scale, researchers are exploring AI\u2019s potential to monitor and collect behavioral and biometric observations using wearables and smart devices, analyze vast volumes of clinical data for new insights, and assist human mental-health professionals to help prevent burnout.\u00a0 Related StoryTherapists are secretly using ChatGPT. Clients are triggered.Read next But so far this largely uncontrolled experiment has produced mixed results. Many people have found solace in chatbots based on large language models (LLMs), and some experts see promise in them as therapists, but other users have been sent into delusional spirals by AI\u2019s hallucinatory whims and breathless sycophancy. Most tragically, multiple families have alleged that chatbots contributed to the suicides of their loved ones, sparking lawsuits against companies responsible for these tools. In October, OpenAI CEO Sam Altman revealed in a blog post that 0.15% of ChatGPT users \u201chave conversations that include explicit indicators of potential suicidal planning or intent.\u201d That\u2019s roughly a million people sharing suicidal ideations with just one of these software systems every week. The real-world consequences of AI therapy came to a head in unexpected ways in 2025 as we waded through a critical mass of stories about human-chatbot relationships, the flimsiness of guardrails on many LLMs, and the risks of sharing profoundly personal information with products made by corporations that have economic incentives to harvest and monetize such sensitive data.",
    "Several authors anticipated this inflection point. Their timely books are a reminder that while the present feels like a blur of breakthroughs, scandals, and confusion, this disorienting time is rooted in deeper histories of care, technology, and trust.\u00a0 LLMs have often been described as \u201cblack boxes\u201d because nobody knows exactly how they produce their results. The inner workings that guide their outputs are opaque because their algorithms are so complex and their training data is so vast. In mental-health circles, people often describe the human brain as a \u201cblack box,\u201d for analogous reasons. Psychology, psychiatry, and related fields must grapple with the impossibility of seeing clearly inside someone else\u2019s head, let alone pinpointing the exact causes of their distress.",
    "These two types of black boxes are now interacting with each other, creating unpredictable feedback loops that may further impede clarity about the origins of people\u2019s mental-\u00adhealth struggles and the solutions that may be possible. Anxiety about these developments has much to do with the explosive recent advances in AI, but it also revives decades-old warnings from pioneers such as the MIT computer scientist Joseph Weizenbaum, who argued against computerized therapy as early as the 1960s.\u00a0\u00a0   Dr. Bot: Why Doctors Can Fail Us\u2014 andHow AI Could Save LivesCharlotte BleaseYALE UNIVERSITY PRESS, 2025   Charlotte Blease, a philosopher of medicine, makes the optimist\u2019s case in Dr. Bot: Why Doctors Can Fail Us\u2014and How AI Could Save Lives. Her book broadly explores the possible positive impacts of AI in a range of medical fields. While she remains clear-eyed about the risks, warning that readers who are expecting \u201ca gushing love letter to technology\u201d will be disappointed, she suggests that these models can help relieve patient suffering and medical burnout alike. \u201cHealth systems are crumbling under patient pressure,\u201d Blease writes. \u201cGreater burdens on fewer doctors create the perfect petri dish for errors,\u201d and \u201cwith palpable shortages of doctors and increasing waiting times for patients, many of us are profoundly frustrated.\u201d Blease believes that AI can not only ease medical professionals\u2019 massive workloads but also relieve the tensions that have always existed between some patients and their caregivers. For example, people often don\u2019t seek needed care because they are intimidated or fear judgment from medical professionals; this is especially true if they have mental-health challenges. AI could allow more people to share their concerns, she argues.\u00a0 Related StoryIt's surprisingly easy to stumble into a relationship with an AI chatbotRead next But she\u2019s aware that these putative upsides need to be weighed against major drawbacks. For instance, AI therapists can provide inconsistent and even dangerous responses to human users, according to a 2025 study, and they also raise privacy concerns, given that AI companies are currently not bound by the same confidentiality and HIPAA standards as licensed therapists.\u00a0 While Blease is an expert in this field, her motivation for writing the book is also personal: She has two siblings with an incurable form of muscular dystrophy, one of whom waited decades for a diagnosis. During the writing of her book, she also lost her partner to cancer and her father to dementia within a devastating six-month period.\u00a0\u201cI witnessed first-hand the sheer brilliance of doctors and the kindness of health professionals,\u201d she writes. \u201cBut I also observed how things can go wrong with care.\u201d   The Silicon Shrink: How Artificial Intelligence Made the World an AsylumDaniel OberhausMIT PRESS, 2025   A similar tension animates Daniel Oberhaus\u2019s engrossing book The Silicon Shrink: How Artificial Intelligence Made the World an Asylum. Oberhaus starts from a point of tragedy: the loss of his younger sister to suicide. As Oberhaus carried out the \u201cdistinctly twenty-first-century mourning process\u201d of sifting through her digital remains, he wondered if technology could have eased the burden of the psychiatric problems that had plagued her since childhood. \u201cIt seemed possible that all of this personal data might have held important clues that her mental health providers could have used to provide more effective treatment,\u201d he writes. \u201cWhat if algorithms running on my sister\u2019s smartphone or laptop had used that data to understand when she was in distress? Could it have led to a timely intervention that saved her life? Would she have wanted that even if it did?\u201d",
    "This concept of digital phenotyping\u2014in which a person\u2019s digital behavior could be mined for clues about distress or illness\u2014seems elegant in theory. But it may also become problematic if integrated into the field of psychiatric artificial intelligence (PAI), which extends well beyond chatbot therapy. Oberhaus emphasizes that digital clues could actually exacerbate the existing challenges of modern psychiatry, a discipline that remains fundamentally uncertain about the underlying causes of mental illnesses and disorders. The advent of PAI, he says, is \u201cthe logical equivalent of grafting physics onto astrology.\u201d In other words, the data generated by digital phenotyping is as precise as physical measurements of planetary positions, but it is then integrated into a broader framework\u2014in this case, psychiatry\u2014that, like astrology, is based on unreliable assumptions.\u00a0\u00a0 Oberhaus, who uses the phrase \u201cswipe psychiatry\u201d to describe the outsourcing of clinical decisions based on behavioral data to LLMs, thinks that this approach cannot escape the fundamental issues facing psychiatry. In fact, it could worsen the problem by causing the skills and judgment of human therapists to atrophy as they grow more dependent on AI systems.\u00a0 Related StoryHelp! My therapist is secretly using ChatGPTRead next He also uses the asylums of the past\u2014in which institutionalized patients lost their right to freedom, privacy, dignity, and agency over their lives\u2014as a touchstone for a more insidious digital captivity that may spring from PAI. LLM users are already sacrificing privacy by telling chatbots sensitive personal information that companies then mine and monetize, contributing to a new surveillance economy. Freedom and dignity are at stake when complex inner lives are transformed into data streams tailored for AI analysis.\u00a0  AI therapists could flatten humanity into patterns of prediction, and so sacrifice the intimate, individualized care that is expected of traditional human therapists. \u201cThe logic of PAI leads to a future where we may all find ourselves patients in an algorithmic asylum administered by digital wardens,\u201d Oberhaus writes. \u201cIn the algorithmic asylum there is no need for bars on the window or white padded rooms because there is no possibility of escape. The asylum is already everywhere\u2014in your homes and offices, schools and hospitals, courtrooms and barracks. Wherever there\u2019s an internet connection, the asylum is waiting.\u201d   Chatbot Therapy:A Critical Analysis ofAI Mental Health TreatmentEoin FullamROUTLEDGE, 2025   Eoin Fullam, a researcher who studies the intersection of technology and mental health, echoes some of the same concerns in Chatbot Therapy: A Critical Analysis of AI Mental Health Treatment. A heady academic primer, the book analyzes the assumptions underlying the automated treatments offered by AI chatbots and the way capitalist incentives could corrupt these kinds of tools.\u00a0\u00a0 Fullam observes that the capitalist mentality behind new technologies \u201coften leads to questionable, illegitimate, and illegal business practices in which the customers\u2019 interests are secondary to strategies of market dominance.\u201d That doesn\u2019t mean that therapy-bot makers \u201cwill inevitably conduct nefarious activities contrary to the users\u2019 interests in the pursuit of market dominance,\u201d Fullam writes.",
    "But he notes that the success of AI therapy depends on the inseparable impulses to make money and to heal people. In this logic, exploitation and therapy feed each other: Every digital therapy session generates data, and that data fuels the system that profits as unpaid users seek care. The more effective the therapy seems, the more the cycle entrenches itself, making it harder to distinguish between care and commodification. \u201cThe more the users benefit from the app in terms of its therapeutic or any other mental health intervention,\u201d he writes, \u201cthe more they undergo exploitation.\u201d\u00a0  This sense of an economic and psychological ouroboros\u2014the snake that eats its own tail\u2014serves as a central metaphor in Sike, the debut novel from Fred Lunzer, an author with a research background in AI.",
    "Described as a \u201cstory of boy meets girl meets AI psychotherapist,\u201d Sike follows Adrian, a young Londoner who makes a living ghostwriting rap lyrics, in his romance with Maquie, a business professional with a knack for spotting lucrative technologies in the beta phase.\u00a0  SikeFred LunzerCELADON BOOKS, 2025   The title refers to a splashy commercial AI therapist called Sike, uploaded into smart glasses, that Adrian uses to interrogate his myriad anxieties. \u201cWhen I signed up to Sike, we set up my dashboard, a wide black panel like an airplane\u2019s cockpit that showed my daily \u2018vitals,\u2019\u201d Adrian narrates. \u201cSike can analyze the way you walk, the way you make eye contact, the stuff you talk about, the stuff you wear, how often you piss, shit, laugh, cry, kiss, lie, whine, and cough.\u201d In other words, Sike is the ultimate digital phenotyper, constantly and exhaustively analyzing everything in a user\u2019s daily experiences. In a twist, Lunzer chooses to make Sike a luxury product, available only to subscribers who can foot the price tag of \u00a32,000 per month.\u00a0 Flush with cash from his contributions to a hit song, Adrian comes to rely on Sike as a trusted mediator between his inner and outer worlds. The novel explores the impacts of the app on the wellness of the well-off, following rich people who voluntarily commit themselves to a boutique version of the digital asylum described by Oberhaus. The only real sense of danger in Sike involves a Japanese torture egg (don\u2019t ask). The novel strangely sidesteps the broader dystopian ripples of its subject matter in favor of drunken conversations at fancy restaurants and elite dinner parties.\u00a0  The sudden ascent of the AI therapist seems startlingly futuristic, as if it should be unfolding in some later time when the streets scrub themselves and we travel the world through pneumatic tubes.  Sike\u2019s creator is simply \u201ca great guy\u201d in Adrian\u2019s estimation, despite his techno-messianic vision of training the app to soothe the ills of entire nations. It always seems as if a shoe is meant to drop, but in the end, it never does, leaving the reader with a sense of non-resolution.",
    "While Sike is set in the present day, something about the sudden ascent of the AI therapist\u2014\u00adin real life as well as in fiction\u2014seems startlingly futuristic, as if it should be unfolding in some later time when the streets scrub themselves and we travel the world through pneumatic tubes. But this convergence of mental health and artificial intelligence has been in the making for more than half a century. The beloved astronomer Carl Sagan, for example, once imagined a \u201cnetwork of computer psychotherapeutic terminals, something like arrays of large telephone booths\u201d that could address the growing demand for mental-health services. Oberhaus notes that one of the first incarnations of a trainable neural network, known as the Perceptron, was devised not by a mathematician but by a psychologist named Frank Rosenblatt, at the Cornell Aeronautical Laboratory in 1958. The potential utility of AI in mental health was widely recognized by the 1960s, inspiring early computerized psychotherapists such as the DOCTOR script that ran on the ELIZA chatbot developed by Joseph Weizenbaum, who shows up in all three of the nonfiction books in this article. Weizenbaum, who died in 2008, was profoundly concerned about the possibility of computerized therapy. \u201cComputers can make psychiatric judgments,\u201d he wrote in his 1976 book Computer Power and Human Reason. \u201cThey can flip coins in much more sophisticated ways than can the most patient human being. The point is that they ought not to be given such tasks. They may even be able to arrive at \u2018correct\u2019 decisions in some cases\u2014but always and necessarily on bases no human being should be willing to accept.\u201d It\u2019s a caution worth keeping in mind. As AI therapists arrive at scale, we\u2019re seeing them play out a familiar dynamic: Tools designed with superficially good intentions are enmeshed with systems that can exploit, surveil, and reshape human behavior. In a frenzied attempt to unlock new opportunities for patients in dire need of mental-health support, we may be locking other doors behind them. Becky Ferreira is a science reporter based in upstate New York and author of First Contact: The Story of Our Obsession with Aliens. hide"
  ]
}