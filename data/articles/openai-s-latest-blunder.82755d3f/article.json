{
  "url": "https://www.technologyreview.com/2024/05/22/1092763/openais-gpt4o-chinese-ai-data/",
  "title": "OpenAI\u2019s latest blunder shows the challenges facing Chinese AI models",
  "ut": 1716337800.0,
  "body_paragraphs": [
    "This story first appeared in China Report, MIT Technology Review\u2019s newsletter about technology in China.\u00a0Sign up\u00a0to receive it in your inbox every Tuesday. Last week\u2019s release of GPT-4o, a new AI \u201comnimodel\u201d that you can interact with using voice, text, or video, was supposed to be a big moment for OpenAI. But just days later, it feels as if the company is in big trouble. From the resignation of most of its safety team to Scarlett Johansson\u2019s accusation that it replicated her voice for the model against her consent, it\u2019s now in damage-control mode.\u00a0  Add to that another thing OpenAI fumbled with GPT-4o: the data it used to train its tokenizer\u2014a tool that helps the model parse and process text more efficiently\u2014is polluted by Chinese spam websites. As a result, the model\u2019s Chinese token library is full of phrases related to pornography and gambling. This could worsen some problems that are common with AI models: hallucinations, poor performance, and misuse.\u00a0 I wrote about it on Friday after several researchers and AI industry insiders flagged the problem. They took a look at GPT-4o\u2019s public token library, which has been significantly updated with the new model to improve support of non-English languages, and saw that more than 90 of the 100 longest Chinese tokens in the model are from spam websites. These are phrases like \u201c_free Japanese porn video to watch,\u201d \u201cBeijing race car betting,\u201d and \u201cChina welfare lottery every day.\u201d",
    "Anyone who reads Chinese could spot the problem with this list of tokens right away. Some such phrases inevitably slip into training data sets because of how popular adult content is online, but for them to account for 90% of the Chinese language used to train the model? That\u2019s alarming. \u201cIt\u2019s an embarrassing thing to see as a Chinese person. Is that just how the quality of the [Chinese] data is? Is it because of insufficient data cleaning or is the language just like that?\u201d says Zhengyang Geng, a PhD student in computer science at Carnegie Mellon University.",
    "It could be tempting to draw a conclusion about a language or a culture from the tokens OpenAI chose for GPT-4o. After all, these are selected as commonly seen and significant phrases from the respective languages. There\u2019s an interesting blog post by a Hong Kong\u2013based researcher named Henry Luo, who queried the longest GPT-4o tokens in various different languages and found that they seem to have different themes. While the tokens in Russian reflect language about the government and public institutions, the tokens in Japanese have a lot of different ways to say \u201cthank you.\u201d Related StoryWhat\u2019s next for OpenAIWe break down what you missed and what\u2019s next for the AI industry.",
    "But rather than reflecting the differences between cultures or countries, I think this explains more about what kind of training data is readily available online, and the websites OpenAI crawled to feed into GPT-4o. After I published the story, Victor Shih, a political science professor at the University of California, San Diego, commented on it on X: \u201cWhen you try not [to] train on Chinese state media content, this is what you get.\u201d It\u2019s half a joke, and half a serious point about the two biggest problems in training large language models to speak Chinese: the readily available data online reflects either the \u201cofficial,\u201d sanctioned way of talking about China or the omnipresent spam content that drowns out real conversations.  In fact, among the few long Chinese tokens in GPT-4o that aren\u2019t either pornography or gambling nonsense, two are \u201csocialism with Chinese characteristics\u201d and \u201cPeople\u2019s Republic of China.\u201d The presence of these phrases suggests that a significant part of the training data actually is from Chinese state media writings, where formal, long expressions are extremely common. OpenAI has historically been very tight-lipped about the data it uses to train its models, and it probably will never tell us how much of its Chinese training database is state media and how much is spam. (OpenAI didn\u2019t respond to MIT Technology Review\u2019s detailed questions sent on Friday.) But it is not the only company struggling with this problem. People inside China who work in its AI industry agree there\u2019s a lack of quality Chinese text data sets for training LLMs. One reason is that the Chinese internet used to be, and largely remains, divided up by big companies like Tencent and ByteDance. They own most of the social platforms and aren\u2019t going to share their data with competitors or third parties to train LLMs.\u00a0 In fact, this is also why search engines, including Google, kinda suck when it comes to searching in Chinese. Since WeChat content can only be searched on WeChat, and content on Douyin (the Chinese TikTok) can only be searched on Douyin, this data is not accessible to a third-party search engine, let alone an LLM. But these are the platforms where actual human conversations are happening, instead of some spam website that keeps trying to draw you into online gambling.",
    "The lack of quality training data is a much bigger problem than the failure to filter out the porn and general nonsense in GPT-4o\u2019s token-training data. If there isn\u2019t an existing data set, AI companies have to put in significant work to identify, source, and curate their own data sets and filter out inappropriate or biased content.\u00a0 It doesn\u2019t seem OpenAI did that, which in fairness makes some sense, given that people in China can\u2019t use its AI models anyway.\u00a0 Still, there are many people living outside China who want to use AI services in Chinese. And they deserve a product that works properly as much as speakers of any other language do.\u00a0 How can we solve the problem of the lack of good Chinese LLM training data? Tell me your idea at zeyi@technologyreview.com.  Now read the rest of China Report Catch up with China 1. China launched an anti-dumping investigation into imports of polyoxymethylene copolymer\u2014a widely used plastic in electronics and cars\u2014from the US, the EU, Taiwan, and Japan. It's widely seen as a response to the new US tariff announced on Chinese EVs. (BBC)  Meanwhile, Latin American countries, including Mexico, Chile, and Brazil, have increased tariffs on Chinese-imported steel, testing China\u2019s relationship with the region. (Bloomberg $)  2. China\u2019s solar-industry boom is incentivizing farmers to install solar panels and make some extra cash by selling the electricity they generate. (Associated Press) 3. Hedging against the potential devaluation of the RMB, Chinese buyers are pushing the price of gold to all-time highs. (Financial Times $) 4. The Shanghai government set up a pilot project that allows data to be transferred out of China without going through the much-dreaded security assessments, a move that has been sought by companies like Tesla. (Reuters $)",
    "5. China's central bank fined seven businesses\u2014including a KFC and branches of state-owned corporations\u2014for rejecting cash payments. The popularization of mobile payment has been a good thing, but the dwindling support for cash is also making life harder for people like the elderly and foreign tourists. (Business Insider $) 6. Alibaba and Baidu are waging an LLM price war in China to attract more users. (Bloomberg $)",
    "7. The Chinese government has sanctioned Mike Gallagher, a former Republican congressman who chaired the Select Committee on China and remains a fierce critic of Beijing. (NBC News) Lost in translation China's National Health Commission is exploring the relaxation of stringent rules around human genetic data to boost the biotech industry, according to the Chinese publication Caixin. A regulation enacted in 1998 required any research that involves the use of this data to clear an approval process. And there\u2019s even more scrutiny if the research involves foreign institutions.\u00a0 In the early years of human genetic research, the regulation helped prevent the nonconsensual collection of DNA. But as the use of genetic data becomes increasingly important in discovering new treatments, the industry has been complaining about the bureaucracy, which can add an extra two to four months to research projects. Now the government is holding discussions on how to revise the regulation, potentially lifting the approval process for smaller-scale research and more foreign entities, as part of a bid to accelerate the growth of biotech research in China. One more thing Did you know that the Beijing Capital International Airport has been employing birds of prey to chase away other birds since 2019? This month, the second generation of Beijing\u2019s birdy employees started their work driving away the migratory birds that could endanger aircraft. The airport even has different kinds of raptors\u2014Eurasian hobbies, Eurasian goshawks, and Eurasian sparrowhawks\u2014to deal with the different bird species that migrate to Beijing at different times. hide"
  ]
}