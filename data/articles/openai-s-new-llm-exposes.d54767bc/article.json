{
  "url": "https://www.technologyreview.com/2025/11/13/1127914/openais-new-llm-exposes-the-secrets-of-how-ai-really-works/",
  "title": "OpenAI\u2019s new LLM exposes the secrets of how AI really works",
  "ut": 1763019000.0,
  "body_paragraphs": [
    "ChatGPT maker OpenAI has built an experimental large language model that is far easier to understand than typical models. That\u2019s a big deal, because today\u2019s LLMs are black boxes: Nobody fully understands how they do what they do. Building a model that is more transparent sheds light on how LLMs work in general, helping researchers figure out why models hallucinate, why they go off the rails, and just how far we should trust them with critical tasks.  \u201cAs these AI systems get more powerful, they\u2019re going to get integrated more and more into very important domains,\u201d Leo Gao, a research scientist at OpenAI, told MIT Technology Review in an exclusive preview of the new work. \u201cIt\u2019s very important to make sure they\u2019re safe.\u201d This is still early research. The new model, called a weight-sparse transformer, is far smaller and far less capable than top-tier mass-market models like the firm\u2019s GPT-5, Anthropic\u2019s Claude, and Google DeepMind\u2019s Gemini. At most it\u2019s as capable as GPT-1, a model that OpenAI developed back in 2018, says Gao (though he and his colleagues haven\u2019t done a direct comparison).",
    "But the aim isn\u2019t to compete with the best in class (at least, not yet). Instead, by looking at how this experimental model works, OpenAI hopes to learn about the hidden mechanisms inside those bigger and better versions of the technology. It\u2019s interesting research, says Elisenda Grigsby, a mathematician at Boston College who studies how LLMs work and who was not involved in the project: \u201cI\u2019m sure the methods it introduces will have a significant impact.\u201d",
    "Lee Sharkey, a research scientist at AI startup Goodfire, agrees. \u201cThis work aims at the right target and seems well executed,\u201d he says. Why models are so hard to understand OpenAI\u2019s work is part of a hot new field of research known as mechanistic interpretability, which is trying to map the internal mechanisms that models use when they carry out different tasks. That\u2019s harder than it sounds. LLMs are built from neural networks, which consist of nodes, called neurons, arranged in layers. In most networks, each neuron is connected to every other neuron in its adjacent layers. Such a network is known as a dense network. Dense networks are relatively efficient to train and run, but they spread what they learn across a vast knot of connections. The result is that simple concepts or functions can be split up between neurons in different parts of a model. At the same time, specific neurons can also end up representing multiple different features, a phenomenon known as superposition (a term borrowed from quantum physics). The upshot is that you can\u2019t relate specific parts of a model to specific concepts. \u201cNeural networks are big and complicated and tangled up and very difficult to understand,\u201d says Dan Mossing, who leads the mechanistic interpretability team at OpenAI. \u201cWe\u2019ve sort of said: \u2018Okay, what if we tried to make that not the case?\u2019\u201d Instead of building a model using a dense network, OpenAI started with a type of neural network known as a weight-sparse transformer, in which each neuron is connected to only a few other neurons. This forced the model to represent features in localized clusters rather than spread them out. Their model is far slower than any LLM on the market. But it is easier to relate its neurons or groups of neurons to specific concepts and functions. \u201cThere\u2019s a really drastic difference in how interpretable the model is,\u201d says Gao. Gao and his colleagues have tested the new model with very simple tasks. For example, they asked it to complete a block of text that opens with quotation marks by adding matching marks at the end.",
    "It\u2019s a trivial request for an LLM. The point is that figuring out how a model does even a straightforward task like that involves unpicking a complicated tangle of neurons and connections, says Gao. But with the new model, they were able to follow the exact steps the model took. \u201cWe actually found a circuit that\u2019s exactly the algorithm you would think to implement by hand, but it\u2019s fully learned by the model,\u201d he says. \u201cI think this is really cool and exciting.\u201d Where will the research go next? Grigsby is not convinced the technique would scale up to larger models that have to handle a variety of more difficult tasks.\u00a0\u00a0\u00a0\u00a0 Gao and Mossing acknowledge that this is a big limitation of the model they have built so far and agree that the approach will never lead to models that match the performance of cutting-edge products like GPT-5. And yet OpenAI thinks it might be able to improve the technique enough to build a transparent model on a par with GPT-3, the firm\u2019s breakthrough 2021 LLM.\u00a0 \u201cMaybe within a few years, we could have a fully interpretable GPT-3, so that you could go inside every single part of it and you could understand how it does every single thing,\u201d says Gao. \u201cIf we had such a system, we would learn so much.\u201d hide"
  ]
}