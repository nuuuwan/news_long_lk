{
  "url": "https://www.technologyreview.com/2024/07/25/1095347/a-new-tool-for-copyright-holders-can-show-if-their-work-is-in-ai-training-data/",
  "title": "\u201cCopyright traps\u201d could tell writers if an AI has scraped their work",
  "ut": 1721893161.0,
  "body_paragraphs": [
    "Since the beginning of the generative AI boom, content creators have argued that their work has been scraped into AI models without their consent. But until now, it has been difficult to know whether specific text has actually been used in a training data set.\u00a0 Now they have a new way to prove it: \u201ccopyright traps\u201d developed by a team at Imperial College London, pieces of hidden text that allow writers and publishers to subtly mark their work in order to later detect whether it has been used in AI models or not. The idea is similar to traps that have been used by copyright holders throughout history\u2014strategies like including fake locations on a map or fake words in a dictionary.\u00a0  These AI copyright traps tap into one of the biggest fights in AI. A number of publishers and writers are in the middle of litigation against tech companies, claiming their intellectual property has been scraped into AI training data sets without their permission. The New York Times\u2019 ongoing case against OpenAI is probably the most high-profile of these.\u00a0\u00a0 The code to generate and detect traps is currently available on GitHub, but the team also intends to build a tool that allows people to generate and insert copyright traps themselves.",
    "\u201cThere is a complete lack of transparency in terms of which content is used to train models, and we think this is preventing finding the right balance [between AI companies and content creators],\u201d says Yves-Alexandre de Montjoye, an associate professor of applied mathematics and computer science at Imperial College London, who led the research. It was presented at the International Conference on Machine Learning, a top AI conference being held in Vienna this week.\u00a0 Related StoryGoogle DeepMind\u2019s new AI systems can now solve complex math problemsAlphaProof and AlphaGeometry 2 are steps toward building systems that can reason, which could unlock exciting new capabilities.",
    "To create the traps, the team used a word generator to create thousands of synthetic sentences. These sentences are long and full of gibberish, and could look something like this: \u201dWhen in comes times of turmoil \u2026 whats on sale and more important when, is best, this list tells your who is opening on Thrs. at night with their regular sale times and other opening time from your neighbors. You still.\u201d",
    "The team generated 100 trap sentences and then randomly chose one to inject into a text many times, de Montjoye explains. The trap could be injected into text in multiple ways\u2014for example, as white text on a white background, or embedded in the article\u2019s source code. This sentence had to be repeated in the text 100 to 1,000 times.\u00a0 To detect the traps, they fed a large language model the 100 synthetic sentences they had generated, and looked at whether it flagged them as new or not. If the model had seen a trap sentence in its training data, it would indicate a lower \u201csurprise\u201d (also known as \u201cperplexity\u201d) score. But if the model was \u201csurprised\u201d about sentences, it meant that it was encountering them for the first time, and therefore they weren\u2019t traps.\u00a0 In the past, researchers have suggested exploiting the fact that language models memorize their training data to determine whether something has appeared in that data. The technique, called a \u201cmembership inference attack,\u201d works effectively in large state-of-the art models, which tend to memorize a lot of their data during training.\u00a0 In contrast, smaller models, which are gaining popularity and can be run on mobile devices, memorize less and are thus less susceptible to membership inference attacks, which makes it harder to determine whether or not they were trained on a particular copyrighted document, says Gautam Kamath, an assistant computer science professor at the University of Waterloo, who was not part of the research.\u00a0  Copyright traps are a way to do membership inference attacks even on smaller models. The team injected their traps into the training data set of CroissantLLM, a new bilingual French-English language model that was trained from scratch by a team of industry and academic researchers that the Imperial College London team partnered with. CroissantLLM has 1.3 billion parameters, a fraction as many as state-of-the-art models (GPT-4 reportedly has 1.76 trillion, for example). Related StoryAI companies are finally being forced to cough up for training dataThe music industry\u2019s lawsuit sends the loudest message yet: High-quality training data is not free.",
    "The research shows it is indeed possible to introduce such traps into text data so as to significantly increase the efficacy of membership inference attacks, even for smaller models, says Kamath. But there's still a lot to be done, he adds.\u00a0 Repeating a 75-word phrase 1,000 times in a document is a big change to the original text, which could allow people training AI models to detect the trap and skip content containing it, or just delete it and train on the rest of the text, Kamath says. It also makes the original text hard to read.\u00a0 This makes copyright traps impractical right now, says Sameer Singh, a professor of computer science at the University of California, Irvine, and a cofounder of the startup Spiffy AI. He was not part of the research. \u201cA lot of companies do deduplication, [meaning] they clean up the data, and a bunch of this kind of stuff will probably get thrown out,\u201d Singh says.",
    "One way to improve copyright traps, says Kamath, would be to find other ways to mark copyrighted content so that membership inference attacks work better on them, or to improve membership inference attacks themselves.\u00a0 De Montjoye acknowledges that the traps are not foolproof. A motivated attacker who knows about a trap can remove them, he says.\u00a0 \u201cWhether they can remove all of them or not is an open question, and that\u2019s likely to be a bit of a cat-and-mouse game,\u201d he says. But even then, the more traps are applied, the harder it becomes to remove all of them without significant engineering resources. \u201cIt\u2019s important to keep in mind that copyright traps may only be a stopgap solution, or merely an inconvenience to model trainers,\u201d says Kamath. \u201cOne can not release a piece of content containing a trap and have any assurance that it will be an effective trap forever.\u201d\u00a0 hide"
  ]
}