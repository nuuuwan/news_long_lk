# â€œCopyright trapsâ€ could tell writers if an AI has scraped their work

## Summary ğŸ¤–

1. ğŸ“š Content creators have expressed concerns over their work being used by AI models without permission. 
2. ğŸ‘€ To address this, researchers at Imperial College London have developed 'copyright traps' to mark a creator's work.
3. ğŸ“ These traps are similar to historical strategies by copyright holders such as fake map locations or words in a dictionary.
4. ğŸ” The code for generating and detecting traps is available on GitHub, with plans to create a tool for individuals to create their own traps.
5. ğŸ› There is ongoing litigation by writers and publishers against tech companies over the use of their work in AI models.
6. âœï¸ To create traps, the team used a word generator to create thousands of synthetic sentences and injected a chosen one into a text multiple times.
7. ğŸ§ The trap can be detected using a language model to determine its 'surprise' score â€“ a low score indicating the model has seen the sentence before.
8. ğŸ”¬ The technique for detection is known as a "membership inference attack" and works well with large models but less effectively with smaller ones.
9. â˜ï¸ There are limitations to this approach as training AI models could detect the trap and skip the content, making it impractical.
10. ğŸ”„ The researchers acknowledge that a trap can be removed by a motivated attacker but the more traps applied, the harder removal becomes.

## Full Text

[https://www.technologyreview.com/2024/07/25/1095347/a-new-tool-for-copyright-holders-can-show-if-their-work-is-in-ai-training-data/](https://www.technologyreview.com/2024/07/25/1095347/a-new-tool-for-copyright-holders-can-show-if-their-work-is-in-ai-training-data/)

*01:09 PM, Thursday, July 25, 2024*

Since the beginning of the generative AI boom, content creators have argued that their work has been scraped into AI models without their consent. But until now, it has been difficult to know whether specific text has actually been used in a training data set.Â  Now they have a new way to prove it: â€œcopyright trapsâ€ developed by a team at Imperial College London, pieces of hidden text that allow writers and publishers to subtly mark their work in order to later detect whether it has been used in AI models or not. The idea is similar to traps that have been used by copyright holders throughout historyâ€”strategies like including fake locations on a map or fake words in a dictionary.Â   These AI copyright traps tap into one of the biggest fights in AI. A number of publishers and writers are in the middle of litigation against tech companies, claiming their intellectual property has been scraped into AI training data sets without their permission. The New York Timesâ€™ ongoing case against OpenAI is probably the most high-profile of these.Â Â  The code to generate and detect traps is currently available on GitHub, but the team also intends to build a tool that allows people to generate and insert copyright traps themselves.

â€œThere is a complete lack of transparency in terms of which content is used to train models, and we think this is preventing finding the right balance [between AI companies and content creators],â€ says Yves-Alexandre de Montjoye, an associate professor of applied mathematics and computer science at Imperial College London, who led the research. It was presented at the International Conference on Machine Learning, a top AI conference being held in Vienna this week.Â  Related StoryGoogle DeepMindâ€™s new AI systems can now solve complex math problemsAlphaProof and AlphaGeometry 2 are steps toward building systems that can reason, which could unlock exciting new capabilities.

To create the traps, the team used a word generator to create thousands of synthetic sentences. These sentences are long and full of gibberish, and could look something like this: â€When in comes times of turmoil â€¦ whats on sale and more important when, is best, this list tells your who is opening on Thrs. at night with their regular sale times and other opening time from your neighbors. You still.â€

The team generated 100 trap sentences and then randomly chose one to inject into a text many times, de Montjoye explains. The trap could be injected into text in multiple waysâ€”for example, as white text on a white background, or embedded in the articleâ€™s source code. This sentence had to be repeated in the text 100 to 1,000 times.Â  To detect the traps, they fed a large language model the 100 synthetic sentences they had generated, and looked at whether it flagged them as new or not. If the model had seen a trap sentence in its training data, it would indicate a lower â€œsurpriseâ€ (also known as â€œperplexityâ€) score. But if the model was â€œsurprisedâ€ about sentences, it meant that it was encountering them for the first time, and therefore they werenâ€™t traps.Â  In the past, researchers have suggested exploiting the fact that language models memorize their training data to determine whether something has appeared in that data. The technique, called a â€œmembership inference attack,â€ works effectively in large state-of-the art models, which tend to memorize a lot of their data during training.Â  In contrast, smaller models, which are gaining popularity and can be run on mobile devices, memorize less and are thus less susceptible to membership inference attacks, which makes it harder to determine whether or not they were trained on a particular copyrighted document, says Gautam Kamath, an assistant computer science professor at the University of Waterloo, who was not part of the research.Â   Copyright traps are a way to do membership inference attacks even on smaller models. The team injected their traps into the training data set of CroissantLLM, a new bilingual French-English language model that was trained from scratch by a team of industry and academic researchers that the Imperial College London team partnered with. CroissantLLM has 1.3 billion parameters, a fraction as many as state-of-the-art models (GPT-4 reportedly has 1.76 trillion, for example). Related StoryAI companies are finally being forced to cough up for training dataThe music industryâ€™s lawsuit sends the loudest message yet: High-quality training data is not free.

The research shows it is indeed possible to introduce such traps into text data so as to significantly increase the efficacy of membership inference attacks, even for smaller models, says Kamath. But there's still a lot to be done, he adds.Â  Repeating a 75-word phrase 1,000 times in a document is a big change to the original text, which could allow people training AI models to detect the trap and skip content containing it, or just delete it and train on the rest of the text, Kamath says. It also makes the original text hard to read.Â  This makes copyright traps impractical right now, says Sameer Singh, a professor of computer science at the University of California, Irvine, and a cofounder of the startup Spiffy AI. He was not part of the research. â€œA lot of companies do deduplication, [meaning] they clean up the data, and a bunch of this kind of stuff will probably get thrown out,â€ Singh says.

One way to improve copyright traps, says Kamath, would be to find other ways to mark copyrighted content so that membership inference attacks work better on them, or to improve membership inference attacks themselves.Â  De Montjoye acknowledges that the traps are not foolproof. A motivated attacker who knows about a trap can remove them, he says.Â  â€œWhether they can remove all of them or not is an open question, and thatâ€™s likely to be a bit of a cat-and-mouse game,â€ he says. But even then, the more traps are applied, the harder it becomes to remove all of them without significant engineering resources. â€œItâ€™s important to keep in mind that copyright traps may only be a stopgap solution, or merely an inconvenience to model trainers,â€ says Kamath. â€œOne can not release a piece of content containing a trap and have any assurance that it will be an effective trap forever.â€Â  hide

