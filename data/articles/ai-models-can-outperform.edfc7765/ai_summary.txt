Some large language models (LLMs) such as OpenAI's GPT-3.5 and GPT-4, and Meta's Llama can perform as well, and in some cases better, than humans in tasks designed to test the ability to comprehend mental states or "theory of mind." Researchers from the University Medical Center Hamburg-Eppendorf used five types of tests to measure the LLMs' abilities to infer someone else's intentions through indirect comments, recognize a faux pas, comprehend irony, and understand what is being implied rather than said directly. The AI models were given each test multiple times and their responses were scored as per human standards. Both versions of GPT performed at or above human averages in tasks involving indirect requests and misdirection while GPT-4 outperformed humans in irony, hinting, and strange stories tests. The largest of the Llama models outperformed humans in recognizing faux pas scenarios. However, scientists warn about overestimating AI abilities as humans' tendency to attribute mental states to entities without a mind might lead to misconceptions.