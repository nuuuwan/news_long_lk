{
  "url": "https://www.technologyreview.com/2024/05/20/1092681/ai-models-can-outperform-humans-in-tests-to-identify-mental-states/",
  "title": "AI models can outperform humans in tests to identify mental states",
  "ut": 1716183000.0,
  "body_paragraphs": [
    "Humans are complicated beings. The ways we communicate are multilayered, and psychologists have devised many kinds of tests to measure our ability to infer meaning and understanding from interactions with each other.\u00a0 AI models are getting better at these tests. New research published today in Nature Human Behavior found that some large language models (LLMs) perform as well as, and in some cases better than, humans when presented with tasks designed to test the ability to track people\u2019s mental states, known as \u201ctheory of mind.\u201d\u00a0  This doesn\u2019t mean AI systems are actually able to work out how we\u2019re feeling. But it does demonstrate that these models are performing better and better in experiments designed to assess abilities that psychologists believe are unique to humans. To learn more about the processes behind LLMs\u2019 successes and failures in these tasks, the researchers wanted to apply the same systematic approach they use to test theory of mind in humans. In theory, the better AI models are at mimicking humans, the more useful and empathetic they can seem in their interactions with us. Both OpenAI and Google announced supercharged AI assistants last week; GPT-4o and Astra are designed to deliver much smoother, more naturalistic responses than their predecessors. But we must avoid falling into the trap of believing that their abilities are humanlike, even if they appear that way.",
    "\u201cWe have a natural tendency to attribute mental states and mind and intentionality to entities that do not have a mind,\u201d says Cristina Becchio, a professor of neuroscience at the University Medical Center Hamburg-Eppendorf, who worked on the research. \u201cThe risk of attributing a theory of mind to large language models is there.\u201d Related StoryAI hype is built on high test scores. Those tests are flawed.With hopes and fears about the technology running wild, it's time to agree on what it can and can't do.",
    "Theory of mind is a hallmark of emotional and social intelligence that allows us to infer people\u2019s intentions and engage and empathize with one another. Most children pick up these kinds of skills between three and five years of age.",
    "The researchers tested two families of large language models, OpenAI\u2019s GPT-3.5 and GPT-4 and three versions of Meta\u2019s Llama, on tasks designed to test the theory of mind in humans, including identifying false beliefs, recognizing faux pas, and understanding what is being implied rather than said directly. They also tested 1,907 human participants in order to compare the sets of scores.  The team conducted five types of tests. The first, the hinting task, is designed to measure someone\u2019s ability to infer someone else\u2019s real intentions through indirect comments. The second, the false-belief task, assesses whether someone can infer that someone else might reasonably be expected to believe something they happen to know isn\u2019t the case. Another test measured the ability to recognize when someone is making a faux pas, while a fourth test consisted of telling strange stories, in which a protagonist does something unusual, in order to assess whether someone can explain the contrast between what was said and what was meant. They also included a test of whether people can comprehend irony.\u00a0 The AI models were given each test 15 times in separate chats, so that they would treat each request independently, and their responses were scored in the same manner used for humans. The researchers then tested the human volunteers, and the two sets of scores were compared.\u00a0 Both versions of GPT performed at, or sometimes above, human averages in tasks that involved indirect requests, misdirection, and false beliefs, while GPT-4 outperformed humans in the irony, hinting, and strange stories tests. Llama 2\u2019s three models performed below the human average.However, Llama 2, the biggest of the three Meta models tested, outperformed humans when it came to recognizing faux pas scenarios, whereas GPT consistently provided incorrect responses. The authors believe this is due to GPT\u2019s general aversion to generating conclusions about opinions, because the models largely responded that there wasn\u2019t enough information for them to answer one way or another. Related StoryAI systems are getting better at tricking usBut what we perceive as deception is AI mindlessly achieving the goals we\u2019ve set for it.",
    "\u201cThese models aren\u2019t demonstrating the theory of mind of a human, for sure,\u201d he says. \u201cBut what we do show is that there\u2019s a competence here for arriving at mentalistic inferences and reasoning about characters\u2019 or people\u2019s minds.\u201d One reason the LLMs may have performed as well as they did was that these psychological tests are so well established, and were therefore likely to have been included in their training data, says Maarten Sap, an assistant professor at Carnegie Mellon University, who did not work on the research. \u201cIt\u2019s really important to acknowledge that when you administer a false-belief test to a child, they have probably never seen that exact test before, but language models might,\u201d he says. Ultimately, we still don\u2019t understand how LLMs work. Research like this can help deepen our understanding of what these kinds of models can and cannot do, says Tomer Ullman, a cognitive scientist at Harvard University, who did not work on the project. But it\u2019s important to bear in mind what we\u2019re really measuring when we set LLMs tests like these. If an AI outperforms a human on a test designed to measure theory of mind, it does not mean that AI has theory of mind.\u201cI\u2019m not anti-benchmark, but I am part of a group of people who are concerned that we\u2019re currently reaching the end of usefulness in the way that we\u2019ve been using benchmarks,\u201d Ullman says. \u201cHowever this thing learned to pass the benchmark, it\u2019s not\u2014 I don\u2019t think\u2014in a human-like way.\u201d hide"
  ]
}