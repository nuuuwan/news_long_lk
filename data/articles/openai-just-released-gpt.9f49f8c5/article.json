{
  "url": "https://www.technologyreview.com/2025/02/27/1112619/openai-just-released-gpt-4-5-and-says-it-is-its-biggest-and-best-chat-model-yet/",
  "title": "OpenAI just released GPT-4.5 and says it is its biggest and best chat model yet",
  "ut": 1740650242.0,
  "body_paragraphs": [
    "OpenAI has just released GPT-4.5, a new version of its flagship large language model. The company claims it is its biggest and best model for all-round chat yet. \u201cIt\u2019s really a step forward for us,\u201d says Mia Glaese, a research scientist at OpenAI. Since the releases of its so-called reasoning models o1 and o3, OpenAI has been pushing two product lines. GPT-4.5 is part of the non-reasoning lineup\u2014what Glaese\u2019s colleague Nick Ryder, also a research scientist, calls \u201can installment in the classic GPT series.\u201d  People with a $200-a-month ChatGPT Pro account can try out GPT-4.5 today. OpenAI says it will begin rolling out to other users next week. With each release of its GPT models, OpenAI has shown that bigger means better. But there has been a lot of talk about how that approach is hitting a wall\u2014including remarks from OpenAI\u2019s former chief scientist Ilya Sutskever. The company\u2019s claims about GPT-4.5 feel like a thumb in the eye to the naysayers.",
    "All large language models pick up patterns across the billions of documents they are trained on. Smaller models learned syntax and basic facts. Bigger models can find more specific patterns like emotional cues, such as when a speaker\u2019s words signal hostility, says Ryder: \u201cAll of these subtle patterns that come through a human conversation\u2014those are the bits that these larger and larger models will pick up on.\u201d \u201cIt has the ability to engage in warm, intuitive, natural, flowing conversations,\u201d says Glaese. \u201cAnd we think that it has a stronger understanding of what users mean, especially when their expectations are more implicit, leading to nuanced and thoughtful responses.\u201d",
    "Making it hum \u201cWe kind of know what the engine looks like at this point, and now it\u2019s really about making it hum,\u201d says Ryder. \u201cThis is primarily an exercise in scaling up the compute, scaling up the data, finding more efficient training methods, and then pushing the frontier.\u201d OpenAI won\u2019t say exactly how big its new model is. But it claims the jump in scale from GPT-4o to GPT-4.5 is the same as the jump from GPT-3.5 to GPT-4o. Experts have estimated that GPT-4 could have as many as 1.8 trillion parameters, the values that get tweaked when a model is trained.\u00a0 GPT-4.5 was trained with techniques similar to those used for its predecessor GPT-4o, including human-led fine-tuning and reinforcement learning with human feedback. \u201cThe key to creating intelligent systems is a recipe we\u2019ve been following for many years, which is to find scalable paradigms where we can pour more and more resources in to get more intelligent systems out,\u201d says Ryder.  Unlike reasoning models such as o1 and o3, which work through answers step by step, most large language models like GPT-4.5 spit out the first response they come up with. But GPT-4.5 is more general-purpose. Tested on SimpleQA, a kind of general-knowledge quiz developed by OpenAI last year that includes questions on topics from science and technology to TV shows and video games, GPT-4.5 scores 62.5% compared with 38.6% for GPT-4o and 15% for o3-mini. What\u2019s more, OpenAI claims that GPT-4.5 responds with far fewer made-up answers (known as hallucinations). On the same test, GPT-4.5 made up answers 37.1% of the time, compared with 59.8% for GPT-4o and 80.3% for o3-mini. But SimpleQA is just one benchmark. On other tests, including MMLU, a more common benchmark for comparing large language models, GPT-4.5 beat OpenAI's previous models by a smaller margin. And on standard science and math benchmarks, GPT-4.5 scores worse than o3-mini. Turning on the charm GPT-4.5\u2019s special charm seems to be its conversational skills. Human testers employed by OpenAI say they preferred GPT-4.5 to GPT-4o for everyday queries, professional queries, and creative tasks, including coming up with poems. (Ryder says it is also great at old-school internet ACSII art.)",
    "For example, tell it that you're going through a rough patch and GPT-4.5 might offer a few words of sympathy before saying: \"Want to talk about what happened, or do you just need a distraction? I'm here either way.\" GPT-4o is less good at reading social cues and might try to fix the problem whether you asked it to or not, hitting you with a bullet point list of ways to cheer yourself up.  And yet after years at the top, OpenAI faces a tough crowd. \u201cThe focus on emotional intelligence and creativity is cool for niche use cases like writing coaches and brainstorming buddies,\u201d says\u00a0Waseem Alshikh, cofounder and CTO of Writer, a startup that develops large language models for enterprise customers. \u201cBut GPT-4.5 feels like a shiny new coat of paint on the same old car,\u201d he says. \u201cThrowing more compute and data at a model can make it sound smoother, but it\u2019s not a game-changer.\u201d \u201cThe juice isn\u2019t worth the squeeze when you consider the energy costs and the fact that most users won\u2019t notice the difference in daily use,\u201d he says. \u201cI\u2019d rather see them pivot to efficiency or niche problem-solving than keep supersizing the same recipe.\u201d Sam Altman has said that GPT-4.5 will be the last release in OpenAI\u2019s classic lineup and that GPT-5 will be a hybrid that combines a general-purpose large language model with a reasoning model. \u201cGPT-4.5 is OpenAI phoning it in while they cook up something bigger behind closed doors,\" says Alshikh. \u201cUntil then, this feels like a pit stop.\u201d Of course, OpenAI insists that its supersized approach still has legs. \u201cPersonally, I\u2019m very optimistic about finding ways through those bottlenecks and continuing to scale,\u201d says Ryder. \u201cI think there\u2019s something extremely profound and exciting about pattern-matching across all of human knowledge.\u201d hide"
  ]
}