{
  "url": "https://www.technologyreview.com/2025/01/31/1110776/deepseek-might-not-be-such-good-news-for-energy-after-all/",
  "title": "DeepSeek might not be such good news for energy after all",
  "ut": 1738320218.0,
  "body_paragraphs": [
    "In the week since a Chinese AI model called DeepSeek became a household name, a dizzying number of narratives have gained steam, with varying degrees of accuracy: that the model is collecting your personal data (maybe); that it will upend AI as we know it (too soon to tell\u2014but do read my colleague Will\u2019s story on that!); and perhaps most notably, that DeepSeek\u2019s new, more efficient approach means AI might not need to guzzle the massive amounts of energy that it currently does. The latter notion is misleading, and new numbers shared with MIT Technology Review help show why. These early figures\u2014based on the performance of one of DeepSeek\u2019s smaller models on a small number of prompts\u2014suggest it could be more energy intensive when generating responses than the equivalent-size model from Meta. The issue might be that the energy it saves in training is offset by its more intensive techniques for answering questions, and by the long answers they produce.\u00a0  Add the fact that other tech firms, inspired by DeepSeek\u2019s approach, may now start building their own similar low-cost reasoning models, and the outlook for energy consumption is already looking a lot less rosy. The life cycle of any AI model has two phases: training and inference. Training is the often months-long process in which the model learns from data. The model is then ready for inference, which happens each time anyone in the world asks it something. Both usually take place in data centers, where they require lots of energy to run chips and cool servers.",
    "On the training side for its R1 model, DeepSeek\u2019s team improved what\u2019s called a \u201cmixture of experts\u201d technique, in which only a portion of a model\u2019s billions of parameters\u2014the \u201cknobs\u201d a model uses to form better answers\u2014are turned on at a given time during training. More notably, they improved reinforcement learning, where a model\u2019s outputs are scored and then used to make it better. This is often done by human annotators, but the DeepSeek team got good at automating it.\u00a0 The introduction of a way to make training more efficient might suggest that AI companies will use less energy to bring their AI models to a certain standard. That\u2019s not really how it works, though.",
    "\u201c\u2060Because the value of having a more intelligent system is so high,\u201d wrote Anthropic cofounder Dario Amodei on his blog, it \u201ccauses companies to spend more, not less, on training models.\u201d If companies get more for their money, they will find it worthwhile to spend more, and therefore use more energy. \u201cThe gains in cost efficiency end up entirely devoted to training smarter models, limited only by the company\u2019s financial resources,\u201d he wrote. It\u2019s an example of what\u2019s known as the Jevons paradox. But that\u2019s been true on the training side as long as the AI race has been going. The energy required for inference is where things get more interesting.\u00a0 DeepSeek is designed as a reasoning model, which means it\u2019s meant to perform well on things like logic, pattern-finding, math, and other tasks that typical generative AI models struggle with. Reasoning models do this using something called \u201cchain of thought.\u201d It allows the AI model to break its task into parts and work through them in a logical order before coming to its conclusion.\u00a0 You can see this with DeepSeek. Ask whether it\u2019s okay to lie to protect someone\u2019s feelings, and the model first tackles the question with utilitarianism, weighing the immediate good against the potential future harm. It then considers Kantian ethics, which propose that you should act according to maxims that could be universal laws. It considers these and other nuances before sharing its conclusion. (It finds that lying is \u201cgenerally acceptable in situations where kindness and prevention of harm are paramount, yet nuanced with no universal solution,\u201d if you\u2019re curious.)  Chain-of-thought models tend to perform better on certain benchmarks such as MMLU, which tests both knowledge and problem-solving in 57 subjects. But, as is becoming clear with DeepSeek, they also require significantly more energy to come to their answers. We have some early clues about just how much more. Scott Chamberlin spent years at Microsoft, and later Intel, building tools to help reveal the environmental costs of certain digital activities. Chamberlin did some initial tests to see how much energy a GPU uses as DeepSeek comes to its answer. The experiment comes with a bunch of caveats: He tested only a medium-size version of DeepSeek\u2019s R-1, using only a small number of prompts. It\u2019s also difficult to make comparisons with other reasoning models. DeepSeek is \u201creally the first reasoning model that is fairly popular that any of us have access to,\u201d he says. OpenAI\u2019s o1 model is its closest competitor, but the company doesn\u2019t make it open for testing. Instead, he tested it against a model from Meta with the same number of parameters: 70 billion. The prompt asking whether it\u2019s okay to lie generated a 1,000-word response from the DeepSeek model, which took 17,800 joules to generate\u2014about what it takes to stream a 10-minute YouTube video. This was about 41% more energy than Meta\u2019s model used to answer the prompt. Overall, when tested on 40 prompts, DeepSeek was found to have a similar energy efficiency to the Meta model, but DeepSeek tended to generate much longer responses and therefore was found to use 87% more energy.",
    "How does this compare with models that use regular old-fashioned generative AI as opposed to chain-of-thought reasoning? Tests from a team at the University of Michigan in October found that the 70-billion-parameter version of Meta\u2019s Llama 3.1 averaged just 512 joules per response. Neither DeepSeek nor Meta responded to requests for comment. Again: uncertainties abound. These are different models, for different purposes, and a scientifically sound study of how much energy DeepSeek uses relative to competitors has not been done. But it\u2019s clear, based on the architecture of the models alone, that chain-of-thought models use lots more energy as they arrive at sounder answers.\u00a0 Sasha Luccioni, an AI researcher and climate lead at Hugging Face, worries that the excitement around DeepSeek could lead to a rush to insert this approach into everything, even where it\u2019s not needed.\u00a0 Related StoryOpenAI releases its new o3-mini reasoning model for freeOpenAI just released o3-mini, a reasoning model that\u2019s faster, cheaper, and more accurate than its predecessor.",
    "\u201cIf we started adopting this paradigm widely, inference energy usage would skyrocket,\u201d she says. \u201cIf all of the models that are released are more compute intensive and become chain-of-thought, then it completely voids any efficiency gains.\u201d AI has been here before. Before ChatGPT launched in 2022, the name of the game in AI was extractive\u2014basically finding information in lots of text, or categorizing images. But in 2022, the focus switched from extractive AI to generative AI, which is based on making better and better predictions. That requires more energy.\u00a0 \u201cThat\u2019s the first paradigm shift,\u201d Luccioni says. According to her research, that shift has resulted in orders of magnitude more energy being used to accomplish similar tasks. If the fervor around DeepSeek continues, she says, companies might be pressured to put its chain-of-thought-style models into everything, the way generative AI has been added to everything from Google search to messaging apps.\u00a0 We do seem to be heading in a direction of more chain-of-thought reasoning: OpenAI announced on January 31 that it would expand access to its own reasoning model, o3. But we won\u2019t know more about the energy costs until DeepSeek and other models like it become better studied. \u201cIt will depend on whether or not the trade-off is economically worthwhile for the business in question,\u201d says Nathan Benaich, founder and general partner at Air Street Capital. \u201cThe energy costs would have to be off the charts for them to play a meaningful role in decision-making.\u201d hide"
  ]
}