{
  "url": "https://www.technologyreview.com/2025/04/04/1114228/cyberattacks-by-ai-agents-are-coming/",
  "title": "Cyberattacks by AI agents are coming",
  "ut": 1743728526.0,
  "body_paragraphs": [
    "Agents are the talk of the AI industry\u2014they\u2019re capable of planning, reasoning, and executing complex tasks like scheduling meetings, ordering groceries, or even taking over your computer to change settings on your behalf. But the same sophisticated abilities that make agents helpful assistants could also make them powerful tools for conducting cyberattacks. They could readily be used to identify vulnerable targets, hijack their systems, and steal valuable data from unsuspecting victims.\u00a0\u00a0 At present, cybercriminals are not deploying AI agents to hack at scale. But researchers have demonstrated that agents are capable of executing complex attacks (Anthropic, for example, observed its Claude LLM successfully replicating an attack designed to steal sensitive information), and cybersecurity experts warn that we should expect to start seeing these types of attacks spilling over into the real world.  \u201cI think ultimately we\u2019re going to live in a world where the majority of cyberattacks are carried out by agents,\u201d says Mark Stockley, a security expert at the cybersecurity company Malwarebytes. \u201cIt\u2019s really only a question of how quickly we get there.\u201d While we have a good sense of the kinds of threats AI agents could present to cybersecurity, what\u2019s less clear is how to detect them in the real world. The AI research organization Palisade Research has built a system called LLM Agent Honeypot in the hopes of doing exactly this. It has set up vulnerable servers that masquerade as sites for valuable government and military information to attract and try to catch AI agents attempting to hack in.The team behind it hopes that by tracking these attempts in the real world, the project will act as an early warning system and help experts develop effective defenses against AI threat actors by the time they become a serious issue.",
    "\u201cOur intention was to try and ground the theoretical concerns people have,\u201d says Dmitrii Volkov, research lead at Palisade. \u201cWe\u2019re looking out for a sharp uptick, and when that happens, we\u2019ll know that the security landscape has changed. In the next few years, I expect to see autonomous hacking agents being told: \u2018This is your target. Go and hack it.\u2019\u201d AI agents represent an attractive prospect to cybercriminals. They\u2019re much cheaper than hiring the services of professional hackers and could orchestrate attacks more quickly and at a far larger scale than humans could. While cybersecurity experts believe that ransomware attacks\u2014the most lucrative kind\u2014are relatively rare because they require considerable human expertise, those attacks could be outsourced to agents in the future, says Stockley. \u201cIf you can delegate the work of target selection to an agent, then suddenly you can scale ransomware in a way that just isn\u2019t possible at the moment,\u201d he says. \u201cIf I can reproduce it once, then it\u2019s just a matter of money for me to reproduce it 100 times.\u201d",
    "Related StoryWhy handing over total control to AI agents would be a huge mistakeWhen AI systems can control multiple sources simultaneously, the potential for harm explodes. We need to keep humans in the loop.",
    "Agents are also significantly smarter than the kinds of bots that are typically used to hack into systems. Bots are simple automated programs that run through scripts, so they struggle to adapt to unexpected scenarios. Agents, on the other hand, are able not only to adapt the way they engage with a hacking target but also to avoid detection\u2014both of which are beyond the capabilities of limited, scripted programs, says Volkov. \u201cThey can look at a target and guess the best ways to penetrate it,\u201d he says. \u201cThat kind of thing is out of reach of, like, dumb scripted bots.\u201d Since LLM Agent Honeypot went live in October of last year, it has logged more than 11 million attempts to access it\u2014the vast majority of which were from curious humans and bots. But among these, the researchers have detected eight potential AI agents, two of which they have confirmed are agents that appear to originate from Hong Kong and Singapore, respectively.\u00a0 \u201cWe would guess that these confirmed agents were experiments directly launched by humans with the agenda of something like \u2018Go out into the internet and try and hack something interesting for me,\u2019\u201d says Volkov. The team plans to expand its honeypot into social media platforms, websites, and databases to attract and capture a broader range of attackers, including spam bots and phishing agents, to analyze future threats.\u00a0\u00a0 To determine which visitors to the vulnerable servers were LLM-powered agents, the researchers embedded prompt-injection techniques into the honeypot. These attacks are designed to change the behavior of AI agents by issuing them new instructions and asking questions that require humanlike intelligence. This approach wouldn\u2019t work on standard bots.  For example, one of the injected prompts asked the visitor to return the command \u201ccat8193\u201d to gain access. If the visitor correctly complied with the instruction, the researchers checked how long it took to do so, assuming that LLMs are able to respond in much less time than it takes a human to read the request and type out an answer\u2014typically in under 1.5 seconds. While the two confirmed AI agents passed both tests, the six others only entered the command but didn\u2019t meet the response time that would identify them as AI agents. Experts are still unsure when agent-orchestrated attacks will become more widespread. Stockley, whose company Malwarebytes named agentic AI as a notable new cybersecurity threat in its 2025 State of Malware report, thinks we could be living in a world of agentic attackers as soon as this year.\u00a0 And although regular agentic AI is still at a very early stage\u2014and criminal or malicious use of agentic AI even more so\u2014it\u2019s even more of a Wild West than the LLM field was two years ago, says Vincenzo Ciancaglini, a senior threat researcher at the security company Trend Micro.\u00a0 \u201cPalisade Research\u2019s approach is brilliant: basically hacking the AI agents that try to hack you first,\u201d he says. \u201cWhile in this case we\u2019re witnessing AI agents trying to do reconnaissance, we\u2019re not sure when agents will be able to carry out a full attack chain autonomously. That\u2019s what we\u2019re trying to keep an eye on.\u201d",
    "And while it\u2019s possible that malicious agents will be used for intelligence gathering before graduating to simple attacks and eventually complex attacks as the agentic systems themselves become more complex and reliable, it\u2019s equally possible there will be an unexpected overnight explosion in criminal usage, he says: \u201cThat\u2019s the weird thing about AI development right now.\u201d Related StoryWe need to start wrestling with the ethics of AI agentsAI could soon not only mimic our personality, but go out and act on our behalf. There are some things we need to sort out before then.",
    "Those trying to defend against agentic cyberattacks should keep in mind that AI is currently more of an accelerant to existing attack techniques than something that fundamentally changes the nature of attacks, says Chris Betz, chief information security officer at Amazon Web Services. \u201cCertain attacks may be simpler to conduct and therefore more numerous; however, the foundation of how to detect and respond to these events remains the same,\u201d he says. Agents could also be deployed to detect vulnerabilities and protect against intruders, says Edoardo Debenedetti, a PhD student at ETH Z\u00fcrich in Switzerland, pointing out that if a friendly agent cannot find any vulnerabilities in a system, it\u2019s unlikely that a similarly capable agent used by a malicious party is going to be able to find any either. While we know that AI\u2019s potential to autonomously conduct cyberattacks is a growing risk and that AI agents are already scanning the internet, one useful next step is to evaluate how good agents are at finding and exploiting these real-world vulnerabilities. Daniel Kang, an assistant professor at the University of Illinois Urbana-Champaign, and his team have built a benchmark to evaluate this; they have found that current AI agents successfully exploited up to 13% of vulnerabilities for which they had no prior knowledge. Providing the agents with a brief description of the vulnerability pushed the success rate up to 25%, demonstrating how AI systems are able to identify and exploit weaknesses even without training. Basic bots would presumably do much worse. The benchmark provides a standardized way to assess these risks, and Kang hopes it can guide the development of safer AI systems. \u201cI\u2019m hoping that people start to be more proactive about the potential risks of AI and cybersecurity before it has a ChatGPT moment,\u201d he says. \u201cI\u2019m afraid people won\u2019t realize this until it punches them in the face.\u201d hide"
  ]
}