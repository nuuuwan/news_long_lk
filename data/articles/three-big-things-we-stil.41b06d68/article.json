{
  "url": "https://www.technologyreview.com/2025/09/09/1123408/three-big-things-we-still-dont-know-about-ais-energy-burden/",
  "title": "Three big things we still don\u2019t know about AI\u2019s energy burden",
  "ut": 1757374200.0,
  "body_paragraphs": [
    "Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.\u00a0 This fundamental number remained elusive even as the scramble to power AI escalated to the White House and the Pentagon, and as projections showed that in three years AI could use as much electricity as 22% of all US households.\u00a0  The problem with finding that number, as we explain in our piece published in May, was that AI companies are the only ones who have it. We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure. Researchers we spoke to who study AI\u2019s impact on energy grids compared it to trying to measure the fuel efficiency of a car without ever being able to drive it, making guesses based on rumors of its engine size and what it sounds like going down the highway.  This story is a part of\u00a0MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.",
    "But then this summer, after we published, a strange thing started to happen. In June, OpenAI\u2019s Sam Altman wrote that an average ChatGPT query uses 0.34 watt-hours of energy. In July, the French AI startup Mistral didn\u2019t publish a number directly but released an estimate of the emissions generated. In August, Google revealed that answering a question to Gemini uses about 0.24 watt-hours of energy. The figures from Google and OpenAI were similar to what Casey and I estimated for medium-size AI models.\u00a0 So with this newfound transparency, is our job complete? Did we finally harpoon our white whale, and if so, what happens next for people studying the climate impact of AI? I reached out to some of our old sources, and some new ones, to find out.",
    "The numbers are vague and chat-only The first thing they told me is that there\u2019s a lot missing from the figures tech companies published this summer.\u00a0 OpenAI\u2019s number, for example, did not appear in a detailed technical paper but rather in a blog post by Altman that leaves lots of unanswered questions, such as which model he was referring to, how the energy use was measured, and how much it varies. Google\u2019s figure, as Crownhart points out, refers to the median amount of energy per query, which doesn\u2019t give us a sense of the more energy-demanding Gemini responses, like when it uses a reasoning model to \u201cthink\u201d through a hard problem or generates a really long response.\u00a0 The numbers also refer only to interactions with chatbots, not the other ways that people are becoming increasingly reliant on generative AI.\u00a0 \u201cAs video and image becomes more prominent and used by more and more people, we need the numbers from different modalities and how they measure up,\u201d says Sasha Luccioni, AI and climate lead at the AI platform Hugging Face.",
    "This is also important because the figures for asking a question to a chatbot are, as expected, undoubtedly small\u2014the same amount of electricity used by a microwave in just seconds. That\u2019s part of the reason AI and climate researchers don\u2019t suggest that any one individual\u2019s AI use creates a significant climate burden.\u00a0 A full accounting of AI\u2019s energy demands\u2014one that goes beyond what\u2019s used to answer an individual query to help us understand its full net impact on the climate\u2014would require application-specific information on how all this AI is being used. Ketan Joshi, an analyst for climate and energy groups, acknowledges that researchers don\u2019t usually get such specific information from other industries but says it might be justified in this case. \u201cThe rate of data center growth is inarguably unusual,\u201d Joshi says. \u201cCompanies should be subject to significantly more scrutiny.\u201d Related StoryWe did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t heard.The emissions from individual AI text, image, and video queries seem small\u2014until you add up what the industry isn\u2019t tracking and consider where it\u2019s heading next.",
    "We have questions about energy efficiency Companies making billion-dollar investments into AI have struggled to square this growth in energy demand with their sustainability goals. In May, Microsoft said that its emissions have soared by over 23% since 2020, owing largely to AI, while the company has promised to be carbon negative by 2030. \u201cIt has become clear that our journey towards being carbon negative is a marathon, not a sprint,\u201d Microsoft wrote.",
    "Tech companies often justify this emissions burden by arguing that soon enough, AI itself will unlock efficiencies that will make it a net positive for the climate. Perhaps the right AI system, the thinking goes, could design more efficient heating and cooling systems for a building, or help discover the minerals required for electric-vehicle batteries.\u00a0 But there are no signs that AI has been usefully used to do these things yet. Companies have shared anecdotes about using AI to find methane emission hot spots, for example, but they haven\u2019t been transparent enough to help us know if these successes outweigh the surges in electricity demand and emissions that Big Tech has produced in the AI boom. In the meantime, more data centers are planned, and AI\u2019s energy demand continues to rise and rise.\u00a0 The \u2018bubble\u2019 question One of the big unknowns in the AI energy equation is whether society will ever adopt AI at the levels that figure into tech companies\u2019 plans. OpenAI has said that ChatGPT receives 2.5 billion prompts per day. It\u2019s possible that this number, and the equivalent numbers for other AI companies, will continue to soar in the coming years. Projections released last year by the Lawrence Berkeley National Laboratory suggest that if they do, AI alone could consume as much electricity annually as 22% of all US households by 2028. But this summer also saw signs of a slowdown that undercut the industry\u2019s optimism. OpenAI\u2019s launch of GPT-5 was largely considered a flop, even by the company itself, and that flop led critics to wonder if AI may be hitting a wall. When a group at MIT found that 95% of businesses are seeing no return on their massive AI investments, stocks floundered. The expansion of AI-specific data centers might be an investment that\u2019s hard to recoup, especially as revenues for AI companies remain elusive.\u00a0 One of the biggest unknowns about AI\u2019s future energy burden isn\u2019t how much a single query consumes, or any other figure that can be disclosed. It\u2019s whether demand will ever reach the scale companies are building for or whether the technology will collapse under its own hype. The answer will determine whether today\u2019s buildout becomes a lasting shift in our energy system or a short-lived spike. hide"
  ]
}