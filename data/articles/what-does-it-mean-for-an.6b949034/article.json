{
  "url": "https://www.technologyreview.com/2025/06/17/1118918/what-does-it-mean-for-an-algorithm-to-be-fair/",
  "title": "What does it mean for an algorithm to be \u201cfair\u201d?",
  "ut": 1750115965.0,
  "body_paragraphs": [
    "What Amsterdam\u2019s welfare fraud algorithm taught me about fair and responsible AI.",
    "Back in February, I flew to Amsterdam to report on a high-stakes experiment the city had recently conducted: a pilot program for what it called Smart Check, which was its attempt to create an effective, fair, and unbiased predictive algorithm to try to detect welfare fraud. But the city fell short of its lofty goals\u2014and, with our partners at Lighthouse Reports and the Dutch newspaper Trouw, we tried to get to the bottom of why. You can read about it in our deep dive published last week. For an American reporter, it\u2019s been an interesting time to write a story on \u201cresponsible AI\u201d in a progressive European city\u2014just as ethical considerations in AI deployments appear to be disappearing in the United States, at least at the national level.\u00a0  For example, a few weeks before my trip, the Trump administration rescinded Biden\u2019s executive order on AI safety and DOGE began turning to AI to decide which federal programs to cut. Then, more recently, House Republicans passed a 10-year moratorium on US states\u2019 ability to regulate AI (though it has yet to be passed by the Senate).\u00a0 What all this points to is a new reality in the United States where responsible AI is no longer a priority (if it ever genuinely was).",
    "But this has also made me think more deeply about the stakes of deploying AI in situations that directly affect human lives, and about what success would even look like.\u00a0 When Amsterdam\u2019s welfare department began developing the algorithm that became Smart Check, the municipality followed virtually every recommendation in the responsible-AI playbook: consulting external experts, running bias tests, implementing technical safeguards, and seeking stakeholder feedback. City officials hoped the resulting algorithm could avoid causing the worst types of harm inflicted by discriminatory AI over nearly a decade.",
    "After talking to a large number of people involved in the project and others who would potentially be affected by it, as well as some experts who did not work on it, it\u2019s hard not to wonder if the city could ever have succeeded in its goals when neither \u201cfairness\u201d nor even \u201cbias\u201d has a universally agreed-upon definition. The city was treating these issues as technical ones that could be answered by reweighting numbers and figures\u2014rather than political and philosophical questions that society as a whole has to grapple with. On the afternoon that I arrived in Amsterdam, I sat down with Anke van der Vliet, a longtime advocate for welfare beneficiaries who served on what\u2019s called the Participation Council, a 15-member citizen body that represents benefits recipients and their advocates. Related StoryInside Amsterdam\u2019s high-stakes experiment to create fair welfare AI",
    "The Dutch city thought it could break a decade-long trend of implementing\u00a0discriminatory algorithms. Its failure raises the question: can these programs ever be fair?",
    "The city had consulted the council during Smart Check\u2019s development, but van der Vliet was blunt in sharing the committee\u2019s criticisms of the plans. Its members simply didn\u2019t want the program. They had well-placed fears of discrimination and disproportionate impact, given that fraud is found in only 3% of applications. To the city\u2019s credit, it did respond to some of their concerns and make changes in the algorithm\u2019s design\u2014like removing from consideration factors, such as age, whose inclusion could have had a discriminatory impact. But the city ignored the Participation Council\u2019s main feedback: its recommendation to stop development altogether.\u00a0 Van der Vliet and other welfare advocates I met on my trip, like representatives from the Amsterdam Welfare Union, described what they see as a number of challenges faced by the city\u2019s some 35,000 benefits recipients: the indignities of having to constantly re-prove the need for benefits, the increases in cost of living that benefits payments do not reflect, and the general feeling of distrust between recipients and the government.\u00a0 City welfare officials themselves recognize the flaws of the system, which \u201cis held together by rubber bands and staples,\u201d as Harry Bodaar, a senior policy advisor to the city who focuses on welfare fraud enforcement, told us. \u201cAnd if you\u2019re at the bottom of that system, you\u2019re the first to fall through the cracks.\u201d So the Participation Council didn\u2019t want Smart Check at all, even as Bodaar and others working in the department hoped that it could fix the system. It\u2019s a classic example of a \u201cwicked problem,\u201d a social or cultural issue with no one clear answer and many potential consequences.\u00a0 After the story was published, I heard from Suresh Venkatasubramanian, a former tech advisor to the White House Office of Science and Technology Policy who co-wrote Biden\u2019s AI Bill of Rights (now rescinded by Trump). \u201cWe need participation early on from communities,\u201d he said, but he added that it also matters what officials do with the feedback\u2014and whether there is \u201ca willingness to reframe the intervention based on what people actually want.\u201d",
    "Had the city started with a different question\u2014what people actually want\u2014perhaps it might have developed a different algorithm entirely. As the Dutch digital rights advocate Hans De Zwart put it to us, \u201cWe are being seduced by technological solutions for the wrong problems \u2026 why doesn\u2019t the municipality build an algorithm that searches for people who do not apply for social assistance but are entitled to it?\u201d\u00a0 These are the kinds of fundamental questions AI developers will need to consider, or they run the risk of repeating (or ignoring) the same mistakes over and over again. Venkatasubramanian told me he found the story to be \u201caffirming\u201d in highlighting the need for \u201cthose in charge of governing these systems\u201d\u00a0 to \u201cask hard questions \u2026 starting with whether they should be used at all.\u201d But he also called the story \u201chumbling\u201d: \u201cEven with good intentions, and a desire to benefit from all the research on responsible AI, it\u2019s still possible to build systems that are fundamentally flawed, for reasons that go well beyond the details of the system constructions.\u201d\u00a0 To better understand this debate, read our full story here. And if you want more detail on how we ran our own bias tests after the city gave us unprecedented access to the Smart Check algorithm, check out the methodology over at Lighthouse. (For any Dutch speakers out there, here\u2019s the companion story in Trouw.) Thanks to the Pulitzer Center for supporting our reporting.\u00a0 This story originally appeared in\u00a0The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. hide"
  ]
}