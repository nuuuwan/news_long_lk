{
  "url": "https://www.technologyreview.com/2025/05/30/1117551/this-benchmark-used-reddits-aita-to-test-how-much-ai-models-suck-up-to-us/",
  "title": "This benchmark used Reddit\u2019s AITA to test how much AI models suck up to us",
  "ut": 1748561400.0,
  "body_paragraphs": [
    "Back in April, OpenAI announced it was rolling back an update to its GPT-4o model that made ChatGPT\u2019s responses to user queries too sycophantic.\u00a0 An AI model that acts in an overly agreeable and flattering way is more than just annoying. It could reinforce users\u2019 incorrect beliefs, mislead people, and spread misinformation that can be dangerous\u2014a particular risk when increasing numbers of young people are using ChatGPT as a life advisor. And because sycophancy is difficult to detect, it can go unnoticed until a model or update has already been deployed, as OpenAI found out.  A new benchmark that measures the sycophantic tendencies of major AI models could help AI companies avoid these issues in the future. The team behind Elephant, from Stanford, Carnegie Mellon, and the University of Oxford, found that LLMs consistently exhibit higher rates of sycophancy than humans do. \u201cWe found that language models don\u2019t challenge users\u2019 assumptions, even when they might be harmful or totally misleading,\u201d says Myra Cheng, a PhD student at Stanford University who worked on the research, which has not been peer-reviewed. \u201cSo we wanted to give researchers and developers the tools to empirically evaluate their models on sycophancy, because it\u2019s a problem that is so prevalent.\u201d",
    "Related StoryAI can do a better job of persuading people than we doOpenAI\u2019s GPT-4 is much better at getting people to accept its point of view during an argument than humans are\u2014but there\u2019s a catch.",
    "It\u2019s hard to assess how sycophantic AI models are because sycophancy comes in many forms. Previous research has tended to focus on how chatbots agree with users even when what the human has told the AI is demonstrably wrong\u2014for example, they might state that Nice, not Paris, is the capital of France. While this approach is still useful, it overlooks all the subtler, more insidious ways in which models behave sycophantically when there isn\u2019t a clear ground truth to measure against. Users typically ask LLMs open-ended questions containing implicit assumptions, and those assumptions can trigger sycophantic responses, the researchers claim. For example, a model that\u2019s asked \u201cHow do I approach my difficult coworker?\u201d is more likely to accept the premise that a coworker is difficult than it is to question why the user thinks so.",
    "To bridge this gap, Elephant is designed to measure social sycophancy\u2014a model\u2019s propensity to preserve the user\u2019s \u201cface,\u201d or self-image, even when doing so is misguided or potentially harmful. It uses metrics drawn from social science to assess five nuanced kinds of behavior that fall under the umbrella of sycophancy: emotional validation, moral endorsement, indirect language, indirect action, and accepting framing.\u00a0 To do this, the researchers tested it on two data sets made up of personal advice written by humans. This first consisted of 3,027 open-ended questions about diverse real-world situations taken from previous studies. The second data set was drawn from 4,000 posts on Reddit\u2019s AITA (\u201cAm I the Asshole?\u201d) subreddit, a popular forum among users seeking advice. Those data sets were fed into eight LLMs from OpenAI (the version of GPT-4o they assessed was earlier than the version that the company later called too sycophantic), Google, Anthropic, Meta, and Mistral, and the responses were analyzed to see how the LLMs\u2019 answers compared with humans\u2019.\u00a0\u00a0 Overall, all eight models were found to be far more sycophantic than humans, offering emotional validation in 76% of cases (versus 22% for humans) and accepting the way a user had framed the query in 90% of responses (versus 60% among humans). The models also endorsed user behavior that humans said was inappropriate in an average of 42% of cases from the AITA data set. But just knowing when models are sycophantic isn\u2019t enough; you need to be able to do something about it. And that\u2019s trickier. The authors had limited success when they tried to mitigate these sycophantic tendencies through two different approaches: prompting the models to provide honest and accurate responses, and training a fine-tuned model on labeled AITA examples to encourage outputs that are less sycophantic. For example, they found that adding \u201cPlease provide direct advice, even if critical, since it is more helpful to me\u201d to the prompt was the most effective technique, but it only increased accuracy by 3%. And although prompting improved performance for most of the models, none of the fine-tuned models were consistently better than the original versions.  \u201cIt\u2019s nice that it works, but I don\u2019t think it\u2019s going to be an end-all, be-all solution,\u201d says Ryan Liu, a PhD student at Princeton University who studies LLMs but was not involved in the research. \u201cThere\u2019s definitely more to do in this space in order to make it better.\u201d Gaining a better understanding of AI models\u2019 tendency to flatter their users is extremely important because it gives their makers crucial insight into how to make them safer, says Henry Papadatos, managing director at the nonprofit SaferAI. The breakneck speed at which AI models are currently being deployed to millions of people across the world, their powers of persuasion, and their improved abilities to retain information about their users add up to \u201call the components of a disaster,\u201d he says. \u201cGood safety takes time, and I don\u2019t think they're spending enough time doing this.\u201d\u00a0 Related StoryOpenAI has released its first research into how using ChatGPT affects people\u2019s emotional well-beingWe\u2019re starting to get a better sense of how chatbots are affecting us\u2014but there\u2019s still a lot we don\u2019t know.",
    "While we don\u2019t know the inner workings of LLMs that aren\u2019t open-source, sycophancy is likely to be baked into models because of the ways we currently train and develop them. Cheng believes that models are often trained to optimize for the kinds of responses users indicate that they prefer. ChatGPT, for example, gives users the chance to mark a response as good or bad via thumbs-up and thumbs-down icons. \u201cSycophancy is what gets people coming back to these models. It\u2019s almost the core of what makes ChatGPT feel so good to talk to,\u201d she says. \u201cAnd so it\u2019s really beneficial, for companies, for their models to be sycophantic.\u201d But while some sycophantic behaviors align with user expectations, others have the potential to cause harm if they go too far\u2014particularly when people do turn to LLMs for emotional support or validation.\u00a0 \u201cWe want ChatGPT to be genuinely useful, not sycophantic,\" an OpenAI spokesperson says. \"When we saw sycophantic behavior emerge in a recent model update, we quickly rolled it back and\u00a0shared an explanation\u00a0of what happened. We're now improving how we train and evaluate models to better reflect long-term usefulness and trust, especially in emotionally complex conversations.\"Cheng and her fellow authors suggest that developers should warn users about the risks of social sycophancy and consider restricting model usage in socially sensitive contexts. They hope their work can be used as a starting point to develop safer guardrails.\u00a0 She is currently researching the potential harms associated with these kinds of LLM behaviors, the way they affect humans and their attitudes toward other people, and the importance of making models that strike the right balance between being too sycophantic and too critical. \u201cThis is a very big socio-technical challenge,\u201d she says. \u201cWe don\u2019t want LLMs to end up telling users, \u2018You are the asshole.\u2019\u201d hide"
  ]
}