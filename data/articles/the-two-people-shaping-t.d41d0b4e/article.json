{
  "url": "https://www.technologyreview.com/2025/07/31/1120885/the-two-people-shaping-the-future-of-openais-research/",
  "title": "The two people shaping the future of OpenAI\u2019s research",
  "ut": 1753918608.0,
  "body_paragraphs": [
    "For the past couple of years, OpenAI has felt like a one-man brand. With his showbiz style and fundraising glitz, CEO Sam Altman overshadows all other big names on the firm\u2019s roster. Even his bungled ouster ended with him back on top\u2014and more famous than ever. But look past the charismatic frontman and you get a clearer sense of where this company is going. After all, Altman is not the one building the technology on which its reputation rests.\u00a0 That responsibility falls to OpenAI\u2019s twin heads of research\u2014chief research officer Mark Chen and chief scientist Jakub Pachocki. Between them, they share the role of making sure OpenAI stays one step ahead of powerhouse rivals like Google.  I sat down with Chen and Pachocki for an exclusive conversation during a recent trip the pair made to London, where OpenAI set up its first international office in 2023. We talked about how they manage the inherent tension between research and product. We also talked about why they think coding and math are the keys to more capable all-purpose models; what they really mean when they talk about AGI; and what happened to OpenAI\u2019s superalignment team, set up by the firm\u2019s cofounder and former chief scientist Ilya Sutskever to prevent a hypothetical superintelligence from going rogue, which disbanded soon after he quit.\u00a0 In particular, I wanted to get a sense of where their heads are at in the run-up to OpenAI\u2019s biggest product release in months: GPT-5.",
    "Reports are out that the firm\u2019s next-generation model will be launched in August. OpenAI\u2019s official line\u2014well, Altman\u2019s\u2014is that it will release GPT-5 \u201csoon.\u201d Anticipation is high. The leaps OpenAI made with GPT-3 and then GPT-4 raised the bar of what was thought possible with this technology. And yet delays to the launch of GPT-5 have fueled rumors that OpenAI has struggled to build a model that meets its own\u2014not to mention everyone else\u2019s\u2014expectations. But expectation management is part of the job for a company that for the last several years has set the agenda for the industry. And Chen and Pachocki set the agenda inside OpenAI.",
    "Twin peaks\u00a0 The firm\u2019s main London office is in St James\u2019s Park, a few hundred meters east of Buckingham Palace. But I met Chen and Pachocki in a conference room in a coworking space near King\u2019s Cross, which OpenAI keeps as a kind of pied-\u00e0-terre in the heart of London\u2019s tech neighborhood (Google DeepMind and Meta are just around the corner). OpenAI\u2019s head of research communications, Laurance Fauconnet, sat with an open laptop at the end of the table.\u00a0 Chen, who was wearing a maroon polo shirt, is clean-cut, almost preppy. He\u2019s media trained and comfortable talking to a reporter. (That\u2019s him flirting with a chatbot in the \u201cIntroducing GPT-4o\u201d video.) Pachocki, in a black elephant-logo tee, has more of a TV-movie hacker look. He stares at his hands a lot when he speaks. But the pair are a tighter double act than they first appear. Pachocki summed up their roles. Chen shapes and manages the research teams, he said. \u201cI am responsible for setting the research roadmap and establishing our long-term technical vision.\u201d Related StoryOpenAI is launching a version of ChatGPT for college studentsStudy Mode helps students walk through topics rather than just giving them answers. But there\u2019s a glaring problem.",
    "\u201cBut there\u2019s fluidity in the roles,\u201d Chen said. \u201cWe\u2019re both researchers, we pull on technical threads. Whatever we see that we can pull on and fix, that\u2019s what we do.\u201d  Chen joined the company in 2018 after working as a quantitative trader at the Wall Street firm Jane Street Capital, where he developed machine-learning models for futures trading. At OpenAI he spearheaded the creation of DALL-E, the firm\u2019s breakthrough generative image model. He then worked on adding image recognition to GPT\u20114 and led the development of Codex, the generative coding model that powers GitHub Copilot. Pachocki left an academic career in theoretical computer science to join OpenAI in 2017 and replaced Sutskever as chief scientist in 2024. Along with Sutskever, he is one of the key architects of OpenAI\u2019s so-called reasoning models\u2014especially o1 and o3\u2014which are designed to tackle complex tasks in science, math, and coding.\u00a0 When we met they were buzzing, fresh off the high of two new back-to-back wins for their company\u2019s technology. On July 16, one of OpenAI\u2019s large language models came in second in the AtCoder World Tour Finals, one of the world\u2019s most hardcore programming competitions. On July 19, OpenAI announced that one of its models had achieved gold-medal-level results on the 2025 International Math Olympiad, one of the world\u2019s most prestigious math contests.",
    "The math result made headlines, not only because of OpenAI\u2019s remarkable achievement, but because rival Google DeepMind revealed two days later that one of its models had achieved the same score in the same competition. Google DeepMind had played by the competition\u2019s rules and waited for its results to be checked by the organizers before making an announcement; OpenAI had in effect marked its own answers. For Chen and Pachocki, the result speaks for itself. Anyway, it\u2019s the programming win they\u2019re most excited about. \u201cI think that\u2019s quite underrated,\u201d Chen told me. A gold medal result in the International Math Olympiad puts you somewhere in the top 20 to 50 competitors, he said. But in the AtCoder contest OpenAI\u2019s model placed in the top two: \u201cTo break into a really different tier of human performance\u2014that\u2019s unprecedented.\u201d Ship, ship, ship! People at OpenAI still like to say they work at a research lab. But the company is very different from the one it was before the release of ChatGPT three years ago. The firm is now in a race with the biggest and richest technology companies in the world and valued at $300 billion. Envelope-pushing research and eye-catching demos no longer cut it. It needs to ship products and get them into people\u2019s hands\u2014and boy, it does.\u00a0 OpenAI has kept up a run of new releases\u2014putting out major updates to its GPT-4 series, launching a string of generative image and video models, and introducing the ability to talk to ChatGPT with your voice. Six months ago it kicked off a new wave of so-called reasoning models with its o1 release, soon followed by o3. And last week it released its browser-using agent Operator to the public. It now claims that more than 400 million people use its products every week and submit 2.5 billion prompts a day.\u00a0  OpenAI\u2019s incoming CEO of applications, Fidji Simo, plans to keep up the momentum. In a memo to the company, she told employees she is looking forward to \u201chelping get OpenAI\u2019s technologies into the hands of more people around the world,\u201d where they will \u201cunlock more opportunities for more people than any other technology in history.\u201d Expect the products to keep coming. I asked how OpenAI juggles open-ended research and product development. \u201cThis is something we have been thinking about for a very long time, long before ChatGPT,\u201d Pachocki said. \u201cIf we are actually serious about trying to build artificial general intelligence, clearly there will be so much that you can do with this technology along the way, so many tangents you can go down that will be big products.\u201d In other words, keep shaking the tree and harvest what you can. A talking point that comes up with OpenAI folks is that putting experimental models out into the world was a necessary part of research. The goal was to make people aware of how good this technology had become. \u201cWe want to educate people about what\u2019s coming so that we can participate in what will be a very hard societal conversation,\u201d Altman told me back in 2022. The makers of this strange new technology were also curious what it might be for: OpenAI was keen to get it into people\u2019s hands to see what they would do with it. Is that still the case? They answered at the same time. \u201cYeah!\u201d Chen said. \u201cTo some extent,\u201d Pachocki said. Chen laughed: \u201cNo, go ahead.\u201d",
    "\u201cI wouldn\u2019t say research iterates on product,\u201d said Pachocki. \u201cBut now that models are at the edge of the capabilities that can be measured by classical benchmarks and a lot of the long-standing challenges that we\u2019ve been thinking about are starting to fall, we\u2019re at the point where it really is about what the models can do in the real world.\u201d Like taking on humans in coding competitions. The person who beat OpenAI\u2019s model at this year\u2019s AtCoder contest, held in Japan, was a programmer named Przemys\u0142aw D\u0119biak, also known as Psyho. The contest was a puzzle-solving marathon in which competitors had 10 hours to find the most efficient way to solve a complex coding problem. After his win, Psyho posted on X: \u201cI\u2019m completely exhausted ... I\u2019m barely alive.\u201d",
    "Chen and Pachocki have strong ties to the world of competitive coding. Both have competed in international coding contests in the past and Chen coaches the USA Computing Olympiad team. I asked whether that personal enthusiasm for competitive coding colors their sense of how big a deal it is for a model to perform well at such a challenge. Related StoryInside OpenAI\u2019s empire: A conversation with Karen Hao",
    "In a Roundtables event for\u00a0MIT Technology Review\u00a0subscribers, the author of\u00a0Empire of AI\u00a0explains how everyone has a stake in AI\u2019s development.",
    "They both laughed. \u201cDefinitely,\u201d said Pachocki. \u201cSo: Psyho is kind of a legend. He\u2019s been the number one competitor for many years. He\u2019s also actually a friend of mine\u2014we used to compete together in these contests.\u201d D\u0119biak also used to work with Pachocki at OpenAI.  When Pachocki competed in coding contests he favored those that focused on shorter problems with concrete solutions. But D\u0119biak liked longer, open-ended problems without an obvious correct answer. \u201cHe used to poke fun at me, saying that the kind of contest I was into will be automated long before the ones he liked,\u201d Pachocki recalled. \u201cSo I was seriously invested in the performance of this model in this latest competition.\u201d Pachocki told me he was glued to the late-night livestream from Tokyo, watching his model come in second: \u201cPsyho resists for now.\u201d\u00a0 \u201cWe\u2019ve tracked the performance of LLMs on coding contests for a while,\u201d said Chen. \u201cWe\u2019ve watched them become better than me, better than Jakub. It feels something like Lee Sedol playing Go.\u201d",
    "Lee is the master Go player who lost a series of matches to DeepMind\u2019s game-playing model AlphaGo in 2016. The results stunned the international Go community and led Lee to give up professional play. Last year he told the New York Times: \u201cLosing to AI, in a sense, meant my entire world was collapsing ... I could no longer enjoy the game.\u201d And yet, unlike Lee, Chen and Pachocki are thrilled to be surpassed.\u00a0\u00a0\u00a0 But why should the rest of us care about these niche wins? It\u2019s clear that this technology\u2014designed to mimic and, ultimately, stand in for human intelligence\u2014is being built by people whose idea of peak intelligence is acing a math contest or holding your own against a legendary coder. Is it a problem that this view of intelligence is skewed toward the mathematical, analytical end of the scale? \u201cI mean, I think you are right that\u2014you know, selfishly, we do want to create models which accelerate ourselves,\u201d Chen told me. \u201cWe see that as a very fast factor to progress.\u201d\u00a0\u00a0 The argument researchers like Chen and Pachocki make is that math and coding are the bedrock for a far more general form of intelligence, one that can solve a wide range of problems in ways we might not have thought of ourselves. \u201cWe\u2019re talking about programming and math here,\u201d said Pachocki. \u201cBut it\u2019s really about creativity, coming up with novel ideas, connecting ideas from different places.\u201d",
    "Look at the two recent competitions: \u201cIn both cases, there were problems which required very hard, out-of-the-box thinking. Psyho spent half the programming competition thinking and then came up with a solution that was really novel and quite different from anything that our model looked at.\u201d \u201cThis is really what we\u2019re after,\u201d Pachocki continued. \u201cHow do we get models to discover this sort of novel insight? To actually advance our knowledge? I think they are already capable of that in some limited ways. But I think this technology has the potential to really accelerate scientific progress.\u201d\u00a0 I returned to the question about whether the focus on math and programming was a problem, conceding that maybe it\u2019s fine if what we\u2019re building are tools to help us do science. We don't necessarily want large language models to replace politicians and have people skills, I suggested. Chen pulled a face and looked up at the ceiling: \u201cWhy not?\u201d What\u2019s missing OpenAI was founded with a level of hubris that stood out even by Silicon Valley standards, boasting about its goal of building AGI back when talk of AGI still sounded kooky. OpenAI remains as gung-ho about AGI as ever, and it has done more than most to make AGI a mainstream multibillion-dollar concern. It\u2019s not there yet, though. I asked Chen and Pachocki what they think is missing. \u201cI think the way to envision the future is to really, deeply study the technology that we see today,\u201d Pachocki said. \u201cFrom the beginning, OpenAI has looked at deep learning as this very mysterious and clearly very powerful technology with a lot of potential. We\u2019ve been trying to understand its bottlenecks. What can it do? What can it not do?\u201d\u00a0\u00a0 At the current cutting edge, Chen said, are reasoning models, which break down problems into smaller, more manageable steps, but even they have limits: \u201cYou know, you have these models which know a lot of things but can\u2019t chain that knowledge together. Why is that? Why can\u2019t it do that in a way that humans can?\u201d OpenAI is throwing everything at answering that question.  \u201cWe are probably still, like, at the very beginning of this reasoning paradigm,\u201d Pachocki told me. \u201cReally, we are thinking about how to get these models to learn and explore over the long term and actually deliver very new ideas.\u201d Chen pushed the point home: \u201cI really don\u2019t consider reasoning done. We\u2019ve definitely not solved it. You have to read so much text to get a kind of approximation of what humans know.\u201d OpenAI won\u2019t say what data it uses to train its models or give details about their size and shape\u2014only that it is working hard to make all stages of the development process more efficient. Those efforts make them confident that so-called scaling laws\u2014which suggest that models will continue to get better the more compute you throw at them\u2014show no sign of breaking down. \u201cI don\u2019t think there\u2019s evidence that scaling laws are dead in any sense,\u201d Chen insisted. \u201cThere have always been bottlenecks, right? Sometimes they\u2019re to do with the way models are built. Sometimes they\u2019re to do with data. But fundamentally it\u2019s just about finding the research that breaks you through the current bottleneck.\u201d\u00a0 The faith in progress is unshakeable. I brought up something Pachocki had said about AGI in an interview with Nature in May: \u201cWhen I joined OpenAI in 2017, I was still among the biggest skeptics at the company.\u201d He looked doubtful.\u00a0 \u201cI\u2019m not sure I was skeptical about the concept,\u201d he said. \u201cBut I think I was\u2014\u201d He paused, looking at his hands on the table in front of him. \u201cWhen I joined OpenAI, I expected the timelines to be longer to get to the point that we are now.\u201d Related StoryOpenAI launches Operator\u2014an agent that can use a computer for youThe announcement confirms one of two rumors that circled the internet this week. The other was about superintelligence.",
    "\u201cThere\u2019s a lot of consequences of AI,\u201d he said. \u201cBut the one I think the most about is automated research. When we look at human history, a lot of it is about technological progress, about humans building new technologies. The point when computers can develop new technologies themselves seems like a very important, um, inflection point.  \u201cWe already see these models assist scientists. But when they are able to work on longer horizons\u2014when they\u2019re able to establish research programs for themselves\u2014the world will feel meaningfully different.\u201d For Chen, that ability for models to work by themselves for longer is key. \u201cI mean, I do think everyone has their own definitions of AGI,\u201d he said. \u201cBut this concept of autonomous time\u2014just the amount of time that the model can spend making productive progress on a difficult problem without hitting a dead end\u2014that\u2019s one of the big things that we\u2019re after.\u201d It\u2019s a bold vision\u2014and far beyond the capabilities of today\u2019s models. But I was nevertheless struck by how Chen and Pachocki made AGI sound almost mundane. Compare this with how Sutskever responded when I spoke to him 18 months ago. \u201cIt\u2019s going to be monumental, earth-shattering,\u201d he told me. \u201cThere will be a before and an after.\u201d Faced with the immensity of what he was building, Sutskever switched the focus of his career from designing better and better models to figuring out how to control a technology that he believed would soon be smarter than himself. Two years ago Sutskever set up what he called a superalignment team that he would co-lead with another OpenAI safety researcher, Jan Leike. The claim was that this team would funnel a full fifth of OpenAI\u2019s resources into figuring out how to control a hypothetical superintelligence. Today, most of the people on the superalignment team, including Sutskever and Leike, have left the company and the team no longer exists.\u00a0\u00a0\u00a0 When Leike quit, he said it was because the team had not been given the support he felt it deserved. He posted this on X: \u201cBuilding smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products.\u201d Other departing researchers shared similar statements. I asked Chen and Pachocki what they make of such concerns. \u201cA lot of these things are highly personal decisions,\u201d Chen said. \u201cYou know, a researcher can kind of, you know\u2014\u201d He started again. \u201cThey might have a belief that the field is going to evolve in a certain way and that their research is going to pan out and is going to bear fruit. And, you know, maybe the company doesn\u2019t reshape in the way that you want it to. It\u2019s a very dynamic field.\u201d \u201cA lot of these things are personal decisions,\u201d he repeated. \u201cSometimes the field is just evolving in a way that is less consistent with the way that you\u2019re doing research.\u201d But alignment, both of them insist, is now part of the core business rather than the concern of one specific team. According to Pachocki, these models don\u2019t work at all unless they work as you expect them to. There\u2019s also little desire to focus on aligning a hypothetical superintelligence with your objectives when doing so with existing models is already enough of a challenge. \u201cTwo years ago the risks that we were imagining were mostly theoretical risks,\u201d Pachocki said. \u201cThe world today looks very different, and I think a lot of alignment problems are now very practically motivated.\u201d Still, experimental technology is being spun into mass-market products faster than ever before. Does that really never lead to disagreements between the two of them? \u201cI am often afforded the luxury of really kind of thinking about the long term, where the technology is headed,\u201d Pachocki said. \u201cContending with the reality of the process\u2014both in terms of people and also, like, the broader company needs\u2014falls on Mark. It\u2019s not really a disagreement, but there is a natural tension between these different objectives and the different challenges that the company is facing that materializes between us.\u201d Chen jumped in: \u201cI think it\u2019s just a very delicate balance.\u201d\u00a0\u00a0 Correction: we have removed a line referring to an Altman message on X about GPT-5. We amended who was behind OpenAI's reasoning models to include reference to Sutskever. hide"
  ]
}