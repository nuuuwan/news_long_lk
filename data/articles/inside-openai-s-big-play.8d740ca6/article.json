{
  "url": "https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/",
  "title": "Inside OpenAI\u2019s big play for science",
  "ut": 1769414535.0,
  "body_paragraphs": [
    "In the three years since ChatGPT\u2019s explosive debut, OpenAI\u2019s technology has upended a remarkable range of everyday activities at home, at work, in schools\u2014anywhere people have a browser open or a phone out, which is everywhere. Now OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a whole new team, called OpenAI for Science, dedicated to exploring how its large language models could help scientists and tweaking its tools to support them.  The last couple of months have seen a slew of social media posts and academic publications in which mathematicians, physicists, biologists, and others have described how LLMs (and OpenAI\u2019s GPT-5 in particular) have helped them make a discovery or nudged them toward a solution they might otherwise have missed. In part, OpenAI for Science was set up to engage with this community. And yet OpenAI is also late to the party. Google DeepMind, the rival firm behind groundbreaking scientific models such as AlphaFold and AlphaEvolve, has had an AI-for-science team for years. (When I spoke to Google DeepMind\u2019s CEO and cofounder Demis Hassabis in 2023 about that team, he told me: \u201cThis is the reason I started DeepMind \u2026 In fact, it\u2019s why I\u2019ve worked my whole career in AI.\u201d)",
    "So why now? How does a push into science fit with OpenAI\u2019s wider mission? And what exactly is the firm hoping to achieve? I put these questions to Kevin Weil, a vice president at OpenAI who leads the new OpenAI for Science team, in an exclusive interview last week.",
    "On mission Weil is a product guy. He joined OpenAI a couple of years ago as chief product officer after being head of product at Twitter and Instagram. But he started out as a scientist. He got two-thirds of the way through a PhD in particle physics at Stanford University before ditching academia for the Silicon Valley dream. Weil is keen to highlight his pedigree: \u201cI thought I was going to be a physics professor for the rest of my life,\u201d he says. \u201cI still read math books on vacation.\u201d Asked how OpenAI for Science fits with the firm\u2019s existing lineup of white-collar productivity tools or the viral video app Sora, Weil recites the company mantra: \u201cThe mission of OpenAI is to try and build artificial general intelligence and, you know, make it beneficial for all of humanity.\u201d Just imagine the future impact this technology could have on science he says: New medicines, new materials, new devices. \u201cThink about it helping us understand the nature of reality, helping us think through open problems. Maybe the biggest, most positive impact we\u2019re going to see from AGI will actually be from its ability to accelerate science.\u201d He adds: \u201cWith GPT-5, we saw that becoming possible.\u201d\u00a0  As Weil tells it, LLMs are now good enough to be useful scientific collaborators. They can spitball ideas, suggest novel directions to explore, and find fruitful parallels between new problems and old solutions published in obscure journals decades ago or in foreign languages. Ask AIWhy it matters to you?BETAHere\u2019s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates\u2014it might get weirdAn industry I care about is.Tell me why it mattersLearn more about how we're using AI. That wasn\u2019t the case a year or so ago. Since it announced its first so-called reasoning model\u2014a type of LLM that can break down problems into multiple steps and work through them one by one\u2014in December 2024, OpenAI has been pushing the envelope of what the technology can do. Reasoning models have made LLMs far better at solving math and logic problems than they used to be.\u00a0\u201cYou go back a few years and we were all collectively mind-blown that the models could get an 800 on the SAT,\u201d says Weil. But soon LLMs were acing math competitions and solving graduate-level physics problems. Last year, OpenAI and Google DeepMind both announced that their LLMs had achieved gold-medal-level performance in the International Math Olympiad, one of the toughest math contests in the world. \u201cThese models are no longer just better than 90% of grad students,\u201d says Weil. \u201cThey\u2019re really at the frontier of human abilities.\u201d That\u2019s a huge claim, and it comes with caveats. Still, there\u2019s no doubt that GPT-5, which includes a reasoning model,  is a big improvement on GPT-4 when it comes to complicated problem-solving. Measured against an industry benchmark known as GPQA, which includes more than 400 multiple-choice questions that test PhD-level knowledge in biology, physics, and chemistry, GPT-4 scores 39%, well below the human-expert baseline of around 70%. According to OpenAI, GPT-5.2 (the latest update to the model, released in December) scores 92%.",
    "Overhyped The excitement is evident\u2014and perhaps excessive. In October, senior figures at OpenAI, including Weil, boasted on X that GPT-5 had found solutions to several unsolved math problems. Mathematicians were quick to point out that in fact what GPT-5 appeared to have done was dig up existing solutions in old research papers, including at least one written in German. That was still useful, but it wasn\u2019t the achievement OpenAI seemed to have claimed. Weil and his colleagues deleted their posts. Now Weil is more careful. It is often enough to find answers that exist but have been forgotten, he says: \u201cWe collectively stand on the shoulders of giants, and if LLMs can kind of accumulate that knowledge so that we don\u2019t spend time struggling on a problem that is already solved, that\u2019s an acceleration all of its own.\u201d He plays down the idea that LLMs are about to come up with a game-changing new discovery. \u201cI don\u2019t think models are there yet,\u201d he says. \u201cMaybe they\u2019ll get there. I\u2019m optimistic that they will.\u201d But, he insists, that\u2019s not the mission: \u201cOur mission is to accelerate science. And I don\u2019t think the bar for the acceleration of science is, like, Einstein-level reimagining of an entire field.\u201d  For Weil, the question is this: \u201cDoes science actually happen faster because scientists plus models can do much more, and do it more quickly, than scientists alone? I think we\u2019re already seeing that.\u201d In November, OpenAI published a series of anecdotal case studies contributed by scientists, both inside and outside the company, that illustrated how they had used GPT-5 and how it had helped. \u201cMost of the cases were scientists that were already using GPT-5 directly in their research and had come to us one way or another saying, \u2018Look at what I\u2019m able to do with these tools,\u2019\u201d says Weil. The key things that GPT-5 seems to be good at are finding references and connections to existing work that scientists were not aware of, which sometimes sparks new ideas; helping scientists sketch mathematical proofs; and suggesting ways for scientists to test hypotheses in the lab.\u00a0\u00a0 \u201cGPT 5.2 has read substantially every paper written in the last 30 years,\u201d says Weil. \u201cAnd it understands not just the field that a particular scientist is working in; it can bring together analogies from other, unrelated fields.\u201d",
    "\u201cThat\u2019s incredibly powerful,\u201d he continues. \u201cYou can always find a human collaborator in an adjacent field, but it\u2019s difficult to find, you know, a thousand collaborators in all thousand adjacent fields that might matter. And in addition to that, I can work with the model late at night\u2014it doesn\u2019t sleep\u2014and I can ask it 10 things in parallel, which is kind of awkward to do to a human.\u201d Solving problems Most of the scientists OpenAI reached out to back up Weil\u2019s position.",
    "Robert Scherrer, a professor of physics and astronomy at Vanderbilt University, only played around with ChatGPT for fun (\u201cI used to it rewrite the theme song for Gilligan\u2019s Island in the style of Beowulf, which it did very well,\u201d he tells me) until his Vanderbilt colleague Alex Lupsasca, a fellow physicist who now works at OpenAI, told him that GPT-5 had helped solve a problem he\u2019d been working on. Lupsasca gave Scherrer access to GPT-5 Pro, OpenAI\u2019s $200-a-month premium subscription. \u201cIt managed to solve a problem that I and my graduate student could not solve despite working on it for several months,\u201d says Scherrer.  It\u2019s not perfect, he says: \u201cGTP-5 still makes dumb mistakes. Of course, I do too, but the mistakes GPT-5 makes are even dumber.\u201d And yet it keeps getting better, he says: \u201cIf current trends continue\u2014and that\u2019s a big if\u2014I suspect that all scientists will be using LLMs soon.\u201d Derya Unutmaz, a professor of biology at the Jackson Laboratory, a nonprofit research institute, uses GPT-5 to brainstorm ideas, summarize papers, and plan experiments in his work studying the immune system. In the case study he shared with OpenAI, Unutmaz used GPT-5 to analyze an old data set that his team had previously looked at. The model came up with fresh insights and interpretations.\u00a0\u00a0 \u201cLLMs are already essential for scientists,\u201d he says. \u201cWhen you can complete analysis of data sets that used to take months, not using them is not an option anymore.\u201d Nikita Zhivotovskiy, a statistician at the University of California, Berkeley, says he has been using LLMs in his research since the first version of ChatGPT came out.",
    "Like Scherrer, he finds LLMs most useful when they highlight unexpected connections between his own work and existing results he did not know about. \u201cI believe that LLMs are becoming an essential technical tool for scientists, much like computers and the internet did before,\u201d he says. \u201cI expect a long-term disadvantage for those who do not use them.\u201d But he does not expect LLMs to make novel discoveries anytime soon. \u201cI have seen very few genuinely fresh ideas or arguments that would be worth a publication on their own,\u201d he says. \u201cSo far, they seem to mainly combine existing results, sometimes incorrectly, rather than produce genuinely new approaches.\u201d I also contacted a handful of scientists who are not connected to OpenAI. Andy Cooper, a professor of chemistry at the University of Liverpool and director of the Leverhulme Research Centre for Functional Materials Design, is less enthusiastic. \u201cWe have not found, yet, that LLMs are fundamentally changing the way that science is done,\u201d he says. \u201cBut our recent results suggest that they do have a place.\u201d",
    "Cooper is leading a project to develop a so-called AI scientist that can fully automate parts of the scientific workflow. He says that his team doesn\u2019t use LLMs to come up with ideas. But the tech is starting to prove useful as part of a wider automated system where an LLM can help direct robots, for example. \u201cMy guess is that LLMs might stick more in robotic workflows, at least initially, because I\u2019m not sure that people are ready to be told what to do by an LLM,\u201d says Cooper. \u201cI\u2019m certainly not.\u201d Making errors  LLMs may be becoming more and more useful, but caution is still key. In December, Jonathan Oppenheim, a scientist who works on quantum mechanics, called out a mistake that had made its way into a scientific journal. \u201cOpenAI leadership are promoting a paper in Physics Letters B where GPT-5 proposed the main idea\u2014possibly the first peer-reviewed paper where an LLM generated the core contribution,\u201d Oppenheim posted on X. \u201cOne small problem: GPT-5\u2019s idea tests the wrong thing.\u201d He continued: \u201cGPT-5 was asked for a test that detects nonlinear theories. It provided a test that detects nonlocal ones. Related-sounding, but different. It\u2019s like asking for a COVID test, and the LLM cheerfully hands you a test for chickenpox.\u201d It is clear that a lot of scientists are finding innovative and intuitive ways to engage with LLMs. It is also clear that the technology makes mistakes that can be so subtle even experts miss them. Part of the problem is the way ChatGPT can flatter you into letting down your guard. As Oppenheim put it: \u201cA core issue is that LLMs are being trained to validate the user, while science needs tools that challenge us.\u201d In an extreme case, one individual (who was not a scientist) was persuaded by ChatGPT into thinking for months that he\u2019d invented a new branch of mathematics. Of course, Weil is well aware of the problem of hallucination. But he insists that newer models are hallucinating less and less. Even so, focusing on hallucination might be missing the point, he says. \u201cOne of my teammates here, an ex math professor, said something that stuck with me,\u201d says Weil. \u201cHe said: \u2018When I\u2019m doing research, if I\u2019m bouncing ideas off a colleague, I\u2019m wrong 90% of the time and that\u2019s kind of the point. We\u2019re both spitballing ideas and trying to find something that works.\u2019\u201d \u201cThat\u2019s actually a desirable place to be,\u201d says Weil. \u201cIf you say enough wrong things and then somebody stumbles on a grain of truth and then the other person seizes on it and says, \u2018Oh, yeah, that\u2019s not quite right, but what if we\u2014\u2019 You gradually kind of find your trail through the woods.\u201d This is Weil\u2019s core vision for OpenAI for Science. GPT-5 is good, but it is not an oracle. The value of this technology is in pointing people in new directions, not coming up with definitive answers, he says. In fact, one of the things OpenAI is now looking at is making GPT-5 dial down its confidence when it delivers a response. Instead of saying Here\u2019s the answer, it might tell scientists: Here\u2019s something to consider. \u201cThat\u2019s actually something that we are spending a bunch of time on,\u201d says Weil. \u201cTrying to make sure that the model has some sort of epistemological humility.\u201d Watching the watchers Another thing OpenAI is looking at is how to use GPT-5 to fact-check GPT-5. It\u2019s often the case that if you feed one of GPT-5\u2019s answers back into the model, it will pick it apart and highlight mistakes. \u201cYou can kind of hook the model up as its own critic,\u201d says Weil. \u201cThen you can get a workflow where the model is thinking and then it goes to another model, and if that model finds things that it could improve, then it passes it back to the original model and says, \u2018Hey, wait a minute\u2014this part wasn\u2019t right, but this part was interesting. Keep it.\u2019 It\u2019s almost like a couple of agents working together and you only see the output once it passes the critic.\u201d What Weil is describing also sounds a lot like what Google DeepMind did with AlphaEvolve, a tool that wrapped the firms LLM, Gemini, inside a wider system that filtered out the good responses from the bad and fed them back in again to be improved on. Google DeepMind has used AlphaEvolve to solve several real-world problems. OpenAI faces stiff competition from rival firms, whose own LLMs can do most, if not all, of the things it claims for its own models. If that\u2019s the case, why should scientists use GPT-5 instead of Gemini or Anthropic\u2019s Claude, families of models that are themselves improving every year? Ultimately, OpenAI for Science may be as much an effort to plant a flag in new territory as anything else. The real innovations are still to come.\u00a0 \u201cI think 2026 will be for science what 2025 was for software engineering,\u201d says Weil. \u201cAt the beginning of 2025, if you were using AI to write most of your code, you were an early adopter. Whereas 12 months later, if you\u2019re not using AI to write most of your code, you\u2019re probably falling behind. We\u2019re now seeing those same early flashes for science as we did for code.\u201d He continues: \u201cI think that in a year, if you\u2019re a scientist and you\u2019re not heavily using AI, you\u2019ll be missing an opportunity to increase the quality and pace of your thinking.\u201d hide"
  ]
}