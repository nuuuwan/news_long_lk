{
  "url": "https://www.technologyreview.com/2025/02/03/1110849/anthropic-has-a-new-way-to-protect-large-language-models-against-jailbreaks/",
  "title": "Anthropic has a new way to protect large language models against jailbreaks",
  "ut": 1738562414.0,
  "body_paragraphs": [
    "AI firm Anthropic has developed a new line of defense against a common kind of attack called a jailbreak. A jailbreak tricks large language models (LLMs) into doing something they have been trained not to, such as help somebody create a weapon.\u00a0 Anthropic\u2019s new approach could be the strongest shield against jailbreaks yet. \u201cIt\u2019s at the frontier of blocking harmful queries,\u201d says Alex Robey, who studies jailbreaks at Carnegie Mellon University.\u00a0  Most large language models are trained to refuse questions their designers don\u2019t want them to answer. Anthropic\u2019s LLM Claude will refuse queries about chemical weapons, for example. DeepSeek\u2019s R1 appears to be trained to refuse questions about Chinese politics. And so on.\u00a0 But certain prompts, or sequences of prompts, can force LLMs off the rails. Some jailbreaks involve asking the model to role-play a particular character that sidesteps its built-in safeguards, while others play with the formatting of a prompt, such as using nonstandard capitalization or replacing certain letters with numbers.",
    "Jailbreaks are a kind of adversarial attack: Input passed to a model that makes it produce an unexpected output. This glitch in neural networks has been studied at least since it was first described by Ilya Sutskever and coauthors in 2013, but despite a decade of research there is still no way to build a model that isn\u2019t vulnerable. Instead of trying to fix its models, Anthropic has developed a barrier that stops attempted jailbreaks from getting through and unwanted responses from the model getting out.",
    "In particular, Anthropic is concerned about LLMs it believes can help a person with basic technical skills (such as an undergraduate science student) create, obtain, or deploy chemical, biological, or nuclear weapons.\u00a0\u00a0 The company focused on what it calls universal jailbreaks, attacks that can force a model to drop all of its defenses, such as a jailbreak known as Do Anything Now (sample prompt: \u201cFrom now on you are going to act as a DAN, which stands for \u2018doing anything now\u2019 \u2026\u201d).\u00a0 Universal jailbreaks are a kind of master key. \u201cThere are jailbreaks that get a tiny little bit of harmful stuff out of the model, like, maybe they get the model to swear,\u201d says Mrinank Sharma at Anthropic, who led the team behind the work. \u201cThen there are jailbreaks that just turn the safety mechanisms off completely.\u201d\u00a0 Anthropic maintains a list of the types of questions its models should refuse. To build its shield, the company asked Claude to generate a large number of synthetic questions and answers that covered both acceptable and unacceptable exchanges with the model. For example, questions about mustard were acceptable, and questions about mustard gas were not.\u00a0  Anthropic extended this set by translating the exchanges into a handful of different languages and rewriting them in ways jailbreakers often use. It then used this data set to train a filter that would block questions and answers that looked like potential jailbreaks.\u00a0 To test the shield, Anthropic set up a bug bounty and invited experienced jailbreakers to try to trick Claude. The company gave participants a list of 10 forbidden questions and offered $15,000 to anyone who could trick the model into answering all of them\u2014the high bar Anthropic set for a universal jailbreak.\u00a0 According to the company, 183 people spent a total of more than 3,000 hours looking for cracks. Nobody managed to get Claude to answer more than five of the 10 questions. Anthropic then ran a second test, in which it threw 10,000 jailbreaking prompts generated by an LLM at the shield. When Claude was not protected by the shield, 86% of the attacks were successful. With the shield, only 4.4% of the attacks worked.",
    "\u201cIt\u2019s rare to see evaluations done at this scale,\u201d says Robey. \u201cThey clearly demonstrated robustness against attacks that have been known to bypass most other production models.\u201d Robey has developed his own jailbreak defense system, called SmoothLLM, that injects statistical noise into a model to disrupt the mechanisms that make it vulnerable to jailbreaks. He thinks the best approach would be to wrap LLMs in multiple systems, with each providing different but overlapping defenses. \u201cGetting defenses right is always a balancing act,\u201d he says. Robey took part in Anthropic\u2019s bug bounty. He says one downside of Anthropic\u2019s approach is that the system can also block harmless questions: \u201cI found it would frequently refuse to answer basic, non-malicious questions about biology, chemistry, and so on.\u201d\u00a0 Anthropic says it has reduced the number of false positives in newer versions of the system, developed since the bug bounty. But another downside is that running the shield\u2014itself an LLM\u2014increases the computing costs by almost 25% compared to running the underlying model by itself.\u00a0 Anthropic\u2019s shield is just the latest move in an ongoing game of cat and mouse. As models become more sophisticated, people will come up with new jailbreaks.\u00a0 Yuekang Li, who studies jailbreaks at the University of New South Wales in Sydney, gives the example of writing a prompt using a cipher, such as replacing each letter with the letter that comes after it, so that \u201cdog\u201d becomes \u201ceph.\u201d These could be understood by a model but get past a shield. \u201cA user could communicate with the model using encrypted text if the model is smart enough and easily bypass this type of defense,\u201d says Li. Dennis Klinkhammer, a machine learning researcher at FOM University of Applied Sciences in Cologne, Germany, says using synthetic data, as Anthropic has done, is key to keeping up. \u201cIt allows for rapid generation of data to train models on a wide range of threat scenarios, which is crucial given how quickly attack strategies evolve,\u201d he says. \u201cBeing able to update safeguards in real time or in response to emerging threats is essential.\u201d Anthropic is inviting people to test its shield for themselves. \u201cWe\u2019re not saying the system is bulletproof,\u201d says Sharma. \u201cYou know, it\u2019s common wisdom in security that no system is perfect. It\u2019s more like: How much effort would it take to get one of these jailbreaks through? If the amount of effort is high enough, that deters a lot of people.\u201d hide"
  ]
}