{
  "url": "https://www.technologyreview.com/2024/07/05/1094703/ai-lie-detectors-are-better-than-humans-at-spotting-lies/",
  "title": "AI lie detectors are better than humans at spotting lies",
  "ut": 1720135800.0,
  "body_paragraphs": [
    "This article first appeared in The Checkup,\u00a0MIT Technology Review\u2019s\u00a0weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,\u00a0sign up here.\u00a0 Can you spot a liar? It\u2019s a question I imagine has been on a lot of minds lately, in the wake of various televised political debates. Research has shown that we\u2019re generally pretty bad at telling a truth from a lie.\u00a0Some believe that AI could help improve our odds, and do better than dodgy old fashioned techniques like polygraph tests. AI-based lie detection systems could one day be used to help us sift fact from fake news, evaluate claims, and potentially even spot fibs and exaggerations in job applications. The question is whether we will trust them. And if we should. Related StoryAI isn\u2019t great at decoding human emotions. So why are regulators targeting the tech?AI, emotion recognition, and Darwin",
    "In a recent study, Alicia von Schenk and her colleagues developed a tool that was significantly better than people at spotting lies. Von Schenk, an economist at the University of W\u00fcrzburg in Germany, and her team then ran some experiments to find out how people used it. In some ways, the tool was helpful\u2014the people who made use of it were better at spotting lies. But they also led people to make a lot more accusations. In their study published in the journal iScience, von Schenk and her colleagues asked volunteers to write statements about their weekend plans. Half the time, people were incentivized to lie; a believable yet untrue statement was rewarded with a small financial payout. In total, the team collected 1,536 statements from 768 people.\u00a0They then used 80% of these statements to train an algorithm on lies and truths, using Google\u2019s AI language model BERT. When they tested the resulting tool on the final 20% of statements, they found it could successfully tell whether a statement was true or false 67% of the time. That\u2019s significantly better than a typical human; we usually only get it right around half the time.\u00a0To find out how people might make use of AI to help them spot lies, von Schenk and her colleagues split 2,040 other volunteers into smaller groups and ran a series of tests.\u00a0One test revealed that when people are given the option to pay a small fee to use an AI tool that can help them detect lies\u2014and earn financial rewards\u2014they still aren\u2019t all that keen on using it. Only a third of the volunteers given that option decided to use the AI tool, possibly because they\u2019re skeptical of the technology, says von Schenk. (They might also be overly optimistic about their own lie-detection skills, she adds.)\u00a0But that one-third of people really put their trust in the technology. \u201cWhen you make the active choice to rely on the technology, we see that people almost always follow the prediction of the AI\u2026 they rely very much on its predictions,\u201d says von Schenk.",
    "Related StoryAI models can outperform humans in tests to identify mental statesLarge language models don\u2019t have a theory of mind the way humans do\u2014but they\u2019re getting better at tasks designed to measure it in humans.",
    "This reliance can shape our behavior. Normally, people tend to assume others are telling the truth. That was borne out in this study\u2014even though the volunteers knew half of the statements were lies, they only marked out 19% of them as such. But that changed when people chose to make use of the AI tool: the accusation rate rose to 58%.\u00a0In some ways, this is a good thing\u2014these tools can help us spot more of the lies we come across in our lives, like the misinformation we might come across on social media.\u00a0But it\u2019s not all good. It could also undermine trust, a fundamental aspect of human behavior that helps us form relationships. If the price of accurate judgements is the deterioration of social bonds, is it worth it?\u00a0And then there\u2019s the question of accuracy. In their study, von Schenk and her colleagues were only interested in creating a tool that was better than humans at lie detection. That isn\u2019t too difficult, given how terrible we are at it. But she also imagines a tool like hers being used to routinely assess the truthfulness of social media posts, or hunt for fake details in a job hunter\u2019s resume or interview responses. In cases like these, it\u2019s not enough for a technology to just be \u201cbetter than human\u201d if it\u2019s going to be making more accusations.\u00a0\u00a0Would we be willing to accept an accuracy rate of 80%, where only four out of every five assessed statements would be correctly interpreted as true or false? Would even 99% accuracy suffice? I\u2019m not sure.\u00a0It's worth remembering the fallibility of historical lie detection techniques. The polygraph was designed to measure heart rate and other signs of \u201carousal\u201d because it was thought some signs of stress were unique to liars. They\u2019re not. And we\u2019ve known that for a long time. That\u2019s why lie detector results are generally not admissible in US court cases. Despite that, polygraph lie detector tests have endured in some settings, and have caused plenty of harm when they\u2019ve been used to hurl accusations at people who fail them on reality TV shows.\u00a0Imperfect AI tools stand to have an even greater impact because they are so easy to scale, says von Schenk. You can only polygraph so many people in a day. The scope for AI lie detection is almost limitless by comparison.\u00a0\u201cGiven that we have so much fake news and disinformation spreading, there is a benefit to these technologies,\u201d says von Schenk. \u201cHowever, you really need to test them\u2014you need to make sure they are substantially better than humans.\u201d If an AI lie detector is generating a lot of accusations, we might be better off not using it at all, she says.  Now read the rest of The Checkup Read more from MIT Technology Review's archive AI lie detectors have also been developed to look for facial patterns of movement and \u201cmicrogestures\u201d associated with deception. As Jake Bittle puts it: \u201cthe dream of a perfect lie detector just won\u2019t die, especially when glossed over with the sheen of AI.\u201d\u00a0On the other hand, AI is also being used to generate plenty of disinformation. As of October last year, generative AI was already being used in at least 16 countries to \u201csow doubt, smear opponents, or influence public debate,\u201d as Tate Ryan-Mosley reported.\u00a0The way AI language models are developed can heavily influence the way that they work. As a result, these models have picked up different political biases, as my colleague Melissa Heikkil\u00e4 covered last year.\u00a0AI, like social media, has the potential for good or ill. In both cases, the regulatory limits we place on these technologies will determine which way the sword falls, argue Nathan E. Sanders and Bruce Schneier.\u00a0Chatbot answers are all made up. But there\u2019s a tool that can give a reliability score to large language model outputs, helping users work out how trustworthy they are. Or, as Will Douglas Heaven put it in an article published a few months ago, a BS-o-meter for chatbots. From around the web Scientists, ethicists and legal experts in the UK have published a new set of guidelines for research on synthetic embryos, or, as they call them, \u201cstem cell-based embryo models (SCBEMs).\u201d There should be limits on how long they are grown in labs, and they should not be transferred into the uterus of a human or animal, the guideline states. They also note that, if, in future, these structures look like they might have the potential to develop into a fetus, we should stop calling them \u201cmodels\u201d and instead refer to them as \u201cembryos.\u201dAntimicrobial resistance is already responsible for 700,000 deaths every year, and could claim 10 million lives per year by 2050. Overuse of broad spectrum antibiotics is partly to blame. Is it time to tax these drugs to limit demand? (International Journal of Industrial Organization)Spaceflight can alter the human brain, reorganizing gray and white matter and causing the brain to shift upwards in the skull. We need to better understand these effects, and the impact of cosmic radiation on our brains, before we send people to Mars. (The Lancet Neurology)The vagus nerve has become an unlikely star of social media, thanks to influencers who drum up the benefits of stimulating it. Unfortunately, the science doesn\u2019t stack up. (New Scientist)A hospital in Texas is set to become the first in the country to enable doctors to see their patients via hologram. Crescent Regional Hospital in Lancaster has installed Holobox\u2014a system that projects a life-sized hologram of a doctor for patient consultations. (ABC News) hide"
  ]
}