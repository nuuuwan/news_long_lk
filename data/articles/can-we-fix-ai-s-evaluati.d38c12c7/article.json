{
  "url": "https://www.technologyreview.com/2025/06/24/1119187/fix-ai-evaluation-crisis/",
  "title": "Can we fix AI\u2019s evaluation crisis?",
  "ut": 1750720830.0,
  "body_paragraphs": [
    "As a tech reporter I often get asked questions like \u201cIs DeepSeek actually better than ChatGPT?\u201d or \u201cIs the Anthropic model any good?\u201d If I don\u2019t feel like turning it into an hour-long seminar, I\u2019ll usually give the diplomatic answer: \u201cThey\u2019re both solid in different ways.\u201d Most people asking aren\u2019t defining \u201cgood\u201d in any precise way, and that\u2019s fair. It\u2019s human to want to make sense of something new and seemingly powerful. But that simple question\u2014Is this model good?\u2014is really just the everyday version of a much more complicated technical problem.  So far, the way we\u2019ve tried to answer that question is through benchmarks. These give models a fixed set of questions to answer and grade them on how many they get right. But just like exams like the SAT (an admissions test used by many US colleges), these benchmarks don\u2019t always reflect deeper abilities. Lately it feels as if a new AI model drops every week, and every time a company launches one, it comes with fresh scores showing it beating the capabilities of predecessors. On paper, everything appears to be getting better all the time. In practice, it\u2019s not so simple. Just as grinding for the SAT might boost your score without improving your critical thinking, models can be trained to optimize for benchmark results without actually getting smarter, as Russell Brandon explained in his piece for us. As OpenAI and Tesla AI veteran Andrej Karpathy recently put it, we\u2019re living through an evaluation crisis\u2014our scoreboard for AI no longer reflects what we really want to measure.Benchmarks have grown stale for a few key reasons. First, the industry has learned to \u201cteach to the test,\u201d training AI models to score well rather than genuinely improve. Second, widespread data contamination means models may have already seen the benchmark questions, or even the answers, somewhere in their training data. And finally, many benchmarks are simply maxed out. On popular tests like SuperGLUE, models have already reached or surpassed 90% accuracy, making further gains feel more like statistical noise than meaningful improvement. At that point, the scores stop telling us anything useful. That\u2019s especially true in high-skill domains like coding, reasoning, and complex STEM problem-solving.",
    "Related StoryHow to build a better AI benchmarkTo fix the way we test and measure models, AI is learning tricks from social science.",
    "However, there are a growing number of teams around the world trying to address the AI evaluation crisis.\u00a0 One result is a new benchmark called LiveCodeBench Pro. It draws problems from international algorithmic olympiads\u2014competitions for elite high school and university programmers where participants solve challenging problems without external tools. The top AI models currently manage only about 53% at first pass on medium-difficulty problems and 0% on the hardest ones. These are tasks where human experts routinely excel.",
    "Zihan Zheng, a junior at NYU and a North America finalist in competitive coding, led the project to develop LiveCodeBench Pro with a team of olympiad medalists. They\u2019ve published both the benchmark and a detailed study showing that top-tier models like GPT o4-mini-high and Google\u2019s Gemini 2.5 perform at a level comparable to the top 10% of human competitors. Across the board, Zheng observed a pattern: AI excels at making plans and executing tasks, but it struggles with nuanced algorithmic reasoning. \u201cIt shows that AI is still far from matching the best human coders,\u201d he says. LiveCodeBench Pro might define a new upper bar. But what about the floor? Earlier this month, a group of researchers from multiple universities argued that LLM agents should be evaluated primarily on the basis of their riskiness, not just how well they perform. In real-world, application-driven environments\u2014especially with AI agents\u2014unreliability, hallucinations, and brittleness are ruinous. One wrong move could spell disaster when money or safety are on the line.There are other new attempts to address the problem. Some benchmarks, like ARC-AGI, now keep part of their data set private to prevent AI models from being optimized excessively for the test, a problem called \u201coverfitting.\u201d Meta\u2019s Yann LeCun has created LiveBench, a dynamic benchmark where questions evolve every six months. The goal is to evaluate models not just on knowledge but on adaptability. Xbench, a Chinese benchmark project developed by HongShan Capital Group (formerly Sequoia China), is another one of these effort.\u00a0I just wrote about it in a story. Xbench was initially built in 2022\u2014right after ChatGPT\u2019s launch\u2014as an internal tool to evaluate models for investment research. Over time, the team expanded the system and brought in external collaborators. It just made parts of its question set publicly available last week.\u00a0 Xbench is notable for its dual-track design, which tries to bridge the gap between lab-based tests and real-world utility. The first track evaluates technical reasoning skills by testing a model\u2019s STEM knowledge and ability to carry out Chinese-language research. The second track aims to assess practical usefulness\u2014how well a model performs on tasks in fields like recruitment and marketing. For instance, one task asks an agent to identify five qualified battery engineer candidates; another has it match brands with relevant influencers from a pool of more than 800 creators.\u00a0 The team behind Xbench has big ambitions. They plan to expand its testing capabilities into sectors like finance, law, and design, and they plan to update the test set quarterly to avoid stagnation.\u00a0 This is something that I often wonder about, because a model\u2019s hardcore reasoning ability doesn\u2019t necessarily translate into a fun, informative, and creative experience. Most queries from average users are probably not going to be rocket science. There isn\u2019t much research yet on how to effectively evaluate a model\u2019s creativity, but I\u2019d love to know which model would be the best for creative writing or art projects. Human preference testing has also emerged as an alternative to benchmarks. One increasingly popular platform is LMarena, which lets users submit questions and compare responses from different models side by side\u2014and then pick which one they like best. Still, this method has its flaws. Users sometimes reward the answer that sounds more flattering or agreeable, even if it\u2019s wrong. That can incentivize \u201csweet-talking\u201d models and skew results in favor of pandering. AI researchers are beginning to realize\u2014and admit\u2014that the status quo of AI testing cannot continue. At the recent CVPR conference, NYU professor Saining Xie drew on historian James Carse\u2019s Finite and Infinite Games to critique the hypercompetitive culture of AI research. An infinite game, he noted, is open-ended\u2014the goal is to keep playing. But in AI, a dominant player often drops a big result, triggering a wave of follow-up papers chasing the same narrow topic. This race-to-publish culture puts enormous pressure on researchers and rewards speed over depth, short-term wins over long-term insight. \u201cIf academia chooses to play a finite game,\u201d he warned, \u201cit will lose everything.\u201d",
    "I found his framing powerful\u2014and maybe it applies to benchmarks, too. So, do we have a truly comprehensive scoreboard for how good a model is? Not really. Many dimensions\u2014social, emotional, interdisciplinary\u2014still evade assessment. But the wave of new benchmarks hints at a shift. As the field evolves, a bit of skepticism is probably healthy. This story originally appeared in\u00a0The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here. Correction: A previous version of the article mistakenly said 4o-mini instead of ChatGPT o4-mini-high, as a top performing model on LiveCodeBench Pro.  hide"
  ]
}