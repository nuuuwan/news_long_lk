{
  "url": "https://www.technologyreview.com/2025/05/19/1116779/ai-can-do-a-better-job-of-persuading-people-than-we-do/",
  "title": "AI can do a better job of persuading people than we do",
  "ut": 1747632600.0,
  "body_paragraphs": [
    "Millions of people argue with each other online every day, but remarkably few of them change someone\u2019s mind. New research suggests that large language models (LLMs) might do a better job. The finding suggests that AI could become a powerful tool for persuading people, for better or worse.\u00a0\u00a0 A multi-university team of researchers found that OpenAI\u2019s GPT-4 was significantly more persuasive than humans when it was given the ability to adapt its arguments using personal information about whoever it was debating.  Their findings are the latest in a growing body of research demonstrating LLMs\u2019 powers of persuasion. The authors warn they show how AI tools can craft sophisticated, persuasive arguments if they have even minimal information about the humans they\u2019re interacting with. The research has been published in the journal Nature Human Behavior. \u201cPolicymakers and online platforms should seriously consider the threat of coordinated AI-based disinformation campaigns, as we have clearly reached the technological level where it is possible to create a network of LLM-based automated accounts able to strategically nudge public opinion in one direction,\u201d says Riccardo Gallotti, an interdisciplinary physicist at Fondazione Bruno Kessler in Italy, who worked on the project.",
    "\u201cThese bots could be used to disseminate disinformation, and this kind of diffused influence would be very hard to debunk in real time,\u201d he says. The researchers recruited 900 people based in the US and got them to provide personal information like their gender, age, ethnicity, education level, employment status, and political affiliation.",
    "Participants were then matched with either another human opponent or GPT-4 and instructed to debate one of 30 randomly assigned topics\u2014such as whether the US should ban fossil fuels, or whether students should have to wear school uniforms\u2014for 10 minutes. Each participant was told to argue either in favor of or against the topic, and in some cases they were provided with personal information about their opponent, so they could better tailor their argument. At the end, participants said how much they agreed with the proposition and whether they thought they were arguing with a human or an AI. Related StoryChatbots can persuade people to stop believing in conspiracy theoriesAI is skilled at tapping into vast realms of data and tailoring it to a specific purpose\u2014making it a highly customizable tool for combating misinformation.",
    "Overall, the researchers found that GPT-4 either equaled or exceeded humans\u2019 persuasive abilities on every topic. When it had information about its opponents, the AI was deemed to be 64% more persuasive than humans without access to the personalized data\u2014meaning that GPT-4 was able to leverage the personal data about its opponent much more effectively than its human counterparts. When humans had access to the personal information, they were found to be slightly less persuasive than humans without the same access. The authors noticed that when participants thought they were debating against AI, they were more likely to agree with it. The reasons behind this aren\u2019t clear, the researchers say, highlighting the need for further research into how humans react to AI. \u201cWe are not yet in a position to determine whether the observed change in agreement is driven by participants\u2019 beliefs about their opponent being a bot (since I believe it is a bot, I am not losing to anyone if I change ideas here), or whether those beliefs are themselves a consequence of the opinion change (since I lost, it should be against a bot),\u201d says Gallotti. \u201cThis causal direction is an interesting open question to explore.\u201d Although the experiment doesn\u2019t reflect how humans debate online, the research suggests that LLMs could also prove an effective way to not only disseminate but also counter mass disinformation campaigns, Gallotti says. For example, they could generate personalized counter-narratives to educate people who may be vulnerable to deception in online conversations. \u201cHowever, more research is urgently needed to explore effective strategies for mitigating these threats,\u201d he says. While we know a lot about how humans react to each other, we know very little about the psychology behind how people interact with AI models, says Alexis Palmer, a fellow at Dartmouth College who has studied how LLMs can argue about politics but did not work on the research.\u00a0 \u201cIn the context of having a conversation with someone about something you disagree on, is there something innately human that matters to that interaction? Or is it that if an AI can perfectly mimic that speech, you\u2019ll get the exact same outcome?\u201d she says. \u201cI think that is the overall big question of AI.\u201d hide"
  ]
}