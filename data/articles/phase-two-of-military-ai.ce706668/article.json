{
  "url": "https://www.technologyreview.com/2025/04/15/1115078/phase-two-of-military-ai-has-arrived/",
  "title": "Phase two of military AI has arrived",
  "ut": 1744673400.0,
  "body_paragraphs": [
    "Last week, I spoke with two US Marines who spent much of last year deployed in the Pacific, conducting training exercises from South Korea to the Philippines. Both were responsible for analyzing surveillance to warn their superiors about possible threats to the unit. But this deployment was unique: For the first time, they were using generative AI to scour intelligence, through a chatbot interface similar to ChatGPT.\u00a0 As I wrote in my new story, this experiment is the latest evidence of the Pentagon\u2019s push to use generative AI\u2014tools that can engage in humanlike conversation\u2014throughout its ranks, for tasks including surveillance. Consider this phase two of the US military\u2019s AI push, where phase one began back in 2017 with older types of AI, like computer vision to analyze drone imagery. Though this newest phase began under the Biden administration, there\u2019s fresh urgency as Elon Musk\u2019s DOGE and Secretary of Defense Pete Hegseth push loudly for AI-fueled efficiency.\u00a0  As I also write in my story, this push raises alarms from some AI safety experts about whether large language models are fit to analyze subtle pieces of intelligence in situations with high geopolitical stakes. It also accelerates the US toward a world where AI is not just analyzing military data but suggesting actions\u2014for example, generating lists of targets. Proponents say this promises greater accuracy and fewer civilian deaths, but many human rights groups argue the opposite.\u00a0 With that in mind, here are three open questions to keep your eye on as the US military, and others around the world, bring generative AI to more parts of the so-called \u201ckill chain.\u201d",
    "What are the limits of \u201chuman in the loop\u201d? Talk to as many defense-tech companies as I have and you\u2019ll hear one phrase repeated quite often: \u201chuman in the loop.\u201d It means that the AI is responsible for particular tasks, and humans are there to check its work. It\u2019s meant to be a safeguard against the most dismal scenarios\u2014AI wrongfully ordering a deadly strike, for example\u2014but also against more trivial mishaps. Implicit in this idea is an admission that AI will make mistakes, and a promise that humans will catch them. But the complexity of AI systems, which pull from thousands of pieces of data, make that a herculean task for humans, says Heidy Khlaaf, who is chief AI scientist at the AI Now Institute, a research organization, and previously led safety audits for AI-powered systems.",
    "\u201c\u2018Human in the loop\u2019 is not always a meaningful mitigation,\u201d she says. When an AI model relies on thousands of data points to draw conclusions, \u201cit wouldn\u2019t really be possible for a human to sift through that amount of information to determine if the AI output was erroneous.\u201d As AI systems rely on more and more data, this problem scales up.\u00a0 Is AI making it easier or harder to know what should be classified? In the Cold War era of US military intelligence, information was captured through covert means, written up into reports by experts in Washington, and then stamped \u201cTop Secret,\u201d with access restricted to those with proper clearances. The age of big data, and now the advent of generative AI to analyze that data, is upending the old paradigm in lots of ways. One specific problem is called classification by compilation. Imagine that hundreds of unclassified documents all contain separate details of a military system. Someone who managed to piece those together could reveal important information that on its own would be classified. For years, it was reasonable to assume that no human could connect the dots, but this is exactly the sort of thing that large language models excel at.\u00a0 With the mountain of data growing each day, and then AI constantly creating new analyses, \u201cI don\u2019t think anyone\u2019s come up with great answers for what the appropriate classification of all these products should be,\u201d says Chris Mouton, a senior engineer for RAND, who recently tested how well suited generative AI is for intelligence and analysis. Underclassifying is a US security concern, but lawmakers have also criticized the Pentagon for overclassifying information.\u00a0 Related StoryOpenAI\u2019s new defense contract completes its military pivotA new partnership with Anduril, announced today, will deploy AI on the battlefield. It represents an overhaul of the company\u2019s position in just a year.",
    "The defense giant Palantir is positioning itself to help, by offering its AI tools to determine whether a piece of data should be classified or not. It\u2019s also working with Microsoft on AI models that would train on classified data.\u00a0 How high up the decision chain should AI go? Zooming out for a moment, it\u2019s worth noting that the US military\u2019s adoption of AI has in many ways followed consumer patterns. Back in 2017, when apps on our phones were getting good at recognizing our friends in photos, the Pentagon launched its own computer vision effort, called Project Maven, to analyze drone footage and identify targets. Now, as large language models enter our work and personal lives through interfaces such as ChatGPT, the Pentagon is tapping some of these models to analyze surveillance.\u00a0 So what\u2019s next? For consumers, it\u2019s agentic AI, or models that can not just converse with you and analyze information but go out onto the internet and perform actions on your behalf. It\u2019s also personalized AI, or models that learn from your private data to be more helpful.",
    "All signs point to the prospect that military AI models will follow this trajectory as well. A report published in March from Georgetown\u2019s Center for Security and Emerging Technology found a surge in military adoption of AI to assist in decision-making. \u201cMilitary commanders are interested in AI\u2019s potential to improve decision-making, especially at the operational level of war,\u201d the authors wrote. In October, the Biden administration released its national security memorandum on AI, which provided some safeguards for these scenarios. This memo hasn\u2019t been formally repealed by the Trump administration, but President Trump has indicated that the race for competitive AI in the US needs more innovation and less oversight. Regardless, it\u2019s clear that AI is quickly moving up the chain not just to handle administrative grunt work, but to assist in the most high-stakes, time-sensitive decisions.\u00a0 I\u2019ll be following these three questions closely. If you have information on how the Pentagon might be handling these questions, please reach out via Signal at jamesodonnell.22.\u00a0 This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here.  hide"
  ]
}