{
  "url": "https://www.technologyreview.com/2025/07/18/1120466/a-major-ai-training-data-set-contains-millions-of-examples-of-personal-data/",
  "title": "A major AI training data set contains millions of examples of personal data",
  "ut": 1752809906.0,
  "body_paragraphs": [
    "Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found. Thousands of images\u2014including identifiable faces\u2014were found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool\u2019s data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions. The study that details the breach was published on arXiv earlier this month.  The bottom line, says William Agnew, a postdoctoral fellow in AI ethics at Carnegie Mellon University and one of the coauthors, is that \u201canything you put online can [be] and probably has been scraped.\u201d The researchers found thousands of instances of validated identity documents\u2014including images of credit cards, driver\u2019s licenses, passports, and birth certificates\u2014as well as over 800 validated job application documents (including r\u00e9sum\u00e9s and cover letters), which were confirmed through LinkedIn and other web searches as being associated with real people. (In many more cases, the researchers did not have time to validate the documents or were unable to because of issues like image clarity.)",
    "A number of the r\u00e9sum\u00e9s disclosed sensitive information including disability status, the results of background checks, birth dates and birthplaces of dependents, and race. When r\u00e9sum\u00e9s were linked to people with online presences, researchers also found contact information, government identifiers, sociodemographic information, face photographs, home addresses, and the contact information of other people (like references).  Examples of identity-related documents found in CommonPool\u2019s small-scale data set show a credit card, a Social Security number, and a driver\u2019s license. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals.COURTESY OF THE RESEARCHERS   When it was released in 2023, DataComp CommonPool, with its 12.8 billion data samples, was the largest existing data set of publicly available image-text pairs, which are often used to train generative text-to-image models. While its curators said that CommonPool was intended for academic research, its license does not prohibit commercial use as well.",
    "CommonPool was created as a follow-up to the LAION-5B data set, which was used to train models including Stable Diffusion and Midjourney. It draws on the same data source: web scraping done by the nonprofit Common Crawl between 2014 and 2022.\u00a0 While commercial models often do not disclose what data sets they are trained on, the shared data sources of DataComp CommonPool and LAION-5B mean that the data sets are similar, and that the same personally identifiable information likely appears in LAION-5B, as well as in other downstream models trained on CommonPool data. CommonPool researchers did not respond to emailed questions. And since DataComp CommonPool has been downloaded more than 2 million times over the past two years, it is likely that \u201cthere [are]many downstream models that are all trained on this exact data set,\u201d says Rachel Hong, a PhD student in computer science at the University of Washington and the paper\u2019s lead author. Those would duplicate similar privacy risks. Good intentions are not enough \u201cYou can assume that any large-scale web-scraped data always contains content that shouldn\u2019t be there,\u201d says Abeba Birhane, a cognitive scientist and tech ethicist who leads Trinity College Dublin\u2019s AI Accountability Lab\u2014whether it\u2019s personally identifiable information (PII), child sexual abuse imagery, or hate speech (which Birhane\u2019s own research into LAION-5B has found).\u00a0  Indeed, the curators of DataComp CommonPool were themselves aware it was likely that PII would appear in the data set and did take some measures to preserve privacy, including automatically detecting and blurring faces. But in their limited data set, Hong\u2019s team found and validated over 800 faces that the algorithm had missed, and they estimated that overall, the algorithm had missed 102 million faces in the entire data set. On the other hand, they did not apply filters that could have recognized known PII character strings, like emails or Social Security numbers.\u00a0 \u201cFiltering is extremely hard to do well,\u201d says Agnew. \u201cThey would have had to make very significant advancements in PII detection and removal that they haven\u2019t made public to be able to effectively filter this.\u201d\u00a0\u00a0  Examples of r\u00e9sum\u00e9 documents and personal disclosures found in CommonPool\u2019s small-scale data set. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals. Image courtesy of the researchers.COURTESY OF THE RESEARCHERS   There are other privacy issues that the face blurring doesn\u2019t address. While the blurring filter is automatically applied, it is optional and can be removed. Additionally, the captions that often accompany the photos, as well as the photos\u2019 metadata, often contain even more personal information, such as names and exact locations. Another privacy mitigation measure comes from Hugging Face, a platform that distributes training data sets and hosts CommonPool, which integrates with a tool that theoretically allows people to search for and remove their own information from a data set. But as the researchers note in their paper, this would require people to know that their data is there to start with. When asked for comment, Florent Daudens of Hugging Face said that \u201cmaximizing the privacy of data subjects across the AI ecosystem takes a multilayered approach, which includes but is not limited to the widget mentioned,\u201d and that the platform is \u201cworking with our community of users to move the needle in a more privacy-grounded direction.\u201d",
    "In any case, just getting your data removed from one data set probably isn\u2019t enough. \u201cEven if someone finds out their data was used in a training data sets and \u2026 exercises their right to deletion, technically the law is unclear about what that means,\u201d \u00a0says Tiffany Li, an associate professor of law at the University of San Francisco School of Law. \u201cIf the organization only deletes data from the training data sets\u2014but does not delete or retrain the already trained model\u2014then the harm will nonetheless be done.\u201d The bottom line, says Agnew, is that \u201cif you web-scrape, you\u2019re going to have private data in there. Even if you filter, you\u2019re still going to have private data in there, just because of the scale of this. And that\u2019s something that we [machine-learning researchers], as a field, really need to grapple with.\u201d Reconsidering consent CommonPool was built on web data scraped between 2014 and 2022, meaning that many of the images likely date to before 2020, when ChatGPT was released. So even if it\u2019s theoretically possible that some people consented to having their information publicly available to anyone on the web, they could not have consented to having their data used to train large AI models that did not yet exist. Related StoryFour ways to protect your art from AI\u00a0Fight back against tech companies that use your work to train their AI systems without your consent.",
    "And with web scrapers often scraping data from each other, an image that was originally uploaded by the owner to one specific location would often find its way into other image repositories. \u201cI might upload something onto the internet, and then \u2026 a year or so later, [I] want to take it down, but then that [removal] doesn\u2019t necessarily do anything anymore,\u201d says Agnew. The researchers also found numerous examples of children\u2019s personal information, including depictions of birth certificates, passports, and health status, but in contexts suggesting that they had been shared for limited purposes. \u201cIt really illuminates the original sin of AI systems built off public data\u2014it\u2019s extractive, misleading, and dangerous to people who have been using the internet with one framework of risk, never assuming it would all be hoovered up by a group trying to create an image generator,\u201d says Ben Winters, the director of AI and privacy at the Consumer Federation of America. Finding a policy that fits Ultimately, the paper calls for the machine-learning community to rethink the common practice of indiscriminate web scraping and also lays out the possible violations of current privacy laws represented by the existence of PII in massive machine-learning data sets, as well as the limitations of those laws\u2019 ability to protect privacy. \u201cWe have the GDPR in Europe, we have the CCPA in California, but there\u2019s still no federal data protection law in America, which also means that different Americans have different rights protections,\u201d says Marietje Schaake, a Dutch lawmaker turned tech policy expert who currently serves as a fellow at Stanford\u2019s Cyber Policy Center.",
    "Besides, these privacy laws apply to companies that meet certain criteria for size and other characteristics. They do not necessarily apply to researchers like those who were responsible for creating and curating DataComp CommonPool. And even state laws that do address privacy, like California\u2019s consumer privacy act, have carve-outs for \u201cpublicly available\u201d information. Machine-learning researchers have long operated on the principle that if it\u2019s available on the internet, then it is public and no longer private information, but Hong, Agnew, and their colleagues hope that their research challenges this assumption.",
    "\u201cWhat we found is that \u2018publicly available\u2019 includes a lot of stuff that a lot of people might consider private\u2014r\u00e9sum\u00e9s, photos, credit card numbers, various IDs, news stories from when you were a child, your family blog. These are probably not things people want to just be used anywhere, for anything,\u201d says Hong.\u00a0\u00a0 Hopefully, Schaake says, this research \u201cwill raise alarm bells and create change.\u201d\u00a0 This article previously misstated Tiffany Li's affiliation. This has been fixed.  hide"
  ]
}