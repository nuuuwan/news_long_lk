{
  "url": "https://www.technologyreview.com/2025/11/24/1128051/the-state-of-ai-chatbot-companions-and-the-future-of-our-privacy/",
  "title": "The State of AI: Chatbot companions and the future of our privacy",
  "ut": 1763964000.0,
  "body_paragraphs": [
    "Welcome back to The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution reshaping global power. In this week's conversation MIT Technology Review\u2019s senior reporter for features and investigations, Eileen Guo, and FT tech correspondent Melissa Heikkil\u00e4 discuss the privacy implications of our new reliance on chatbots.   Eileen Guo writes: Even if you don\u2019t have an AI friend yourself, you probably know someone who does. A recent study found that one of the top uses of generative AI is companionship: On platforms like Character.AI, Replika, or Meta AI, people can create personalized chatbots to pose as the ideal friend, romantic partner, parent, therapist, or any other persona they can dream up.",
    "It\u2019s wild how easily people say these relationships can develop. And multiple studies have found that the more conversational and human-like an AI chatbot is, the more likely it is that we\u2019ll trust it and be influenced by it. This can be dangerous, and the chatbots have been accused of pushing some people toward harmful behaviors\u2014including, in a few extreme examples, suicide.\u00a0 Some state governments are taking notice and starting to regulate companion AI. New York requires AI companion companies to create safeguards and report expressions of suicidal ideation, and last month California passed a more detailed bill requiring AI companion companies to protect children and other vulnerable groups.",
    "But tellingly, one area the laws fail to address is user privacy. This is despite the fact that AI companions, even more so than other types of generative AI, depend on people to share deeply personal information\u2014from their day-to-day-routines, innermost thoughts, and questions they might not feel comfortable asking real people. After all, the more users tell their AI companions, the better the bots become at keeping them engaged. This is what MIT researchers Robert Mahari and Pat Pataranutaporn called \u201caddictive intelligence\u201d in an op-ed we published last year, warning that the developers of AI companions make \u201cdeliberate design choices ... to maximize user engagement.\u201d\u00a0 Related StoryHow AGI became the most consequential conspiracy theory of our timeRead next Ultimately, this provides AI companies with something incredibly powerful, not to mention lucrative: a treasure trove of conversational data that can be used to further improve their LLMs. Consider how the venture capital firm Andreessen Horowitz explained it in 2023:\u00a0  \"Apps such as Character.AI, which both control their models and own the end customer relationship, have a tremendous opportunity to\u00a0 generate market value in the emerging AI value stack. In a world where data is limited, companies that can create a magical data feedback loop by connecting user engagement back into their underlying model to continuously improve their product will be among the biggest winners that emerge from this ecosystem.\" This personal information is also incredibly valuable to marketers and data brokers. Meta recently announced that it will deliver ads through its AI chatbots. And research conducted this year by the security company Surf Shark found that four out of the five AI companion apps it looked at in the Apple App Store were collecting data such as user or device IDs, which can be combined with third-party data to create profiles for targeted ads. (The only one that said it did not collect data for tracking services was Nomi, which told me earlier this year that it would not \u201ccensor\u201d chatbots from giving explicit suicide instructions.)\u00a0 All of this means that the privacy risks posed by these AI companions are, in a sense, required: They are a feature, not a bug. And we haven\u2019t even talked about the additional security risks presented by the way AI chatbots collect and store so much personal information in one place.\u00a0 So, is it possible to have prosocial and privacy-protecting AI companions? That\u2019s an open question.",
    "What do you think, Melissa, and what is top of mind for you when it comes to privacy risks from AI companions? And do things look any different in Europe?\u00a0 Ask AIWhy it matters to you?BETAHere\u2019s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates\u2014it might get weirdA location I care about is.Tell me why it mattersLearn more about how we're using AI. Melissa Heikkil\u00e4 replies: Thanks, Eileen. I agree with you. If social media was a privacy nightmare, then AI chatbots put the problem on steroids.\u00a0 In many ways, an AI chatbot creates what feels like a much more intimate interaction than a Facebook page. The conversations we have are only with our computers, so there is little risk of your uncle or your crush ever seeing what you write. The AI companies building the models, on the other hand, see everything.\u00a0  Companies are optimizing their AI models for engagement by designing them to be as human-like as possible. But AI developers have several other ways to keep us hooked. The first is sycophancy, or the tendency for chatbots to be overly agreeable.\u00a0 This feature stems from the way the language model behind the chatbots is trained using reinforcement learning. Human data labelers rate the answers generated by the model as either acceptable or not. This teaches the model how to behave.\u00a0 Because people generally like answers that are agreeable, such responses are weighted more heavily in training.\u00a0 Related StoryOpenAI\u2019s new LLM exposes the secrets of how AI really worksRead next AI companies say they use this technique because it helps models become more helpful. But it creates a perverse incentive.",
    "After encouraging us to pour our hearts out to chatbots, companies from Meta to OpenAI are now looking to monetize these conversations. OpenAI recently told us it was looking at a number of ways to meet $1 trillion spending pledges, which included advertising and shopping features.\u00a0 AI models are already incredibly persuasive. Researchers at the UK\u2019s AI Security Institute have shown that they are far more skilled than humans at persuading people to change their minds on politics, conspiracy theories, and vaccine skepticism. They do this by generating large amounts of relevant evidence and communicating it in an effective and understandable way.",
    "This feature, paired with their sycophancy and a wealth of personal data, could be a powerful tool for advertisers\u2014one that is more manipulative than anything we have seen before.\u00a0 By default, chatbot users are opted in to data collection. Opt-out policies place the onus on users to understand the implications of sharing their information. It\u2019s also unlikely that data already used in training will be removed.\u00a0  We are all part of this phenomenon whether we want to be or not. Social media platforms from Instagram to LinkedIn now use our personal data to train generative AI models.\u00a0 Companies are sitting on treasure troves that consist of our most intimate thoughts and preferences, and language models are very good at picking up on subtle hints in language that could help advertisers profile us better by inferring our age, location, gender, and income level. We are being sold the idea of an omniscient AI digital assistant, a superintelligent confidante. In return, however, there is a very real risk that our information is about to be sent to the highest bidder once again. Eileen responds:",
    "I think the comparison between AI companions and social media is both apt and concerning.\u00a0 As Melissa highlighted, the privacy risks presented by AI chatbots aren\u2019t new\u2014they just \u201cput the [privacy] problem on steroids.\u201d AI companions are more intimate and even better optimized for engagement than social media, making it more likely that people will offer up more personal information. Here in the US, we are far from solving the privacy issues already presented by social networks and the internet\u2019s ad economy, even without the added risks of AI. And without regulation, the companies themselves are not following privacy best practices either. One recent study found that the major AI models train their LLMs on user chat data by default unless users opt out, while several don\u2019t offer opt-out mechanisms at all.",
    "In an ideal world, the greater risks of companion AI would give more impetus to the privacy fight\u2014but I don\u2019t see any evidence this is happening.\u00a0 Further reading\u00a0 FT reporters peer under the hood of OpenAI\u2019s five-year business plan as it tries to meet its vast $1 trillion spending pledges.\u00a0 Is it really such a problem if AI chatbots tell people what they want to hear? This FT feature asks what\u2019s wrong with sycophancy\u00a0 In a recent print issue of MIT Technology Review, Rhiannon Williams spoke to a number of people about the types of relationships they are having with AI chatbots. Eileen broke the story for MIT Technology Review about a chatbot that was encouraging some users to kill themselves. hide"
  ]
}