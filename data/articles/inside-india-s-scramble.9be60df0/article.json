{
  "url": "https://www.technologyreview.com/2025/07/04/1119705/inside-indias-scramble-for-ai-independence/",
  "title": "Inside India\u2019s scramble for AI independence",
  "ut": 1751582707.0,
  "body_paragraphs": [
    "In Bengaluru, India, Adithya Kolavi felt a mix of excitement and validation as he watched DeepSeek unleash its disruptive language model on the world earlier this year. The Chinese technology rivaled the best of the West in terms of benchmarks, but it had been built with far less capital in far less time.\u00a0 \u201cI thought: \u2018This is how we disrupt with less,\u2019\u201d says Kolavi, the 20-year-old founder of the Indian AI startup CognitiveLab. \u201cIf DeepSeek could do it, why not us?\u201d\u00a0  But for Abhishek Upperwal, founder of Soket AI Labs and architect of one of India\u2019s earliest efforts to develop a foundation model, the moment felt more bittersweet.\u00a0 Upperwal\u2019s model, called Pragna-1B, had struggled to stay afloat with tiny grants while he watched global peers raise millions. The multilingual model had a relatively modest 1.25 billion parameters and was designed to reduce the \u201clanguage tax,\u201d the extra costs that arise because India\u2014unlike the US or even China\u2014has a multitude of languages to support. His team had trained it, but limited resources meant it couldn\u2019t scale. As a result, he says, the project became a proof of concept rather than a product.",
    "\u201cIf we had been funded two years ago, there\u2019s a good chance we\u2019d be the ones building what DeepSeek just released,\u201d he says. Kolavi\u2019s enthusiasm and Upperwal\u2019s dismay reflect the spectrum of emotions among India\u2019s AI builders. Despite its status as a global tech hub, the country lags far behind the likes of the US and China when it comes to homegrown AI. That gap has opened largely because India has chronically underinvested in R&D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.",
    "Related StoryWhy the world is looking to ditch US AI modelsContent moderation systems are being abandoned and defunded, leaving many countries looking for alternatives.",
    "Historically known as the global back office for the software industry, India has a tech ecosystem that evolved with a services-first mindset. Giants like Infosys and TCS built their success on efficient software delivery, but invention was neither prioritized nor rewarded. Meanwhile, India\u2019s R&D spending hovered at just 0.65% of GDP ($25.4 billion) in 2024, far behind China\u2019s 2.68% ($476.2 billion) and the US\u2019s 3.5% ($962.3 billion). The muscle to invent and commercialize deep tech, from algorithms to chips, was just never built. Isolated pockets of world-class research do exist within government agencies like the DRDO (Defense Research & Development Organization) and ISRO (Indian Space Research Organization), but their breakthroughs rarely spill into civilian or commercial use. India lacks the bridges to connect risk-taking research to commercial pathways, the way DARPA does in the US. Meanwhile, much of India\u2019s top talent migrates abroad, drawn to ecosystems that better understand and, crucially, fund deep tech.So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure, and how urgently it needed to respond. India responds In January 2025, 10 days after DeepSeek-R1\u2019s launch, the Ministry of Electronics and Information Technology (MeitY) solicited proposals for India\u2019s own foundation models, which are large AI models that can be adapted to a wide range of tasks. Its public tender invited private-sector cloud and data\u2011center companies to reserve GPU compute capacity for government\u2011led AI research.\u00a0 Providers including Jio, Yotta, E2E Networks, Tata, AWS partners, and CDAC responded. Through this arrangement, MeitY suddenly had access to nearly 19,000 GPUs at subsidized rates, repurposed from private infrastructure and allocated specifically to foundational AI projects. This triggered a surge of proposals from companies wanting to build their own models.\u00a0  Within two weeks, it had 67 proposals in hand. That number tripled by mid-March.\u00a0 In April, the government announced plans to develop six large-scale models by the end of 2025, plus 18 additional AI applications targeting sectors like agriculture, education, and climate action. Most notably, it tapped Sarvam AI to build a 70-billion-parameter model optimized for Indian languages and needs.\u00a0 For a nation long restricted by limited research infrastructure, things moved at record speed, marking a rare convergence of ambition, talent, and political will. \u201cIndia could do a Mangalyaan in AI,\u201d said Gautam Shroff of IIIT-Delhi, referencing the country\u2019s cost-effective, and successful, Mars orbiter mission.",
    "Jaspreet Bindra, cofounder of AI&Beyond, an organization focused on teaching AI literacy, captured the urgency: \u201cDeepSeek is probably the best thing that happened to India. It gave us a kick in the backside to stop talking and start doing something.\u201d The language problem One of the most fundamental challenges in building foundational AI models for India is the country\u2019s sheer linguistic diversity. With 22 official languages, hundreds of dialects, and millions of people who are multilingual, India poses a problem that few existing LLMs are equipped to handle. Whereas a massive amount of high-quality web data is available in English, Indian languages collectively make up less than 1% of online content. The lack of digitized, labeled, and cleaned data in languages like Bhojpuri and Kannada makes it difficult to train LLMs that understand how Indians actually speak or search. Global tokenizers, which break text into units a model can process, also perform poorly on many Indian scripts, misinterpreting characters or skipping some altogether. As a result, even when Indian languages are included in multilingual models, they\u2019re often poorly understood and inaccurately generated.  And unlike OpenAI and DeepSeek, which achieved scale using structured English-language data, Indian teams often begin with fragmented and low-quality data sets encompassing dozens of Indian languages. This makes the early steps of training foundation models far more complex. Related StoryThe grassroots push to digitize India\u2019s most precious documentsThe Servants of Knowledge collection on the Internet Archive is an effort to make up for the lack of library resources in India.",
    "Nonetheless, a small but determined group of Indian builders is starting to shape the country\u2019s AI future. For example, Sarvam AI has created OpenHathi-Hi-v0.1, an open-source Hindi language model that shows the Indian AI field\u2019s growing ability to address the country\u2019s vast linguistic diversity. The model, built on Meta\u2019s Llama 2 architecture, was trained on 40 billion tokens of Hindi and related Indian-language content, making it one of the largest open-source Hindi models available to date. Pragna-1B, the multilingual model from Upperwal, is more evidence that India could solve for its own linguistic complexity. Trained on 300 billion tokens for just $250,000, it introduced a technique called \u201cbalanced tokenization\u201d to address a unique challenge in Indian AI, enabling a 1.25-billion-parameter model to behave like a much larger one.The issue is that Indian languages use complex scripts and agglutinative grammar, where words are formed by stringing together many smaller units of meaning using prefixes and suffixes. Unlike English, which separates words with spaces and follows relatively simple structures, Indian languages like Hindi, Tamil, and Kannada often lack clear word boundaries and pack a lot of information into single words. Standard tokenizers struggle with such inputs. They end up breaking Indian words into too many tokens, which bloats the input and makes it harder for models to understand the meaning efficiently or respond accurately.",
    "With the new technique, however, \u201ca billion-parameter model was equivalent to a 7 billion one like Llama 2,\u201d Upperwal says. This performance was particularly marked in Hindi and Gujarati, where global models often underperform because of limited multilingual training data. It was a reminder that with smart engineering, small teams could still push boundaries.Upperwal eventually repurposed his core tech to build speech APIs for 22 Indian languages, a more immediate solution better suited to rural users who are often left out of English-first AI experiences. \u201cIf the path to AGI is a hundred-step process, training a language model is just step one,\u201d he says.",
    "At the other end of the spectrum are startups with more audacious aims. Krutrim-2, for instance, is a 12-billion-parameter multilingual language model optimized for English and 22 Indian languages.\u00a0 Krutrim-2 is attempting to solve India\u2019s specific problems of linguistic diversity, low-quality data, and cost constraints. The team built a custom Indic tokenizer, optimized training infrastructure, and designed models for multimodal and voice-first use cases from the start, crucial in a country where text interfaces can be a problem.  Krutrim\u2019s bet is that its approach will not only enable Indian AI sovereignty but also offer a model for AI that works across the Global South. Besides public funding and compute infrastructure, India also needs the institutional support of talent, the research depth, and the long-horizon capital that produce globally competitive science. While venture capital still hesitates to bet on research, new experiments are emerging. Paras Chopra, an entrepreneur who previously built and sold the software-as-a-service company Wingify, is now personally funding Lossfunk, a Bell Labs\u2013style AI residency program designed to attract independent researchers with a taste for open-source science.\u00a0 \u201cWe don\u2019t have role models in academia or industry,\u201d says Chopra. \u201cSo we\u2019re creating a space where top researchers can learn from each other and have startup-style equity upside.\u201d",
    "Government-backed bet on sovereign AI The clearest marker of India\u2019s AI ambitions came when the government selected Sarvam AI to develop a model focused on Indian languages and voice fluency. The idea is that it would not only help Indian companies compete in the global AI arms race but benefit the wider population as well. \u201cIf it becomes part of the India stack, you can educate hundreds of millions through conversational interfaces,\u201d says Bindra.\u00a0 Sarvam was given access to 4,096 Nvidia H100 GPUs for training a 70-billion-parameter Indian language model over six months. (The company previously released a 2-billion-parameter model trained in 10 Indian languages, called Sarvam-1.) Sarvam\u2019s project and others are part of a larger strategy called the IndiaAI Mission, a $1.25 billion national initiative launched in March 2024 to build out India\u2019s core AI infrastructure and make advanced tools more widely accessible. Led by MeitY, the mission is focused on supporting AI startups, particularly those developing foundation models in Indian languages and applying AI to key sectors such as health care, education, and agriculture.",
    "Under its compute program, the government is deploying more than 18,000 GPUs, including nearly 13,000 high-end H100 chips, to a select group of Indian startups that currently includes Sarvam, Upperwal\u2019s Soket Labs, Gnani AI, and Gan AI.\u00a0 The mission also includes plans to launch a national multilingual data set repository, establish AI labs in smaller cities, and fund deep-tech R&D. The broader goal is to equip Indian developers with the infrastructure needed to build globally competitive AI and ensure that the results are grounded in the linguistic and cultural realities of India and the Global South.According to Abhishek Singh, CEO of IndiaAI and an officer with MeitY, India\u2019s broader push into deep tech is expected to raise around $12 billion in research and development investment over the next five years.\u00a0 This includes approximately $162 million through the IndiaAI Mission, with about $32 million earmarked for direct startup funding. The National Quantum Mission is contributing another $730 million to support India\u2019s ambitions in quantum research. In addition to this, the national budget document for 2025-26 announced a $1.2 billion Deep Tech Fund of Funds aimed at catalyzing early-stage innovation in the private sector. The rest, nearly $9.9 billion, is expected to come from private and international sources including corporate R&D, venture capital firms, high-net-worth individuals, philanthropists, and global technology leaders such as Microsoft.\u00a0 IndiaAI has now received more than 500 applications from startups proposing use cases in sectors like health, governance, and agriculture.\u00a0 \u201cWe\u2019ve already announced support for Sarvam, and 10 to 12 more startups will be funded solely for foundational models,\u201d says Singh. Selection criteria include access to training data, talent depth, sector fit, and scalability. Open or closed? The IndiaAI program, however, is not without controversy. Sarvam is being built as a closed model, not open-source, despite its public tech roots. That has sparked debate about the proper balance between private enterprise and the public good.\u00a0 \u201cTrue sovereignty should be rooted in openness and transparency,\u201d says Amlan Mohanty, an AI policy specialist. He points to DeepSeek-R1, which despite its 236-billion parameter size was made freely available for commercial use.\u00a0 Related StoryWhy Chinese companies are betting on open-source AIFor Alibaba and several Chinese AI startups, open-source AI presents an opportunity for faster commercialization and global recognition.",
    "Its release allowed developers around the world to fine-tune it on low-cost GPUs, creating faster variants and extending its capabilities to non-English applications. \u201cReleasing an open-weight model with efficient inference can democratize AI,\u201d says Hancheng Cao, an assistant professor of information systems and operations management at Emory University. \u201cIt makes it usable by developers who don\u2019t have massive infrastructure.\u201d IndiaAI, however, has taken a neutral stance on whether publicly funded models should be open-source.\u00a0 \u201cWe didn\u2019t want to dictate business models,\u201d says Singh. \u201cIndia has always supported open standards and open source, but it\u2019s up to the teams. The goal is strong Indian models, whatever the route.\u201d There are other challenges as well. In late May, Sarvam\u202fAI unveiled Sarvam\u2011M, a 24-billion-parameter multilingual LLM fine-tuned for 10 Indian languages and built on top of Mistral\u202fSmall, an efficient model developed by the French company Mistral AI. Sarvam\u2019s cofounder Vivek\u202fRaghavan called the model \u201can important stepping stone on our journey to build sovereign AI for India.\u201d But its download numbers were underwhelming, with only 300 in the first two days. The venture capitalist Deedy Das called the launch \u201cembarrassing.\u201dAnd the issues go beyond the lukewarm early reception. Many developers in India still lack easy access to GPUs and the broader ecosystem for Indian-language AI applications is still nascent.\u00a0 The compute question Compute scarcity is emerging as one of the most significant bottlenecks in generative AI, not just in India but across the globe. For countries still heavily reliant on imported GPUs and lacking domestic fabrication capacity, the cost of building and running large models is often prohibitive.\u00a0 India still imports most of its chips rather than producing them domestically, and training large models remains expensive. That\u2019s why startups and researchers alike are focusing on software-level efficiencies that involve smaller models, better inference, and fine-tuning frameworks that optimize for performance on fewer GPUs. \u201cThe absence of infrastructure doesn\u2019t mean the absence of innovation,\u201d says Cao. \u201cSupporting optimization science is a smart way to work within constraints.\u201d\u00a0 Yet Singh of IndiaAI argues that the tide is turning on the infrastructure challenge thanks to the new government programs and private-public partnerships. \u201cI believe that within the next three months, we will no longer face the kind of compute bottlenecks we saw last year,\u201d he says. India also has a cost advantage.According to Gupta, building a hyperscale data center in India costs about $5 million, roughly half what it would cost in markets like the US, Europe, or Singapore. That\u2019s thanks to affordable land, lower construction and labor costs, and a large pool of skilled engineers.\u00a0 For now, India\u2019s AI ambitions seem less about leapfrogging OpenAI or DeepSeek and more about strategic self-determination. Whether its approach takes the form of smaller sovereign models, open ecosystems, or public-private hybrids, the country is betting that it can chart its own course.\u00a0 While some experts argue that the government\u2019s action, or reaction (to DeepSeek), is performative and aligned with its nationalistic agenda, many startup founders are energized. They see the growing collaboration between the state and the private sector as a real opportunity to overcome India's long-standing structural challenges in tech innovation. At a Meta summit held in Bengaluru last year, Nandan Nilekani, the chairman of Infosys, urged India to resist chasing a me-too AI dream.\u00a0 \u201cLet the big boys in the Valley do it,\u201d he said of building LLMs. \u201cWe will use it to create synthetic data, build small language models quickly, and train them using appropriate data.\u201d\u00a0 His view that India should prioritize strength over spectacle had a divided reception. But it reflects a broader growing consensus on whether India should play a different game altogether. \u201cTrying to dominate every layer of the stack isn\u2019t realistic, even for China,\u201d says Shobhankita Reddy, a researcher at the Takshashila Institution, an Indian public policy nonprofit. \u201cDominate one layer, like applications, services, or talent, so you remain indispensable.\u201d\u00a0 Correction: We amended Reddy's name  hide"
  ]
}