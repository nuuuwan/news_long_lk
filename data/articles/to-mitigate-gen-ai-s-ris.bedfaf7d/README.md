# To Mitigate Gen AI’s Risks, Draw on Your Team’s Collective Judgment

[https://hbr.org/2024/11/to-mitigate-gen-ais-risks-draw-on-your-teams-collective-judgment](https://hbr.org/2024/11/to-mitigate-gen-ais-risks-draw-on-your-teams-collective-judgment)

*01:15 PM, Wednesday, November 20, 2024*

Organizations today face the challenge of using generative AI without falling prey to its drawbacks. Many companies have employed two basic layers of risk management strategies: policies on how to use the technology, and critical thinking about gen AI’s outputs....more

Generative AI (gen AI) offers transformative opportunities for learning, work, creativity, and decision-making across organizations. This marks a paradigm shift toward unprecedented human-machine collaboration. However, its integration into the way organizations work is complex. The risks are many, including trust and accuracy issues, hallucinations, and inherited biases from the underlying foundational models. Many companies today face the conundrum of harnessing the power of gen AI without falling prey to its potential drawbacks.

The fear of making critical mistakes with gen AI has led some organizations to severely throttle its use or ban it outright. This avoidant approach is more dangerous than it seems, as it can lead to unintended consequences (such as employees using personal accounts to circumvent restrictions) and prevents companies from reaping the benefits of gen AI.

Other companies more willing to experiment with gen AI have turned to a basic layer of smart risk management strategies. These mitigations begin with usage policies, codes of conduct, restricting to approved tools, and guardrails. These measures are advisable and necessary, but they are insufficient; they don’t address many unknown, future, and hard-to-define scenarios, especially given the rapid pace of technological change.

A second layer of risk mitigation is growing more prevalent: Many companies now encourage their people to employ critical thinking and judgment around AI tools. This individual-level judgment goes beyond simply vetting AI-generated output; it involves framing the right collaboration mode with AI, setting clear context and boundaries, and giving the machine continuous feedback.

Exercising individual judgment has benefits over policy alone. It empowers employees to interpret guidelines in the context of use, as policies cannot cover every situation. Judgment enables individuals to translate and adapt abstract rules into real-world scenarios — for example, ensuring confidentiality by carefully sanitizing specific sensitive elements included in a prompt. However, individual judgment may not be enough in all circumstances. In fact, when AI hallucinates or generates inaccurate outputs, an individual may unknowingly fall into a “trust trap,” swayed by the authoritative tone of AI-generated suggestions.

Insight Center Collection

Collaborating with AI

How humans and machines can best work together.

In this light, we propose a novel layer of risk mitigation: team-based judgment.

The Three Types of Team-Based Judgment

Research on fields as diverse as healthcare, air traffic control, and nuclear power generation highlights the critical role of teamwork in mitigating operational risk. One of us, Michelle Renecle, has explored how integrating strict policies with collective sensemaking can enhance risk management. Results confirmed that organizations can nurture a shared responsibility and develop a mindful orientation toward risk.

A similar approach of combining strict rules with team-level judgment can help organizations effectively tackle AI challenges. Discussions with colleagues can bring diverse perspectives, challenge the output, and uncover hidden biases. Reaching out to domain experts can help validate specific data or review areas where there are doubts or concerns due to a lack of knowledge to judge confidently. Team-level judgment has three primary manifestations:

Collective Judgment: Teams should engage in discussions to ensure accuracy, identify risks, and evaluate AI-generated output. For example, if members are unsure about an AI-generated output, they will consult with other colleagues or teams and review the AI-generated content before accepting and using it. These shared reflection and discussion routines enable a comprehensive assessment of potential impacts, including ethical and legal implications.

Domain Judgment: When unexpected events occur or anomalies are detected in the output of AI systems, teams should delegate decision-making authority to those with the highest expertise or those closest to the work, rather than to those with the highest rank. By identifying subject matter experts to consult with when questions or problems arise with AI, their insights can guide the team in a timely manner and prevent negative consequences from spreading.

Reflective Judgment: Teams should meet regularly to share and reflect on their experiences and on those of others using AI, enhancing their understanding and ability to navigate the AI systems. This ongoing learning is critical for keeping pace with the rapid evolution of the technology, allowing teams to learn from mistakes and successes to optimize safe and effective use. One example of reflective judgment is holding regular team retrospectives to discuss lessons learned.

Incorporating these forms of team-level judgment addresses critical elements of risk that policy or individual judgment alone may miss. This collaborative approach helps to uncover blind spots and biases, offering more comprehensive safeguards.

Team-Based Judgment in Practice

Let’s explore a concrete example. Imagine a team of three people — Gaby, Trent, and Xi — who are tasked with preparing a report for their management on digital trends and their impact on supply chain operations. Imagine two scenarios:

Scenario 1, the team falls into the trap: Gaby, Trent, and Xi begin crafting a report using gen AI to research industry trends. The team divides the report into three sections, with each member responsible for writing one section. Gaby instructs the gen AI system to collect data on technology trends in various regions, including examples of early applications in supply chain cases. The gen AI model performs well overall, but unfortunately, it hallucinates on one specific aspect, fabricating a believable case study and related sources in the analysis for one country. The next two sections rely in part on the fabricated case created by Gaby’s section. Trent and Xi continue to build the report based on the findings of the fabricated case, amplifying the error.

Scenario 2, the team avoids the trap with judgment: At the start of the project, Gaby, Trent, and Xi discuss the brief and how to use gen AI to help them throughout the process. While each has been assigned a section to write, they decide to meet once a week to collectively review each section for potential errors or inconsistencies (collective judgment). In the second week of the project, the team reviews Gaby’s section on technology trends. Trent points out that the example mentioned in the study is intriguing, but since it refers to a specific country, it may be wise to consult Jane, a supply chain expert from that country. Xi contacts Jane, who shares that the example is not plausible due to local regulations in that country (domain judgment). Upon further investigation, they determine that the case study was a hallucination, and that no reliable data exists. After this accident, the team hosts a workshop about what they learned from the project to help other colleagues understand how to recognize and deal with potential hallucinations (reflective judgment).

Let’s consider a real-life application. In the automotive industry, teams of engineers design complex mechanical systems. Optimizing and troubleshooting these machines inevitably requires team problem-solving and root cause analysis. Fault trees can be complex: Analyzing the potential causes of oil leakage in a turbocharged engine involves evaluating origins such as gasket failure, seal deterioration, component fatigue, improper assembly process, and inaccurate oil system monitoring. In these situations, gen AI can augment the team’s capabilities — but AI may lead the team down the wrong path if taken with little discernment. A high level of team-based judgment is key to managing these risks.

In the words of Valentine Marguet, Powertrain Project Lead at Ferrari, “In my team, we ensure we stay in control, reducing the risk of using incorrect or irrelevant information by critically assessing AI outputs with our experts. We quickly recalibrate and refine our root-case analysis process, ensuring that AI contributions are technically grounded.”

This example shows all three components of team-based judgment working in concert: Collective judgment ensures thorough review and cross-checking of AI outputs, domain judgment provides expert validation of specific details, and reflective judgment promotes ongoing learning and improvement.

Developing Team-Based Judgment

Team-based judgment needs to be cultivated before it can be effective. It requires ongoing practice and reinforcement to develop into a robust mechanism for managing risks and ensuring high-quality outcomes. Here are some actionable recommendations to help leaders build this ability within their teams:

How to strengthen collective judgment

Create routines for reflection and discussion within project timelines. This could include short meetings or checkpoints specifically designed for collective sensemaking. For instance, after a one-on-one interaction with the machine, taking a moment to review and critique the AI-generated outputs with peers can provide valuable insights, and ensures that the outputs are not accepted uncritically, but instead examined for accuracy and relevance. Potential questions for such challenge sessions: Is this plausible? Are the sources cited correctly? Is this recommendation prone to potential risks or biases? Making this critical judgment in a group is more powerful because you have different perspectives and expertise, and together are less likely to fall into the trust trap.

How to strengthen domain judgment

Establish mechanisms for identifying and activating domain experts throughout the project. First, ensure that individuals understand the need for engaging fellow experts along the human-AI collaborative process. Then, streamline the process of identifying the right experts (having experts maps or a list of subject matter experts) and reaching out to experts (for example, asking questions on internal communities of practice, often through virtual chats system or discussion forums). Introduce incentives for domain experts who support teams and inform decision-making when issues arise.

How to strengthen reflective judgment

Allow time for continuous learning activities and encourage the sharing of experiences through dedicated sessions and repositories. Retrospective sessions should be built into project plans to facilitate learning within and across teams. Useful questions to explore include: What did we learn this month using this new technology? What are the most common errors? How are our colleagues dealing with similar challenges? How can we better integrate gen AI into our workflows to avoid these errors? In addition, learning can be consolidated and curated in a knowledge repository or as part of gen AI academies.

When you are ready to practice and reinforce team-based judgment for gen AI, use this table. It includes definitions and sample questions to start a discussion and strengthen your team’s approach.

The Three Types of Team-Based Judgment

Using the three forms of team-level judgment together can address elements of gen AI risk that policy or individual judgment may miss.

What it is Teams critically discuss the AI-generated content.

Raise awareness of “AI traps”

Anticipate potential risks of using gen AI

Organize “challenge sessions” within the team

Sample questions What common “AI traps” should we be aware of when using this content, and have we taken steps to mitigate them?Is the level of risk high enough to justify a dedicated “challenge session” with experts?

What it is Teams reach out to experts to check the accuracy of AI-generated content when it falls outside their knowledge domain or zone of confidence.

Online community of practice

Sample questions Does this AI-generated content fall outside of our expertise? If so, have we consulted with the relevant experts? Is there a need to engage with a specialized community of practice to gain deeper insights or clarification on this content?

What it is Teams periodically extract, share, and discuss learnings.

Retrospective sessions

Experience-sharing sessions

Shared repository to store all learnings

Sample questions How well are we collecting and sharing best practices with other teams to mitigate future risks?

Source: Gabriele Rosani, Elisa Farri, and Michelle Renecle

Team-based judgment is a strategic asset for navigating an AI-driven future. By cultivating this capability at scale, firms are better equipped to rapidly adapt to the fast-evolving AI landscape, including risks that are not widespread or even known yet. This collective wisdom empowers organizations to confidently embrace AI integration, adapt to changing workflows, and manage the complexities of human-AI collaboration, accelerating the realization of benefits while proactively mitigating both existing and emerging risks.

Readers Also Viewed These Items

The Harvard Business Review Sales Management Handbook: How to Lead High-Performing Sales Teams

Generative AI Skills: Craft Smart Prompts (Virtual Group Learning)

Read more on Generative AI

Collaboration and teams and Risk management

