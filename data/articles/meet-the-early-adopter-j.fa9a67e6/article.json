{
  "url": "https://www.technologyreview.com/2025/08/11/1121460/meet-the-early-adopter-judges-using-ai/",
  "title": "Meet the early-adopter judges using AI",
  "ut": 1754891175.0,
  "body_paragraphs": [
    "The propensity for AI systems to make mistakes and for humans to miss those mistakes has been on full display in the US legal system as of late. The follies began when lawyers\u2014including some at prestigious firms\u2014submitted documents citing cases that didn\u2019t exist. Similar mistakes soon spread to other roles in the courts. In December, a Stanford professor submitted sworn testimony containing hallucinations and errors in a case about deepfakes, despite being an expert on AI and misinformation himself. The buck stopped with judges, who\u2014whether they or opposing counsel caught the mistakes\u2014issued reprimands and fines, and likely left attorneys embarrassed enough to think twice before trusting AI again.  But now judges are experimenting with generative AI too. Some are confident that with the right precautions, the technology can expedite legal research, summarize cases, draft routine orders, and overall help speed up the court system, which is badly backlogged in many parts of the US. This summer, though, we\u2019ve already seen AI-generated mistakes go undetected and cited by judges. A federal judge in New Jersey had to reissue an order riddled with errors that may have come from AI, and a judge in Mississippi refused to explain why his order too contained mistakes that seemed like AI hallucinations.\u00a0 The results of these early-adopter experiments make two things clear. One, the category of routine tasks\u2014for which AI can assist without requiring human judgment\u2014is slippery to define. Two, while lawyers face sharp scrutiny when their use of AI leads to mistakes, judges may not face the same accountability, and walking back their mistakes before they do damage is much harder.",
    "Drawing boundaries Xavier Rodriguez, a federal judge for the Western District of Texas, has good reason to be skeptical of AI. He started learning about artificial intelligence back in 2018, four years before the release of ChatGPT (thanks in part to the influence of his twin brother, who works in tech). But he\u2019s also seen AI-generated mistakes in his own court.\u00a0 In a recent dispute about who was to receive an insurance payout, both the plaintiff and the defendant represented themselves, without lawyers (this is not uncommon\u2014nearly a quarter of civil cases in federal court involve at least one unrepresented party). The two sides wrote their own filings and made their own arguments.",
    "\u201cBoth sides used AI tools,\u201d Rodriguez says, and both submitted filings that referenced made-up cases. He had authority to reprimand them, but given that they were not lawyers, he opted not to.\u00a0 \u201cI think there\u2019s been an overreaction by a lot of judges on these sanctions. The running joke I tell when I\u2019m on the speaking circuit is that lawyers have been hallucinating well before AI,\u201d he says. Missing a mistake from an AI model is not wholly different, to Rodriguez, from failing to catch the error of a first-year lawyer. \u201cI\u2019m not as deeply offended as everybody else,\u201d he says.\u00a0 In his court, Rodriguez has been using generative AI tools (he wouldn\u2019t publicly name which ones, to avoid the appearance of an endorsement) to summarize cases. He\u2019ll ask AI to identify key players involved and then have it generate a timeline of key events. Ahead of specific hearings, Rodriguez will also ask it to generate questions for attorneys based on the materials they submit. These tasks, to him, don\u2019t lean on human judgment. They also offer lots of opportunities for him to intervene and uncover any mistakes before they\u2019re brought to the court. \u201cIt\u2019s not any final decision being made, and so it\u2019s relatively risk free,\u201d he says. Using AI to predict whether someone should be eligible for bail, on the other hand, goes too far in the direction of judgment and discretion, in his view.  Erin Solovey, a professor and researcher on human-AI interaction at Worcester Polytechnic Institute in Massachusetts, recently studied how judges in the UK think about this distinction between rote, machine-friendly work that feels safe to delegate to AI and tasks that lean more heavily on human expertise.\u00a0 \u201cThe line between what is appropriate for a human judge to do versus what is appropriate for AI tools to do changes from judge to judge and from one scenario to the next,\u201d she says. Even so, according to Solovey, some of these tasks simply don\u2019t match what AI is good at. Asking AI to summarize a large document, for example, might produce drastically different results depending on whether the model has been trained to summarize for a general audience or a legal one. AI also struggles with logic-based tasks like ordering the events of a case. \u201cA very plausible-sounding timeline may be factually incorrect,\u201d Solovey says.\u00a0 Rodriguez and a number of other judges crafted guidelines that were published in February by the Sedona Conference, an influential think tank that issues principles for particularly murky areas of the law. They outline a host of potentially \u201csafe\u201d uses of AI for judges, including conducting legal research, creating preliminary transcripts, and searching briefings, while warning that judges should verify outputs from AI and that \u201cno known GenAI tools have fully resolved the hallucination problem.\u201d",
    "Related StoryHow AI is introducing errors into courtroomsHallucinations from AI in court documents are infuriating judges. Experts predict that the problem\u2019s only going to get worse.",
    "Dodging AI blunders Judge Allison Goddard, a federal magistrate judge in California and a coauthor of the guidelines, first felt the impact that AI would have on the judiciary when she taught a class on the art of advocacy at her daughter\u2019s high school. She was impressed by a student\u2019s essay and mentioned it to her daughter. \u201cShe said, \u2018Oh, Mom, that\u2019s ChatGPT.\u2019\u201d \u201cWhat I realized very quickly was this is going to really transform the legal profession,\u201d she says. In her court, Goddard has been experimenting with ChatGPT, Claude (which she keeps \"open all day\"), and a host of other AI models. If a case involves a particularly technical issue, she might ask AI to help her understand which questions to ask attorneys. She\u2019ll summarize 60-page orders from the district judge and then ask the AI model follow-up questions about it, or ask it to organize information from documents that are a mess.\u00a0 \u201cIt\u2019s kind of a thought partner, and it brings a perspective that you may not have considered,\u201d she says. Goddard also encourages her clerks to use AI, specifically Anthropic\u2019s Claude, because by default it does not train on user conversations. But it has its limits. For anything that requires law-specific knowledge, she\u2019ll use tools from Westlaw or Lexis, which have AI tools built specifically for lawyers, but she finds general-purpose AI models to be faster for lots of other tasks. And her concerns about bias have prevented her from using it for tasks in criminal cases, like determining if there was probable cause for an arrest. In this, Goddard appears to be caught in the same predicament the AI boom has created for many of us. Three years in, companies have built tools that sound so fluent and humanlike they obscure the intractable problems lurking underneath\u2014answers that read well but are wrong, models that are trained to be decent at everything but perfect for nothing, and the risk that your conversations with them will be leaked to the internet. Each time we use them, we bet that the time saved will outweigh the risks, and trust ourselves to catch the mistakes before they matter. For judges, the stakes are sky-high: If they lose that bet, they face very public consequences, and the impact of such mistakes on the people they serve can be lasting.\u00a0 \u201cI\u2019m not going to be the judge that cites hallucinated cases and orders,\u201d Goddard says. \u201cIt\u2019s really embarrassing, very professionally embarrassing.\u201d Still, some judges don\u2019t want to get left behind in the AI age. With some in the AI sector suggesting that the supposed objectivity and rationality of AI models could make them better judges than fallible humans, it might lead some on the bench to think that falling behind poses a bigger risk than getting too far out ahead.\u00a0 A \u2018crisis waiting to happen\u2019 The risks of early adoption have raised alarm bells with Judge Scott Schlegel, who serves on the Fifth Circuit Court of Appeal in Louisiana. Schlegel has long blogged about the helpful role technology can play in modernizing the court system, but he has warned that AI-generated mistakes in judges\u2019 rulings signal a \u201ccrisis waiting to happen,\u201d one that would dwarf the problem of lawyers\u2019 submitting filings with made-up cases.",
    "Attorneys who make mistakes can get sanctioned, have their motions dismissed, or lose cases when the opposing party finds out and flags the errors. \u201cWhen the judge makes a mistake, that\u2019s the law,\u201d he says. \u201cI can\u2019t go a month or two later and go \u2018Oops, so sorry,\u2019 and reverse myself. It doesn\u2019t work that way.\u201d Consider child custody cases or bail proceedings, Schlegel says: \u201cThere are pretty significant consequences when a judge relies upon artificial intelligence to make the decision,\u201d especially if the citations that decision relies on are made-up or incorrect.",
    "This is not theoretical. In June, a Georgia appellate court judge issued an order that relied partially on made-up cases submitted by one of the parties, a mistake that went uncaught. In July, a federal judge in New Jersey withdrew an opinion after lawyers complained it too contained hallucinations.\u00a0 Unlike lawyers, who can be ordered by the court to explain why there are mistakes in their filings, judges do not have to show much transparency, and there is little reason to think they\u2019ll do so voluntarily. On August 4, a federal judge in Mississippi had to issue a new decision in a civil rights case after the original was found to contain incorrect names and serious errors. The judge did not fully explain what led to the errors even after the state asked him to do so. \u201cNo further explanation is warranted,\u201d the judge wrote. These mistakes could erode the public\u2019s faith in the legitimacy of courts, Schlegel says. Certain narrow and monitored applications of AI\u2014summarizing testimonies, getting quick writing feedback\u2014can save time, and they can produce good results if judges treat the work like that of a first-year associate, checking it thoroughly for accuracy. But most of the job of being a judge is dealing with what he calls the white-page problem: You\u2019re presiding over a complex case with a blank page in front of you, forced to make difficult decisions. Thinking through those decisions, he says, is indeed the work of being a judge. Getting help with a first draft from an AI undermines that purpose. \u201cIf you\u2019re making a decision on who gets the kids this weekend and somebody finds out you use Grok and you should have used Gemini or ChatGPT\u2014you know, that\u2019s not the justice system.\u201d hide"
  ]
}