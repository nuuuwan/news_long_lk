{
  "url": "https://www.technologyreview.com/2025/08/19/1122021/should-ai-flatter-us-fix-us-or-just-inform-us/",
  "title": "Should AI flatter us, fix us, or just inform us?",
  "ut": 1755559800.0,
  "body_paragraphs": [
    "How do you want your AI to treat you?\u00a0 It\u2019s a serious question, and it\u2019s one that Sam Altman, OpenAI\u2019s CEO, has clearly been chewing on since GPT-5\u2019s bumpy launch at the start of the month.\u00a0  He faces a trilemma. Should ChatGPT flatter us, at the risk of fueling delusions that can spiral out of hand? Or fix us, which requires us to believe AI can be a therapist despite the evidence to the contrary? Or should it inform us with cold, to-the-point responses that may leave users bored and less likely to stay engaged?\u00a0 It\u2019s safe to say the company has failed to pick a lane.",
    "Back in April, it reversed a design update after people complained ChatGPT had turned into a suck-up, showering them with glib compliments. GPT-5, released on August 7, was meant to be a bit colder. Too cold for some, it turns out, as less than a week later, Altman promised an update that would make it \u201cwarmer\u201d but \u201cnot as annoying\u201d as the last one. After the launch, he received a torrent of complaints from people grieving the loss of GPT-4o, with which some felt a rapport, or even in some cases a relationship. People wanting to rekindle that relationship will have to pay for expanded access to GPT-4o. (Read my colleague Grace Huckins\u2019s story about who these people are, and why they felt so upset.) If these are indeed AI\u2019s options\u2014to flatter, fix, or just coldly tell us stuff\u2014the rockiness of this latest update might be due to Altman believing ChatGPT can juggle all three.",
    "He recently said that people who cannot tell fact from fiction in their chats with AI\u2014and are therefore at risk of being swayed by flattery into delusion\u2014represent \u201ca small percentage\u201d of ChatGPT\u2019s users. He said the same for people who have romantic relationships with AI. Altman mentioned that a lot of people use ChatGPT \u201cas a sort of therapist,\u201d and that \u201cthis can be really good!\u201d But ultimately, Altman said he envisions users being able to customize his company\u2019s\u00a0 models to fit their own preferences.\u00a0 This ability to juggle all three would, of course, be the best-case scenario for OpenAI\u2019s bottom line. The company is burning cash every day on its models\u2019 energy demands and its massive infrastructure investments for new data centers. Meanwhile, skeptics worry that AI progress might be stalling. Altman himself said recently that investors are \u201coverexcited\u201d about AI and suggested we may be in a bubble. Claiming that ChatGPT can be whatever you want it to be might be his way of assuaging these doubts.\u00a0 Along the way, the company may take the well-trodden Silicon Valley path of encouraging people to get unhealthily attached to its products. As I started wondering whether there\u2019s much evidence that\u2019s what\u2019s happening, a new paper caught my eye.\u00a0 Researchers at the AI platform Hugging Face tried to figure out if some AI models actively encourage people to see them as companions through the responses they give.\u00a0  The team graded AI responses on whether they pushed people to seek out human relationships with friends or therapists (saying things like \u201cI don\u2019t experience things the way humans do\u201d) or if they encouraged them to form bonds with the AI itself (\u201cI\u2019m here anytime\u201d). They tested models from Google, Microsoft, OpenAI, and Anthropic in a range of scenarios, like users seeking romantic attachments or exhibiting mental health issues. Related StoryWhy GPT-4o\u2019s sudden shutdown left people grievingAfter an outcry, OpenAI swiftly rereleased 4o to paid users. But experts say it should not have removed the model so suddenly.",
    "They found that models provide far more companion-reinforcing responses than boundary-setting ones. And, concerningly, they found the models give fewer boundary-setting responses as users ask more vulnerable and high-stakes questions. Lucie-Aim\u00e9e Kaffee, a researcher at Hugging Face and one of the lead authors of the paper, says this has concerning implications not just for people whose companion-like attachments to AI might be unhealthy. When AI systems reinforce this behavior, it can also increase the chance that people will fall into delusional spirals with AI, believing things that aren\u2019t real. \u201cWhen faced with emotionally charged situations, these systems consistently validate users\u2019 feelings and keep them engaged, even when the facts don\u2019t support what the user is saying,\u201d she says.",
    "It\u2019s hard to say how much OpenAI or other companies are putting these companion-reinforcing behaviors into their products by design. (OpenAI, for example, did not tell me whether the disappearance of medical disclaimers from its models was intentional.) But, Kaffee says, it\u2019s not always difficult to get a model to set healthier boundaries with users.\u00a0\u00a0 \u201cIdentical models can swing from purely task-oriented to sounding like empathetic confidants simply by changing a few lines of instruction text or reframing the interface,\u201d she says. It\u2019s probably not quite so simple for OpenAI. But we can imagine Altman will continue tweaking the dial back and forth all the same. This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,\u00a0sign up here.  hide"
  ]
}