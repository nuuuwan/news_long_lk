{
  "url": "https://www.technologyreview.com/2024/12/31/1109612/biggest-worst-ai-artificial-intelligence-flops-fails-2024/",
  "title": "The biggest AI flops of 2024",
  "ut": 1735636380.0,
  "body_paragraphs": [
    "The past 12 months have been undeniably busy for those working in AI. There have been more successful product launches than we can count, and even Nobel Prizes. But it hasn\u2019t always been smooth sailing. AI is an unpredictable technology, and the increasing availability of generative models has led people to test their limits in new, weird, and sometimes harmful ways. These were some of 2024\u2019s biggest AI misfires.\u00a0  AI slop infiltrated almost every corner of the internet Generative AI makes creating reams of text, images, videos, and other types of material a breeze. Because it takes just a few seconds for your model of choice to spit out a result once you enter a prompt, these models have become a quick, easy way to produce content on a massive scale. And 2024 was the year we started calling this (generally poor-quality) media what it is: AI slop.\u00a0\u00a0 This low-stakes way of creating AI slop means it can now be found in pretty much every corner of the internet, from the newsletters in your inbox and books sold on Amazon to ads and articles across the web and shonky pictures on your social media feeds. The more emotionally evocative these pictures are (wounded veterans, crying children, a signal of support in the Israel-Palestine conflict), the more likely they are to be shared, resulting in higher engagement and ad revenue for their savvy creators.",
    "AI slop isn\u2019t just annoying\u2014its rise poses a genuine problem for the future of the very models that helped to produce it. Because those models are trained on data scraped from the internet, the increasing number of junky websites containing AI garbage means there\u2019s a very real danger models\u2019 output and performance will get steadily worse.\u00a0 AI art is warping our expectations of real events This was also the year that the effects of surreal AI images started seeping into our real lives. Willy\u2019s Chocolate Experience, a wildly unofficial immersive event inspired by Roald Dahl\u2019s Charlie and the Chocolate Factory, made headlines across the world in February after its fantastical AI-generated marketing materials gave visitors the impression it would be much grander than the sparsely decorated warehouse its producers created.Similarly, hundreds of people lined the streets of Dublin for a Halloween parade that didn\u2019t exist. A Pakistan-based website used AI to create a list of events in the city, which was shared widely across social media ahead of October 31. Although the SEO-baiting site (myspirithalloween.com) has since been taken down, both events illustrate how misplaced public trust in AI-generated material online can come back to haunt us.",
    "Grok allows users to create images of pretty much any scenario The vast majority of major AI image generators have guardrails\u2014rules that dictate what AI models can and can\u2019t do\u2014to prevent users from creating violent, explicit, illegal, and otherwise harmful content. Sometimes these guardrails are just meant to make sure that no one makes blatant use of others\u2019 intellectual property. But Grok, an assistant made by Elon Musk\u2019s AI company, called xAI, ignores almost all of these principles in line with Musk\u2019s rejection of what he calls \u201cwoke AI.\u201d Related StoryWhat is AI?Everyone thinks they know, but no one can agree. And that\u2019s a problem.",
    "Whereas other image models will generally refuse to create images of celebrities, copyrighted material, violence, or terrorism\u2014unless they\u2019re tricked into ignoring these rules\u2014Grok will happily generate images of Donald Trump firing a bazooka, or Mickey Mouse holding a bomb. While it draws the line at generating nude images, its refusal to play by the rules undermines other companies\u2019 efforts to steer clear of creating problematic material. Sexually explicit deepfakes of Taylor Swift circulated online In January, nonconsensual deepfake nudes of singer Taylor Swift started circulating on social media, including X and Facebook. A Telegram community tricked Microsoft\u2019s AI image generator Designer into making the explicit images, demonstrating how guardrails can be circumvented even when they are in place.\u00a0 While Microsoft quickly closed the system\u2019s loopholes, the incident shined a light on the platforms\u2019 poor content-moderation policies, after posts containing the images circulated widely and remained live for days. But the most chilling takeaway is how powerless we still are to fight nonconsensual deepfake porn. While watermarking and data-poisoning tools can help, they\u2019ll need to be adopted much more widely to make a difference.  Business chatbots went haywire As AI becomes more widespread, businesses are racing to adopt generative tools to save time and money, and to maximize efficiency. The problem is that chatbots make stuff up and can\u2019t be relied upon to provide you with accurate information.Air Canada found this out the hard way after its chatbot advised a customer to follow a bereavement refund policy that didn\u2019t exist. In February, a Canadian small-claims tribunal upheld the customer\u2019s legal complaint, despite the airline\u2019s assertion that the chatbot was a \u201cseparate legal entity that is responsible for its own actions.\u201d Related StoryThe AI Hype Index: Robot pets, simulated humans, and Apple\u2019s AI text summariesMIT Technology Review\u2019s highly subjective take on the latest buzz about AI.",
    "In other high-profile examples of how chatbots can do more harm than good, the delivery firm DPD\u2019s bot cheerfully swore and called itself useless with little prompting, while a different bot set up to provide New Yorkers with accurate information about their city\u2019s government ended up dispensing guidance on how to break the law. AI gadgets aren\u2019t exactly setting the market alight Hardware assistants are something the AI industry tried, and failed, to crack in 2024. Humane attempted to sell customers on the promise of the Ai Pin, a wearable lapel computer, but even slashing its price failed to boost weak sales. The Rabbit R1, a ChatGPT-based personal assistant device, suffered a similar fate, following a rash of critical reviews and reports that it was slow and buggy. Both products seemed to be trying to solve a problem that did not actually exist.\u00a0 AI search summaries went awry Have you ever added glue to a pizza, or eaten a small rock? These are just some of the outlandish suggestions that Google\u2019s AI Overviews feature gave web users in May after the search giant added generated responses to the top of search results. The problem was that AI systems can\u2019t tell the difference between a factually correct news story and a joke post on Reddit. Users raced to find the strangest responses AI Overviews could generate.These fails were funny, but AI summaries can also have serious consequences. A new iPhone feature that groups app notifications together and creates summaries of their contents recently generated a false BBC News headline. The summary falsely stated that Luigi Mangione, who has been charged with the murder of the health insurance CEO Brian Thompson, had shot himself. The same feature had previously created a headline claiming that Israeli prime minister Benjamin Netanyahu had been arrested, which was also incorrect. These kinds of errors can inadvertently spread misinformation and undermine trust in news organizations. hide"
  ]
}